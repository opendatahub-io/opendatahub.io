{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/serving-models/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/api-workbench/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"api-workbench-overview_api-workbench"},{"parentId":null,"name":"Creating a custom image by using the <code>ImageStream</code> CRD","level":1,"index":1,"id":"api-custom-image-creating_api-workbench"},{"parentId":null,"name":"Creating a workbench by using the <code>Notebook</code> CRD","level":1,"index":2,"id":"api-workbench-creating_api-workbench"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/configuring-feature-store/"},"sections":[{"parentId":null,"name":"Overview of machine learning features and Feature Store","level":1,"index":0,"id":"overview-of-features-and-feature-store_featurestore"},{"parentId":"overview-of-features-and-feature-store_featurestore","name":"Overview of machine learning features","level":2,"index":0,"id":"_overview_of_machine_learning_features"},{"parentId":"overview-of-features-and-feature-store_featurestore","name":"Overview of Feature Store","level":2,"index":1,"id":"_overview_of_feature_store"},{"parentId":"overview-of-features-and-feature-store_featurestore","name":"Audience for Feature Store","level":2,"index":2,"id":"_audience_for_feature_store"},{"parentId":null,"name":"Before you begin","level":1,"index":1,"id":"before-you-begin_featurestore"},{"parentId":null,"name":"Enabling the Feature Store component","level":1,"index":2,"id":"enabling-the-feature-store-component_featurestore"},{"parentId":null,"name":"Deploying a feature store instance in a data science project","level":1,"index":3,"id":"deploying-a-feature-store-instance-in-a-data-science-project_featurestore"},{"parentId":null,"name":"Customizing your feature store configuration","level":1,"index":4,"id":"customizing-your-feature-store-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Specifying to use a feature project from a Git repository","level":2,"index":0,"id":"specifying-to-use-a-feature-project-from-git-repository_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an offline store","level":2,"index":1,"id":"configuring-an-offline-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an online store","level":2,"index":2,"id":"configuring-an-online-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring the feature registry","level":2,"index":3,"id":"configuring-the-feature-registry_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring role-based access control","level":2,"index":4,"id":"configuring-role-based-access-control_featurestore"},{"parentId":"configuring-role-based-access-control_featurestore","name":"Default authorization configuration","level":3,"index":0,"id":"ref-default-authorization-configuration_featurestore"},{"parentId":"configuring-role-based-access-control_featurestore","name":"Example OIDC Authorization configuration","level":3,"index":1,"id":"ref-example-oidc-authorization-configuration_featurestore"},{"parentId":"configuring-role-based-access-control_featurestore","name":"Example Kubernetes Authorization configuration","level":3,"index":2,"id":"ref-example-kubernetes-authorization-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Editing an existing feature store instance","level":2,"index":5,"id":"editing-an-existing-feature-store-instance_featurestore"},{"parentId":null,"name":"Viewing feature store objects in the web-based UI","level":1,"index":5,"id":"viewing-feature-store-objects-in-the-web-based-ui_featurestore"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/customizing-models-with-lab-tuning/"},"sections":[{"parentId":null,"name":"Enabling LAB-tuning","level":1,"index":0,"id":"enabling-lab-tuning_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Overview of enabling LAB-tuning","level":2,"index":0,"id":"overview-of-enabling-lab-tuning_lab-tuning"},{"parentId":"overview-of-enabling-lab-tuning_lab-tuning","name":"Requirements for LAB-tuning","level":3,"index":0,"id":"_requirements_for_lab_tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Installing the required Operators for LAB-tuning","level":2,"index":1,"id":"installing-the-required-operators-for-lab-tuning_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Installing the required components for LAB-tuning","level":2,"index":2,"id":"installing-the-required-components-for-lab-tuning_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Configuring a storage class for LAB-tuning","level":2,"index":3,"id":"configuring-a-storage-class-for-lab-tuning_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Making LAB-tuning and hardware profile features visible","level":2,"index":4,"id":"making-lab-tuning-and-hardware-profile-features-visible_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Creating a model registry for LAB-tuning","level":2,"index":5,"id":"creating-a-model-registry-for-lab-tuning_lab-tuning"},{"parentId":"enabling-lab-tuning_lab-tuning","name":"Creating a hardware profile for LAB-tuning","level":2,"index":6,"id":"creating-a-hardware-profile-for-lab-tuning_lab-tuning"},{"parentId":null,"name":"Overview of LAB-tuning","level":1,"index":1,"id":"overview-of-lab-tuning_lab-tuning"},{"parentId":"overview-of-lab-tuning_lab-tuning","name":"LAB-tuning workflow","level":2,"index":0,"id":"_lab_tuning_workflow"},{"parentId":"overview-of-lab-tuning_lab-tuning","name":"Model customization page","level":2,"index":1,"id":"_model_customization_page"},{"parentId":null,"name":"Preparing LAB-tuning resources","level":1,"index":2,"id":"preparing-lab-tuning-resources_lab-tuning"},{"parentId":"preparing-lab-tuning-resources_lab-tuning","name":"Creating a taxonomy","level":2,"index":0,"id":"creating-a-taxonomy_lab-tuning"},{"parentId":"preparing-lab-tuning-resources_lab-tuning","name":"Preparing a storage location for the LAB-tuned model","level":2,"index":1,"id":"preparing-a-storage-location-for-the-lab-tuned-model_lab-tuning"},{"parentId":"preparing-lab-tuning-resources_lab-tuning","name":"Creating a project for LAB-tuning","level":2,"index":2,"id":"creating-a-project-for-lab-tuning_lab-tuning"},{"parentId":"preparing-lab-tuning-resources_lab-tuning","name":"Deploying teacher and judge models","level":2,"index":3,"id":"deploying-teacher-and-judge-models_lab-tuning"},{"parentId":null,"name":"Using LAB-tuning","level":1,"index":3,"id":"using-lab-tuning_lab-tuning"},{"parentId":"using-lab-tuning_lab-tuning","name":"Registering a base model","level":2,"index":0,"id":"_registering_a_base_model"},{"parentId":"using-lab-tuning_lab-tuning","name":"Starting a LAB-tuning run from the registered model","level":2,"index":1,"id":"_starting_a_lab_tuning_run_from_the_registered_model"},{"parentId":"using-lab-tuning_lab-tuning","name":"Monitoring your LAB-tuning run","level":2,"index":2,"id":"_monitoring_your_lab_tuning_run"},{"parentId":"using-lab-tuning_lab-tuning","name":"Reviewing and deploying your LAB-tuned model","level":2,"index":3,"id":"_reviewing_and_deploying_your_lab_tuned_model"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":"logging-in_get-started","name":"Viewing installed Open Data Hub components","level":2,"index":0,"id":"viewing-installed-components_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding how Open Data Hub handles certificates","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates","level":2,"index":1,"id":"_adding_certificates"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a cluster-wide CA bundle","level":2,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a custom CA bundle","level":2,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":4,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":3,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for data science pipelines","level":3,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for workbenches","level":3,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using the cluster-wide CA bundle for the single-model serving platform","level":3,"index":3,"id":"using-the-cluster-CA-bundle-for-single-model-serving_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates without the Open Data Hub Operator","level":2,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":"working-with-certificates_certs","name":"Removing the CA bundle","level":2,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":3,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":3,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":5,"id":"viewing-logs-and-audit-records_install"},{"parentId":"viewing-logs-and-audit-records_install","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_install"},{"parentId":"configuring-the-operator-logger_install","name":"Viewing the Open Data Hub Operator log","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"viewing-logs-and-audit-records_install","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_install"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":3,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":3,"index":2,"id":"revoking-user-access-to-basic-workbenches_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":2,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration","level":2,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":3,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about available applications","level":2,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default basic workbench application","level":2,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-data-science-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"_enabling_accelerators"},{"parentId":"_enabling_accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"_enabling_accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":"_enabling_accelerators","name":"AMD GPU Integration","level":2,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":3,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":3,"index":1,"id":"enabling-amd-gpus_managing-odh"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":7,"id":"managing-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Overview of Kueue resources","level":2,"index":0,"id":"overview-of-kueue-resources_managing-odh"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Resource flavor","level":3,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Cluster queue","level":3,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Local queue","level":3,"index":2,"id":"_local_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Example Kueue resource configurations","level":2,"index":1,"id":"ref-example-kueue-resource-configurations_managing-odh"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs without shared cohort","level":3,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":3,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":4,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":4,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":4,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":4,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring quota management for distributed workloads","level":2,"index":2,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Enforcing the use of local queues","level":2,"index":3,"id":"enforcing-local-queues_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Enforcing the local-queue labeling policy for all projects","level":3,"index":0,"id":"enforcing-lqlabel-all_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Disabling the local-queue labeling policy for all projects","level":3,"index":1,"id":"disabling-lqlabel-all_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Enforcing the local-queue labeling policy for some projects only","level":3,"index":2,"id":"enforcing-lqlabel-some_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring the CodeFlare Operator","level":2,"index":4,"id":"configuring-the-codeflare-operator_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring a cluster for RDMA","level":2,"index":5,"id":"configuring-a-cluster-for-rdma_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":3,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":3,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":3,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":3,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"},{"parentId":null,"name":"Backing up data","level":1,"index":8,"id":"_backing_up_data"},{"parentId":"_backing_up_data","name":"Backing up storage data","level":2,"index":0,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_backing_up_data","name":"Backing up your cluster","level":2,"index":1,"id":"backing-up-your-cluster_managing-odh"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":9,"id":"viewing-logs-and-audit-records_managing-odh"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_managing-odh"},{"parentId":"configuring-the-operator-logger_managing-odh","name":"Viewing the Open Data Hub Operator log","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":1,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":2,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing connection types","level":1,"index":3,"id":"managing-connection-types"},{"parentId":"managing-connection-types","name":"Viewing connection types","level":2,"index":0,"id":"viewing-connection-types_managing-resources"},{"parentId":"managing-connection-types","name":"Creating a connection type","level":2,"index":1,"id":"creating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Duplicating a connection type","level":2,"index":2,"id":"duplicating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Editing a connection type","level":2,"index":3,"id":"editing-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Enabling a connection type","level":2,"index":4,"id":"enabling-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Deleting a connection type","level":2,"index":5,"id":"deleting-a-connection-type_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":4,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":0,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":2,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing basic workbenches","level":1,"index":5,"id":"managing-basic-workbenches"},{"parentId":"managing-basic-workbenches","name":"Accessing the administration interface for basic workbenches","level":2,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Starting basic workbenches owned by other users","level":2,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Accessing basic workbenches owned by other users","level":2,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping basic workbenches owned by other users","level":2,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping idle workbenches","level":2,"index":4,"id":"stopping-idle-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Adding workbench pod tolerations","level":2,"index":5,"id":"adding-workbench-pod-tolerations_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Troubleshooting common problems in workbenches for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user&#8217;s workbench does not start","level":3,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with standard model deployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing data drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":6,"id":"evaluating-large-language-models_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job","level":2,"index":1,"id":"lmeval-evaluation-job_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job properties","level":2,"index":2,"id":"lmeval-evaluation-job-properties_monitor"},{"parentId":"lmeval-evaluation-job-properties_monitor","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval scenarios","level":2,"index":3,"id":"lmeval-scenarios_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Configuring the LM-Eval environment","level":3,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":"lmeval-scenarios_monitor","name":"Using a custom Unitxt card","level":3,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":"lmeval-scenarios_monitor","name":"Using PVCs as storage","level":3,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_monitor","name":"Using an InferenceService","level":3,"index":3,"id":"_using_an_inferenceservice"},{"parentId":"lmeval-scenarios_monitor","name":"Setting up LMEval S3 Support","level":3,"index":4,"id":"_setting_up_lmeval_s3_support"},{"parentId":null,"name":"Configuring the Guardrails Orchestrator service","level":1,"index":7,"id":"configuring-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Deploying the Guardrails Orchestrator service","level":2,"index":0,"id":"deploying-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Guardrails Orchestrator parameters","level":2,"index":1,"id":"guardrails-orchestrator-parameters_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Monitoring user inputs with the Guardrails Orchestrator service","level":2,"index":2,"id":"guardrails-orchestrator-hap-scenario_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Configuring the regex detector and guardrails gateway","level":2,"index":3,"id":"configuring-regex-guardrails-gateway_monitor"},{"parentId":"configuring-regex-guardrails-gateway_monitor","name":"Sending requests to the regex detector","level":3,"index":0,"id":"sending-requests-to-the-regex-detector_monitor"},{"parentId":"configuring-regex-guardrails-gateway_monitor","name":"Querying using guardrails gateway","level":3,"index":1,"id":"querying-using-guardrails-gateway_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Configuring the OpenTelemetry exporter","level":2,"index":4,"id":"configuring-the-opentelemetry-exporter_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Using Hugging Face models with Guardrails Orchestrator","level":2,"index":5,"id":"using-hugging-face-models-with-guardrails-orchestrator_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Configuring the Guardrails Detector Hugging Face serving runtime","level":2,"index":6,"id":"configuring-the-guardrails-detector-hugging-face-serving-runtime_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Using a Hugging Face Prompt Injection detector with the Guardrails Orchestrator","level":2,"index":7,"id":"using-a-hugging-face-prompt-injection-detector-with-guardrails-orchestrator_monitor"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":8,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":1,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Requirements for upgrading Open Data Hub version 2","level":2,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":3,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Requirements for upgrading Open Data Hub version 1","level":2,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":1,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":"about-model-serving_about-model-serving","name":"Single-model serving platform","level":2,"index":0,"id":"_single_model_serving_platform"},{"parentId":"about-model-serving_about-model-serving","name":"Multi-model serving platform","level":2,"index":1,"id":"_multi_model_serving_platform"},{"parentId":"about-model-serving_about-model-serving","name":"NVIDIA NIM model serving platform","level":2,"index":2,"id":"_nvidia_nim_model_serving_platform"},{"parentId":null,"name":"Serving models on the single-model serving platform","level":1,"index":1,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Model-serving runtimes","level":2,"index":1,"id":"model-serving-runtimes_serving-large-models"},{"parentId":"model-serving-runtimes_serving-large-models","name":"ServingRuntime","level":3,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_serving-large-models","name":"InferenceService","level":3,"index":1,"id":"_inferenceservice"},{"parentId":"serving-large-models_serving-large-models","name":"About KServe deployment modes","level":2,"index":2,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Advanced mode","level":3,"index":0,"id":"_advanced_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Standard mode","level":3,"index":1,"id":"_standard_mode"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":3,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":4,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models by using multiple GPU nodes","level":3,"index":4,"id":"deploying-models-using-multiple-gpu-nodes_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Setting a timeout for KServe","level":3,"index":5,"id":"setting-timeout-for-kserve_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizing the parameters of a deployed model-serving runtime","level":3,"index":6,"id":"customizing-parameters-serving-runtime_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizable model serving runtime parameters","level":3,"index":7,"id":"customizable-model-serving-runtime-parameters_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using accelerators with vLLM","level":3,"index":8,"id":"using-accelerators-with-vllm_serving-large-models"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"NVIDIA GPUs","level":4,"index":0,"id":"_nvidia_gpus"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"Intel Gaudi accelerators","level":4,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"AMD GPUs","level":4,"index":2,"id":"_amd_gpus"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":3,"index":9,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Storing a model in an OCI image","level":4,"index":0,"id":"storing-a-model-in-oci-image_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image by using the CLI","level":4,"index":1,"id":"deploying-model-stored-in-oci-image_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the authentication token for a deployed model","level":3,"index":10,"id":"accessing-authentication-token-for-deployed-model_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":11,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":5,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":6,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Monitoring model performance","level":2,"index":7,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a Grafana metrics dashboard","level":3,"index":1,"id":"Deploying-a-grafana-metrics-dashboard_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":3,"index":2,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Grafana metrics","level":3,"index":3,"id":"ref-grafana-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"Accelerator metrics","level":4,"index":0,"id":"ref-accelerator-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"CPU metrics","level":4,"index":1,"id":"ref-cpu-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"vLLM metrics","level":4,"index":2,"id":"ref-vllm-metrics_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Optimizing model-serving runtimes","level":2,"index":8,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Enabling speculative decoding and multi-modal inferencing","level":3,"index":0,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":9,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Supported model-serving runtimes","level":2,"index":10,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Tested and verified model-serving runtimes","level":2,"index":11,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Inference endpoints","level":2,"index":12,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":3,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":3,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":3,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":3,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM AMD GPU ServingRuntime for KServe","level":3,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":3,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":3,"index":8,"id":"_additional_resources"},{"parentId":"serving-large-models_serving-large-models","name":"About the NVIDIA NIM model serving platform","level":2,"index":13,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":3,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":3,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":3,"index":2,"id":"Customizing-model-selection-options_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":3,"index":3,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling graph generation for an existing NIM deployment","level":4,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling metrics collection for an existing NIM deployment","level":4,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing NVIDIA NIM metrics for a NIM model","level":3,"index":4,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing performance metrics for a NIM model","level":3,"index":5,"id":"viewing-performance-metrics-for-a-nim-model_serving-large-models"},{"parentId":null,"name":"Serving models on the multi-model serving platform","level":1,"index":2,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance_2"},{"parentId":"_monitoring_model_performance_2","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance_2","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing Jupyter notebooks","level":2,"index":0,"id":"creating-and-importing-jupyter-notebooks_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":1,"id":"collaborating-on-jupyter-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in workbenches for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-workbenches-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"_working_in_code_server"},{"parentId":"_working_in_code_server","name":"Creating code-server workbenches","level":2,"index":0,"id":"creating-code-server-workbenches_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Creating a workbench","level":3,"index":0,"id":"creating-a-project-workbench_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Uploading an existing notebook file to code-server from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_ide"},{"parentId":"_working_in_code_server","name":"Collaborating on workbenches in code-server by using Git","level":2,"index":1,"id":"collaborating-on-workbenches-in-code-server-by-using-git_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using code-server","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Updating your project in code-server with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Pushing project changes in code-server to a Git repository","level":3,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_ide"},{"parentId":"_working_in_code_server","name":"Managing Python packages in code-server","level":2,"index":2,"id":"managing-python-packages-in-code-server_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Viewing Python packages installed on your code-server workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Installing Python packages on your code-server workbench","level":3,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_ide"},{"parentId":"_working_in_code_server","name":"Installing extensions with code-server","level":2,"index":3,"id":"installing-extensions-with-code-server_ide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using connections","level":1,"index":2,"id":"using-connections_projects"},{"parentId":"using-connections_projects","name":"Adding a connection to your data science project","level":2,"index":0,"id":"adding-a-connection-to-your-data-science-project_projects"},{"parentId":"using-connections_projects","name":"Updating a connection","level":2,"index":1,"id":"updating-a-connection_projects"},{"parentId":"using-connections_projects","name":"Deleting a connection","level":2,"index":2,"id":"deleting-a-connection_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Configuring access to a data science project","level":2,"index":0,"id":"configuring-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":1,"id":"enabling-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":2,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":3,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":4,"id":"amd-gpu-integration_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_accelerators"},{"parentId":null,"name":"Working with hardware profiles","level":1,"index":5,"id":"working-with-hardware-profiles_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Creating a hardware profile","level":2,"index":0,"id":"creating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Updating a hardware profile","level":2,"index":1,"id":"updating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Deleting a hardware profile","level":2,"index":2,"id":"deleting-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for workbench images","level":2,"index":3,"id":"configuring-a-recommended-accelerator-for-workbench-images_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"About GPU time slicing","level":1,"index":6,"id":"about-gpu-time-slicing_accelerators"},{"parentId":null,"name":"Enabling GPU time slicing","level":1,"index":7,"id":"enabling-gpu-time-slicing_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using basic workbenches","level":1,"index":3,"id":"_using_basic_workbenches"},{"parentId":"_using_basic_workbenches","name":"Starting a basic workbench","level":2,"index":0,"id":"starting-a-basic-workbench_connected-apps"},{"parentId":"_using_basic_workbenches","name":"Creating and importing Jupyter notebooks","level":2,"index":1,"id":"creating-and-importing-jupyter-notebooks_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_using_basic_workbenches","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":2,"id":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"_using_basic_workbenches","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_connected-apps"},{"parentId":"_using_basic_workbenches","name":"Updating workbench settings by restarting your workbench","level":2,"index":4,"id":"updating-workbench-settings-by-restarting-your-workbench_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":1,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Managing data science pipelines","level":1,"index":0,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"configuring-a-pipeline-server_ds-pipelines","name":"Configuring a pipeline server with an external Amazon RDS database","level":3,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of data science pipelines caching","level":2,"index":12,"id":"overview-of-data-science-pipelines-caching_ds-pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Disabling caching for specific tasks or pipelines","level":3,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":4,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Verification and troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":1,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in an experiment","level":2,"index":7,"id":"comparing-runs-in-an-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in different experiments","level":2,"index":8,"id":"comparing-runs-in-different-experiments_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":5,"id":"troubleshooting-dspa-component-errors_ds-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ds-pipelines","name":"Common errors across DSP components","level":2,"index":0,"id":"_common_errors_across_dsp_components"},{"parentId":null,"name":"Migrating to data science pipelines 2.0","level":1,"index":6,"id":"migrating-to-data-science-pipelines-2_ds-pipelines"},{"parentId":"migrating-to-data-science-pipelines-2_ds-pipelines","name":"Upgrading to data science pipelines 2.0","level":2,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":"migrating-to-data-science-pipelines-2_ds-pipelines","name":"Removing data science pipelines 1.0 resources","level":2,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"},{"parentId":null,"name":"Additional resources","level":1,"index":7,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Distributed workloads infrastructure","level":2,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Types of distributed workloads","level":2,"index":1,"id":"_types_of_distributed_workloads"},{"parentId":null,"name":"Preparing the distributed training environment","level":1,"index":1,"id":"preparing-the-distributed-training-environment_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Creating a workbench for distributed training","level":2,"index":0,"id":"creating-a-workbench-for-distributed-training_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Using the cluster server and token to authenticate","level":2,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Managing custom training images","level":2,"index":2,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":3,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":3,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":3,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running Ray-based distributed workloads","level":1,"index":2,"id":"running-ray-based-distributed-workloads_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from Jupyter notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Running Training Operator-based distributed training workloads","level":1,"index":3,"id":"running-kfto-based-distributed-training-workloads_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Kubeflow Training Operator to run distributed training workloads","level":2,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":3,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource","level":3,"index":1,"id":"creating-a-kfto-pytorchjob-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":3,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorch training scripts","level":3,"index":3,"id":"example-kfto-pytorch-training-scripts_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: NCCL","level":4,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccldistributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: DDP","level":4,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: FSDP","level":4,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Dockerfile for a Training Operator PyTorch training script","level":3,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource for multi-node training","level":3,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Training Operator SDK to run distributed training workloads","level":2,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Configuring a training job by using the Training Operator SDK","level":3,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Running a training job by using the Training Operator SDK","level":3,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"TrainingClient API: Job-related methods","level":3,"index":2,"id":"ref-trainingclient-api-job-related-methods_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Fine-tuning a model by using Kubeflow Training","level":2,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Configuring the fine-tuning job","level":3,"index":0,"id":"configuring-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Running the fine-tuning job","level":3,"index":1,"id":"running-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Deleting the fine-tuning job","level":3,"index":2,"id":"deleting-the-fine-tuning-job_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Creating a multi-node PyTorch training job with RDMA","level":2,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":2,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":4,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster does not start","level":2,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"Default Local Queue not found\" error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"local_queue provided does not exist\" error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of model registries","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":null,"name":"Configuring the model registry component","level":0,"index":1,"id":"_configuring_the_model_registry_component"},{"parentId":"_configuring_the_model_registry_component","name":"Configuring the model registry component","level":1,"index":0,"id":"configuring-the-model-registry-component_model-registry"},{"parentId":"_configuring_the_model_registry_component","name":"Enabling the model catalog","level":1,"index":1,"id":"enabling-the-model-catalog_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Editing a model registry","level":1,"index":1,"id":"editing-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":2,"id":"managing-model-registry-permissions_model-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":3,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model from the model catalog","level":2,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":2,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":3,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":4,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":5,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":6,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":7,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model from the model catalog","level":2,"index":8,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing the deployment properties of a deployed model version from a model registry","level":2,"index":9,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":3,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":3,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deleting a deployed model version from a model registry","level":2,"index":10,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":11,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":12,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":13,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":14,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-jupyter-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-workbenches-in-code-server-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using code-server","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project in code-server with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes in code-server to a Git repository","level":1,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":1,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-guardrails-orchestrator-service/"},"sections":[{"parentId":null,"name":"Deploying the Guardrails Orchestrator service","level":1,"index":0,"id":"deploying-the-guardrails-orchestrator-service_{context}"},{"parentId":null,"name":"Guardrails Orchestrator parameters","level":1,"index":1,"id":"guardrails-orchestrator-parameters_{context}"},{"parentId":null,"name":"Monitoring user inputs with the Guardrails Orchestrator service","level":1,"index":2,"id":"guardrails-orchestrator-hap-scenario_{context}"},{"parentId":null,"name":"Configuring the regex detector and guardrails gateway","level":1,"index":3,"id":"configuring-regex-guardrails-gateway_{context}"},{"parentId":"configuring-regex-guardrails-gateway_{context}","name":"Sending requests to the regex detector","level":2,"index":0,"id":"sending-requests-to-the-regex-detector_{context}"},{"parentId":"configuring-regex-guardrails-gateway_{context}","name":"Querying using guardrails gateway","level":2,"index":1,"id":"querying-using-guardrails-gateway_{context}"},{"parentId":null,"name":"Configuring the OpenTelemetry exporter","level":1,"index":4,"id":"configuring-the-opentelemetry-exporter_{context}"},{"parentId":null,"name":"Using Hugging Face models with Guardrails Orchestrator","level":1,"index":5,"id":"using-hugging-face-models-with-guardrails-orchestrator_{context}"},{"parentId":null,"name":"Configuring the Guardrails Detector Hugging Face serving runtime","level":1,"index":6,"id":"configuring-the-guardrails-detector-hugging-face-serving-runtime_{context}"},{"parentId":null,"name":"Using a Hugging Face Prompt Injection detector with the Guardrails Orchestrator","level":1,"index":7,"id":"using-a-hugging-face-prompt-injection-detector-with-guardrails-orchestrator_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for your model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"},{"parentId":null,"name":"Enabling TrustyAI Integration with standard model deployment","level":1,"index":4,"id":"enabling-trustyai-kserve-integration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-jupyter-notebooks/"},"sections":[{"parentId":null,"name":"Creating a Jupyter notebook","level":1,"index":0,"id":"creating-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-code-server-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench","level":1,"index":0,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration","level":1,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-your-feature-store-configuration/"},"sections":[{"parentId":null,"name":"Specifying to use a feature project from a Git repository","level":1,"index":0,"id":"specifying-to-use-a-feature-project-from-git-repository_{context}"},{"parentId":null,"name":"Configuring an offline store","level":1,"index":1,"id":"configuring-an-offline-store_{context}"},{"parentId":null,"name":"Configuring an online store","level":1,"index":2,"id":"configuring-an-online-store_{context}"},{"parentId":null,"name":"Configuring the feature registry","level":1,"index":3,"id":"configuring-the-feature-registry_{context}"},{"parentId":null,"name":"Configuring role-based access control","level":1,"index":4,"id":"configuring-role-based-access-control_{context}"},{"parentId":"configuring-role-based-access-control_{context}","name":"Default authorization configuration","level":2,"index":0,"id":"ref-default-authorization-configuration_{context}"},{"parentId":"configuring-role-based-access-control_{context}","name":"Example OIDC Authorization configuration","level":2,"index":1,"id":"ref-example-oidc-authorization-configuration_{context}"},{"parentId":"configuring-role-based-access-control_{context}","name":"Example Kubernetes Authorization configuration","level":2,"index":2,"id":"ref-example-kubernetes-authorization-configuration_{context}"},{"parentId":null,"name":"Editing an existing feature store instance","level":1,"index":5,"id":"editing-an-existing-feature-store-instance_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-lab-tuning/"},"sections":[{"parentId":null,"name":"Overview of enabling LAB-tuning","level":1,"index":0,"id":"overview-of-enabling-lab-tuning_{context}"},{"parentId":"overview-of-enabling-lab-tuning_{context}","name":"Requirements for LAB-tuning","level":2,"index":0,"id":"_requirements_for_lab_tuning"},{"parentId":null,"name":"Installing the required Operators for LAB-tuning","level":1,"index":1,"id":"installing-the-required-operators-for-lab-tuning_{context}"},{"parentId":null,"name":"Installing the required components for LAB-tuning","level":1,"index":2,"id":"installing-the-required-components-for-lab-tuning_{context}"},{"parentId":null,"name":"Configuring a storage class for LAB-tuning","level":1,"index":3,"id":"configuring-a-storage-class-for-lab-tuning_{context}"},{"parentId":null,"name":"Making LAB-tuning and hardware profile features visible","level":1,"index":4,"id":"making-lab-tuning-and-hardware-profile-features-visible_{context}"},{"parentId":null,"name":"Creating a model registry for LAB-tuning","level":1,"index":5,"id":"creating-a-model-registry-for-lab-tuning_{context}"},{"parentId":null,"name":"Creating a hardware profile for LAB-tuning","level":1,"index":6,"id":"creating-a-hardware-profile-for-lab-tuning_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enforcing-local-queues/"},"sections":[{"parentId":null,"name":"Enforcing the local-queue labeling policy for all projects","level":1,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":null,"name":"Disabling the local-queue labeling policy for all projects","level":1,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":null,"name":"Enforcing the local-queue labeling policy for some projects only","level":1,"index":2,"id":"enforcing-lqlabel-some_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-large-language-models/"},"sections":[{"parentId":null,"name":"Setting up LM-Eval","level":1,"index":0,"id":"setting-up-lmeval_{context}"},{"parentId":null,"name":"LM-Eval evaluation job","level":1,"index":1,"id":"lmeval-evaluation-job_{context}"},{"parentId":null,"name":"LM-Eval evaluation job properties","level":1,"index":2,"id":"lmeval-evaluation-job-properties_{context}"},{"parentId":"lmeval-evaluation-job-properties_{context}","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":2,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":null,"name":"LM-Eval scenarios","level":1,"index":3,"id":"lmeval-scenarios_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Configuring the LM-Eval environment","level":2,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":"lmeval-scenarios_{context}","name":"Using a custom Unitxt card","level":2,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":"lmeval-scenarios_{context}","name":"Using PVCs as storage","level":2,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":3,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":3,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_{context}","name":"Using an InferenceService","level":2,"index":3,"id":"_using_an_inferenceservice"},{"parentId":"lmeval-scenarios_{context}","name":"Setting up LMEval S3 Support","level":2,"index":4,"id":"_setting_up_lmeval_s3_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/example-kfto-pytorch-training-scripts/"},"sections":[{"parentId":null,"name":"Example Training Operator PyTorch training script: NCCL","level":1,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: DDP","level":1,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: FSDP","level":1,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/fine-tuning-a-model-by-using-kubeflow-training/"},"sections":[{"parentId":null,"name":"Configuring the fine-tuning job","level":1,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Running the fine-tuning job","level":1,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Deleting the fine-tuning job","level":1,"index":2,"id":"deleting-the-fine-tuning-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Configuring custom namespaces","level":1,"index":0,"id":"configuring-custom-namespaces"},{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Configuring access to a data science project","level":1,"index":0,"id":"configuring-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Sharing access to a data science project","level":1,"index":1,"id":"sharing-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":2,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":3,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about available applications","level":1,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":null,"name":"Hiding the default basic workbench application","level":1,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-basic-workbenches/"},"sections":[{"parentId":null,"name":"Accessing the administration interface for basic workbenches","level":1,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_{context}"},{"parentId":null,"name":"Starting basic workbenches owned by other users","level":1,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing basic workbenches owned by other users","level":1,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping basic workbenches owned by other users","level":1,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle workbenches","level":1,"index":4,"id":"stopping-idle-workbenches_{context}"},{"parentId":null,"name":"Adding workbench pod tolerations","level":1,"index":5,"id":"adding-workbench-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in workbenches for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user&#8217;s workbench does not start","level":2,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-connection-types/"},"sections":[{"parentId":null,"name":"Viewing connection types","level":1,"index":0,"id":"viewing-connection-types_{context}"},{"parentId":null,"name":"Creating a connection type","level":1,"index":1,"id":"creating-a-connection-type_{context}"},{"parentId":null,"name":"Duplicating a connection type","level":1,"index":2,"id":"duplicating-a-connection-type_{context}"},{"parentId":null,"name":"Editing a connection type","level":1,"index":3,"id":"editing-a-connection-type_{context}"},{"parentId":null,"name":"Enabling a connection type","level":1,"index":4,"id":"enabling-a-connection-type_{context}"},{"parentId":null,"name":"Deleting a connection type","level":1,"index":5,"id":"deleting-a-connection-type_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":"configuring-a-pipeline-server_{context}","name":"Configuring a pipeline server with an external Amazon RDS database","level":2,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"},{"parentId":null,"name":"Overview of data science pipelines caching","level":1,"index":12,"id":"overview-of-data-science-pipelines-caching_{context}"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Disabling caching for specific tasks or pipelines","level":2,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":3,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Verification and troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of Kueue resources","level":1,"index":0,"id":"overview-of-kueue-resources_{context}"},{"parentId":"overview-of-kueue-resources_{context}","name":"Resource flavor","level":2,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_{context}","name":"Cluster queue","level":2,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_{context}","name":"Local queue","level":2,"index":2,"id":"_local_queue"},{"parentId":null,"name":"Example Kueue resource configurations","level":1,"index":1,"id":"ref-example-kueue-resource-configurations_{context}"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs without shared cohort","level":2,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":3,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":3,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":3,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":3,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":2,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":3,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":3,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":3,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":3,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":2,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Enforcing the use of local queues","level":1,"index":3,"id":"enforcing-local-queues_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Enforcing the local-queue labeling policy for all projects","level":2,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Disabling the local-queue labeling policy for all projects","level":2,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Enforcing the local-queue labeling policy for some projects only","level":2,"index":2,"id":"enforcing-lqlabel-some_{context}"},{"parentId":null,"name":"Configuring the CodeFlare Operator","level":1,"index":4,"id":"configuring-the-codeflare-operator_{context}"},{"parentId":null,"name":"Configuring a cluster for RDMA","level":1,"index":5,"id":"configuring-a-cluster-for-rdma_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":2,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":2,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":2,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs in an experiment","level":1,"index":7,"id":"comparing-runs-in-an-experiment_{context}"},{"parentId":null,"name":"Comparing runs in different experiments","level":1,"index":8,"id":"comparing-runs-in-different-experiments_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages-in-code-server/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your code-server workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your code-server workbench","level":1,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your workbench","level":1,"index":1,"id":"installing-python-packages-on-your-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"Configuring storage class settings","level":1,"index":0,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":2,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":2,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":2,"index":2,"id":"revoking-user-access-to-basic-workbenches_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing data drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using drift metrics","level":1,"index":3,"id":"using-drift-metrics_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Using bias metrics","level":1,"index":3,"id":"using-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/preparing-lab-tuning-resources/"},"sections":[{"parentId":null,"name":"Creating a taxonomy","level":1,"index":0,"id":"creating-a-taxonomy_{context}"},{"parentId":null,"name":"Preparing a storage location for the LAB-tuned model","level":1,"index":1,"id":"preparing-a-storage-location-for-the-lab-tuned-model_{context}"},{"parentId":null,"name":"Creating a project for LAB-tuning","level":1,"index":2,"id":"creating-a-project-for-lab-tuning_{context}"},{"parentId":null,"name":"Deploying teacher and judge models","level":1,"index":3,"id":"deploying-teacher-and-judge-models_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/preparing-the-distributed-training-environment/"},"sections":[{"parentId":null,"name":"Creating a workbench for distributed training","level":1,"index":0,"id":"creating-a-workbench-for-distributed-training_{context}"},{"parentId":null,"name":"Using the cluster server and token to authenticate","level":1,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_{context}"},{"parentId":null,"name":"Managing custom training images","level":1,"index":2,"id":"managing-custom-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-kfto-based-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Using the Kubeflow Training Operator to run distributed training workloads","level":1,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":2,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource","level":2,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":2,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorch training scripts","level":2,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":3,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":3,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":3,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Dockerfile for a Training Operator PyTorch training script","level":2,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorchJob resource for multi-node training","level":2,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"},{"parentId":null,"name":"Using the Training Operator SDK to run distributed training workloads","level":1,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Configuring a training job by using the Training Operator SDK","level":2,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Running a training job by using the Training Operator SDK","level":2,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"TrainingClient API: Job-related methods","level":2,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"},{"parentId":null,"name":"Fine-tuning a model by using Kubeflow Training","level":1,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Configuring the fine-tuning job","level":2,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Running the fine-tuning job","level":2,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Deleting the fine-tuning job","level":2,"index":2,"id":"deleting-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Creating a multi-node PyTorch training job with RDMA","level":1,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":1,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-ray-based-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from Jupyter notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Model-serving runtimes","level":1,"index":1,"id":"model-serving-runtimes_serving-large-models"},{"parentId":"model-serving-runtimes_serving-large-models","name":"ServingRuntime","level":2,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_serving-large-models","name":"InferenceService","level":2,"index":1,"id":"_inferenceservice"},{"parentId":null,"name":"About KServe deployment modes","level":1,"index":2,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Advanced mode","level":2,"index":0,"id":"_advanced_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Standard mode","level":2,"index":1,"id":"_standard_mode"},{"parentId":null,"name":"Installing KServe","level":1,"index":3,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":4,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models by using multiple GPU nodes","level":2,"index":4,"id":"deploying-models-using-multiple-gpu-nodes_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Setting a timeout for KServe","level":2,"index":5,"id":"setting-timeout-for-kserve_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizing the parameters of a deployed model-serving runtime","level":2,"index":6,"id":"customizing-parameters-serving-runtime_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizable model serving runtime parameters","level":2,"index":7,"id":"customizable-model-serving-runtime-parameters_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using accelerators with vLLM","level":2,"index":8,"id":"using-accelerators-with-vllm_serving-large-models"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"Intel Gaudi accelerators","level":3,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"AMD GPUs","level":3,"index":2,"id":"_amd_gpus"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":2,"index":9,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Storing a model in an OCI image","level":3,"index":0,"id":"storing-a-model-in-oci-image_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image by using the CLI","level":3,"index":1,"id":"deploying-model-stored-in-oci-image_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the authentication token for a deployed model","level":2,"index":10,"id":"accessing-authentication-token-for-deployed-model_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":11,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":5,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":6,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":7,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a Grafana metrics dashboard","level":2,"index":1,"id":"Deploying-a-grafana-metrics-dashboard_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":2,"index":2,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Grafana metrics","level":2,"index":3,"id":"ref-grafana-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"Accelerator metrics","level":3,"index":0,"id":"ref-accelerator-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"CPU metrics","level":3,"index":1,"id":"ref-cpu-metrics_serving-large-models"},{"parentId":"ref-grafana-metrics_serving-large-models","name":"vLLM metrics","level":3,"index":2,"id":"ref-vllm-metrics_serving-large-models"},{"parentId":null,"name":"Optimizing model-serving runtimes","level":1,"index":8,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Enabling speculative decoding and multi-modal inferencing","level":2,"index":0,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":9,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Supported model-serving runtimes","level":1,"index":10,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":null,"name":"Tested and verified model-serving runtimes","level":1,"index":11,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":null,"name":"Inference endpoints","level":1,"index":12,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":2,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":2,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":2,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":2,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM AMD GPU ServingRuntime for KServe","level":2,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":2,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":2,"index":8,"id":"_additional_resources"},{"parentId":null,"name":"About the NVIDIA NIM model serving platform","level":1,"index":13,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":2,"index":2,"id":"Customizing-model-selection-options_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":2,"index":3,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling graph generation for an existing NIM deployment","level":3,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling metrics collection for an existing NIM deployment","level":3,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing NVIDIA NIM metrics for a NIM model","level":2,"index":4,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing performance metrics for a NIM model","level":2,"index":5,"id":"viewing-performance-metrics-for-a-nim-model_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Uploading training data to TrustyAI","level":1,"index":1,"id":"uploading-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":2,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":3,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 1","level":1,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":1,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 2","level":1,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-connections/"},"sections":[{"parentId":null,"name":"Adding a connection to your data science project","level":1,"index":0,"id":"adding-a-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating a connection","level":1,"index":1,"id":"updating-a-connection_{context}"},{"parentId":null,"name":"Deleting a connection","level":1,"index":2,"id":"deleting-a-connection_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Using explainers","level":1,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kfto-sdk-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Configuring a training job by using the Training Operator SDK","level":1,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"Running a training job by using the Training Operator SDK","level":1,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"TrainingClient API: Job-related methods","level":1,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kubeflow-training-operator-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":1,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource","level":1,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":1,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training scripts","level":1,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":2,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":2,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":2,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":null,"name":"Example Dockerfile for a Training Operator PyTorch training script","level":1,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource for multi-node training","level":1,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/viewing-logs-and-audit-records/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_{context}"},{"parentId":"configuring-the-operator-logger_{context}","name":"Viewing the {productname-short} Operator log","level":2,"index":0,"id":"_viewing_the_productname_short_operator_log"},{"parentId":null,"name":"Viewing audit records","level":1,"index":1,"id":"viewing-audit-records_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding how {productname-short} handles certificates","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":null,"name":"Adding certificates","level":1,"index":1,"id":"_adding_certificates"},{"parentId":null,"name":"Adding certificates to a cluster-wide CA bundle","level":1,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":null,"name":"Adding certificates to a custom CA bundle","level":1,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":4,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":2,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for data science pipelines","level":2,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for workbenches","level":2,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using the cluster-wide CA bundle for the single-model serving platform","level":2,"index":3,"id":"using-the-cluster-CA-bundle-for-single-model-serving_certs"},{"parentId":null,"name":"Managing certificates without the {productname-long} Operator","level":1,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":null,"name":"Removing the CA bundle","level":1,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":2,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":2,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":0,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model from the model catalog","level":1,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":2,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":3,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":4,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":5,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":6,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":7,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model from the model catalog","level":1,"index":8,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Editing the deployment properties of a deployed model version from a model registry","level":1,"index":9,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":2,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":2,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":null,"name":"Deleting a deployed model version from a model registry","level":1,"index":10,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":11,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":12,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":13,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":14,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":2,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":3,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":4,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":5,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":6,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Advanced mode","level":1,"index":0,"id":"_advanced_mode"},{"parentId":null,"name":"Standard mode","level":1,"index":1,"id":"_standard_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-storage-class-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-guardrails-detector-hugging-face-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-hardware-profile-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-taxonomy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-feature-store-instance-in-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-teacher-and-judge-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-configuring-regex-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-hap-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-querying-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-regex-using/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-required-components-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-required-operators-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Configuring the LM-Eval environment","level":1,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using an InferenceService","level":1,"index":3,"id":"_using_an_inferenceservice"},{"parentId":null,"name":"Setting up LMEval S3 Support","level":1,"index":4,"id":"_setting_up_lmeval_s3_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-lab-tuning-and-hardware-profiles-features-visible/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-to-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Removing data science pipelines 1.0 resources","level":1,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-enabling-lab-tuning/"},"sections":[{"parentId":null,"name":"Requirements for LAB-tuning","level":1,"index":0,"id":"_requirements_for_lab_tuning"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-features-and-feature-store/"},"sections":[{"parentId":null,"name":"Overview of machine learning features","level":1,"index":0,"id":"_overview_of_machine_learning_features"},{"parentId":null,"name":"Overview of Feature Store","level":1,"index":1,"id":"_overview_of_feature_store"},{"parentId":null,"name":"Audience for Feature Store","level":1,"index":2,"id":"_audience_for_feature_store"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-lab-tuning/"},"sections":[{"parentId":null,"name":"LAB-tuning workflow","level":1,"index":0,"id":"_lab_tuning_workflow"},{"parentId":null,"name":"Model customization page","level":1,"index":1,"id":"_model_customization_page"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preparing-a-storage-location-for-the-lab-tuned-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-PVC-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-to-use-a-feature-project-from-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-accelerators-with-vllm/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-hugging-face-models-with-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-lab-tuning/"},"sections":[{"parentId":null,"name":"Registering a base model","level":1,"index":0,"id":"_registering_a_base_model"},{"parentId":null,"name":"Starting a LAB-tuning run from the registered model","level":1,"index":1,"id":"_starting_a_lab_tuning_run_from_the_registered_model"},{"parentId":null,"name":"Monitoring your LAB-tuning run","level":1,"index":2,"id":"_monitoring_your_lab_tuning_run"},{"parentId":null,"name":"Reviewing and deploying your LAB-tuned model","level":1,"index":3,"id":"_reviewing_and_deploying_your_lab_tuned_model"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-CA-bundle-for-single-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Advanced mode","level":1,"index":0,"id":"_advanced_mode"},{"parentId":null,"name":"Standard mode","level":1,"index":1,"id":"_standard_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-storage-class-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-hugging-face-prompt-injection-detector-with-the-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-guardrails-detector-hugging-face-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-hardware-profile-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-taxonomy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-feature-store-instance-in-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-teacher-and-judge-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-configuring-regex-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-hap-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-querying-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-regex-using/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-required-components-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-required-operators-for-lab-tuning/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Configuring the LM-Eval environment","level":1,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using an InferenceService","level":1,"index":3,"id":"_using_an_inferenceservice"},{"parentId":null,"name":"Setting up LMEval S3 Support","level":1,"index":4,"id":"_setting_up_lmeval_s3_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-lab-tuning-and-hardware-profiles-features-visible/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-to-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Removing data science pipelines 1.0 resources","level":1,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-enabling-lab-tuning/"},"sections":[{"parentId":null,"name":"Requirements for LAB-tuning","level":1,"index":0,"id":"_requirements_for_lab_tuning"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-features-and-feature-store/"},"sections":[{"parentId":null,"name":"Overview of machine learning features","level":1,"index":0,"id":"_overview_of_machine_learning_features"},{"parentId":null,"name":"Overview of Feature Store","level":1,"index":1,"id":"_overview_of_feature_store"},{"parentId":null,"name":"Audience for Feature Store","level":1,"index":2,"id":"_audience_for_feature_store"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-lab-tuning/"},"sections":[{"parentId":null,"name":"LAB-tuning workflow","level":1,"index":0,"id":"_lab_tuning_workflow"},{"parentId":null,"name":"Model customization page","level":1,"index":1,"id":"_model_customization_page"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preparing-a-storage-location-for-the-lab-tuned-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-PVC-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-to-use-a-feature-project-from-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-hugging-face-prompt-injection-detector-with-the-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-accelerators-with-vllm/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-lab-tuning/"},"sections":[{"parentId":null,"name":"Registering a base model","level":1,"index":0,"id":"_registering_a_base_model"},{"parentId":null,"name":"Starting a LAB-tuning run from the registered model","level":1,"index":1,"id":"_starting_a_lab_tuning_run_from_the_registered_model"},{"parentId":null,"name":"Monitoring your LAB-tuning run","level":1,"index":2,"id":"_monitoring_your_lab_tuning_run"},{"parentId":null,"name":"Reviewing and deploying your LAB-tuned model","level":1,"index":3,"id":"_reviewing_and_deploying_your_lab_tuned_model"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-hugging-face-models-with-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-CA-bundle-for-single-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#about-model-serving_about-model-serving\">About model serving</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_single_model_serving_platform\">Single-model serving platform</a></li>\n<li><a href=\"#_multi_model_serving_platform\">Multi-model serving platform</a></li>\n<li><a href=\"#_nvidia_nim_model_serving_platform\">NVIDIA NIM model serving platform</a></li>\n</ul>\n</li>\n<li><a href=\"#serving-large-models_serving-large-models\">Serving models on the single-model serving platform</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#about-the-single-model-serving-platform_serving-large-models\">About the single-model serving platform</a></li>\n<li><a href=\"#model-serving-runtimes_serving-large-models\">Model-serving runtimes</a></li>\n<li><a href=\"#about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</a></li>\n<li><a href=\"#installing-kserve_serving-large-models\">Installing KServe</a></li>\n<li><a href=\"#deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models by using the single-model serving platform</a></li>\n<li><a href=\"#configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</a></li>\n<li><a href=\"#viewing-metrics-for-the-single-model-serving-platform_serving-large-models\">Viewing model-serving runtime metrics for the single-model serving platform</a></li>\n<li><a href=\"#_monitoring_model_performance\">Monitoring model performance</a></li>\n<li><a href=\"#_optimizing_model_serving_runtimes\">Optimizing model-serving runtimes</a></li>\n<li><a href=\"#_performance_tuning_on_the_single_model_serving_platform\">Performance tuning on the single-model serving platform</a></li>\n<li><a href=\"#supported-model-serving-runtimes_serving-large-models\">Supported model-serving runtimes</a></li>\n<li><a href=\"#tested-verified-runtimes_serving-large-models\">Tested and verified model-serving runtimes</a></li>\n<li><a href=\"#inference-endpoints_serving-large-models\">Inference endpoints</a></li>\n<li><a href=\"#about-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">About the NVIDIA NIM model serving platform</a></li>\n</ul>\n</li>\n<li><a href=\"#serving-small-and-medium-sized-models_model-serving\">Serving models on the multi-model serving platform</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_configuring_model_servers\">Configuring model servers</a></li>\n<li><a href=\"#_working_with_deployed_models\">Working with deployed models</a></li>\n<li><a href=\"#configuring-monitoring-for-the-multi-model-serving-platform_model-serving\">Configuring monitoring for the multi-model serving platform</a></li>\n<li><a href=\"#viewing-metrics-for-the-multi-model-serving-platform_model-serving\">Viewing model-serving runtime metrics for the multi-model serving platform</a></li>\n<li><a href=\"#_monitoring_model_performance_2\">Monitoring model performance</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"about-model-serving_about-model-serving\">About model serving</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>When you serve a model, you upload a trained model into Open Data Hub for querying, which allows you to integrate your trained models into intelligent applications.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can upload a model to an S3-compatible object storage, persistent volume claim, or Open Container Initiative (OCI) image. You can then access and train the model from your project workbench. After training the model, you can serve or deploy the model using a model-serving platform.</p>\n</div>\n<div class=\"paragraph\">\n<p>Serving or deploying the model makes the model available as a service, or model runtime server, that you can access using an API. You can then access the inference endpoints for the deployed model from the dashboard and see predictions based on data inputs that you provide through API calls. Querying the model through the API is also called model inferencing.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can serve models on one of the following model-serving platforms:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Single-model serving platform</p>\n</li>\n<li>\n<p>Multi-model serving platform</p>\n</li>\n<li>\n<p>NVIDIA NIM model serving platform</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The model-serving platform that you choose depends on your business needs:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you want to deploy each model on its own runtime server, or want to use a serverless deployment, select the <strong>single-model serving platform</strong>. The single-model serving platform is recommended for production use.</p>\n</li>\n<li>\n<p>If you want to deploy multiple models with only one runtime server, select the <strong>multi-model serving platform</strong>. This option is best if you are deploying more than 1,000 small and medium models and want to reduce resource consumption.</p>\n</li>\n<li>\n<p>If you want to use NVIDIA Inference Microservices (NIMs) to deploy a model, select the <strong>NVIDIA NIM-model serving platform</strong>.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_single_model_serving_platform\">Single-model serving platform</h3>\n<div class=\"paragraph\">\n<p>You can deploy each model from a dedicated model serving on the single-model serving platform. Deploying models from a dedicated model server can help you deploy, monitor, scale, and maintain models that require increased resources. This model serving platform is deal for serving large models. The single-model serving platform is based on the <a href=\"https://github.com/kserve/kserve\" target=\"_blank\" rel=\"noopener\">KServe</a> component.</p>\n</div>\n<div class=\"paragraph\">\n<p>The single-model serving platform is helpful for use cases such as:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Large language models (LLMs)</p>\n</li>\n<li>\n<p>Generative AI</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_multi_model_serving_platform\">Multi-model serving platform</h3>\n<div class=\"paragraph\">\n<p>You can deploy multiple models from the same model server on the multi-model serving platform. Each of the deployed models shares the server resources. Deploying multiple models from the same model server can be advantageous on OpenShift clusters that have finite compute resources or pods. This model serving platform is ideal for serving small and medium models in large quantities. The multi-model serving platform is based on the <a href=\"https://github.com/kserve/modelmesh\" target=\"_blank\" rel=\"noopener\">ModelMesh</a> component.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_nvidia_nim_model_serving_platform\">NVIDIA NIM model serving platform</h3>\n<div class=\"paragraph\">\n<p>You can deploy models using NVIDIA Inference Microservices (NIM) on the NVIDIA NIM model serving platform.</p>\n</div>\n<div class=\"paragraph\">\n<p>NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.</p>\n</div>\n<div class=\"paragraph\">\n<p>NVIDIA NIM inference services are helpful for use cases such as:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Using GPU-accelerated containers inferencing models optimized by NVIDIA</p>\n</li>\n<li>\n<p>Deploying generative AI for virtual screening, content generation, and avatar creation</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"serving-large-models_serving-large-models\">Serving models on the single-model serving platform</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>For deploying large models, such as large language models (LLMs), Open Data Hub includes a single-model serving platform that is based on the KServe component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require more resources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-the-single-model-serving-platform_serving-large-models\">About the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The single-model serving platform consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://github.com/opendatahub-io/kserve\" target=\"_blank\" rel=\"noopener\">KServe</a>: A Kubernetes custom resource definition (CRD) that orchestrates model serving for all types of models. It includes model-serving runtimes that implement the loading of given types of model servers. KServe handles the lifecycle of the deployment object, storage access, and networking setup.</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_serverless/1.33/html/about_openshift_serverless/index\" target=\"_blank\" rel=\"noopener\">Red&#160;Hat OpenShift Serverless</a>: A cloud-native development model that allows for serverless deployments of models. OpenShift Serverless is based on the open source <a href=\"https://knative.dev/docs/\" target=\"_blank\" rel=\"noopener\">Knative</a> project.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>To install the single-model serving platform, you have the following options:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Automated installation</dt>\n<dd>\n<p>If you have not already created a <code>ServiceMeshControlPlane</code> or <code>KNativeServing</code> resource on your OpenShift cluster, you can configure the Open Data Hub Operator to install KServe and configure its dependencies.</p>\n</dd>\n<dt class=\"hdlist1\">Manual installation</dt>\n<dd>\n<p>If you have already created a <code>ServiceMeshControlPlane</code> or <code>KNativeServing</code> resource on your OpenShift cluster, you <em>cannot</em> configure the Open Data Hub Operator to install KServe and configure its dependencies. In this situation, you must install KServe manually.</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p>When you have installed KServe, you can use the Open Data Hub dashboard to deploy models using preinstalled or custom model-serving runtimes.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub includes preinstalled runtimes for KServe. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#ref-supported-runtimes_serving-large-models\">Supported model-serving runtimes</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure monitoring for the single-model serving platform and use Prometheus to scrape the available metrics.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"model-serving-runtimes_serving-large-models\">Model-serving runtimes</h3>\n<div class=\"paragraph _abstract\">\n<p>You can serve models on the single-model serving platform by using model-serving runtimes. The configuration of a model-serving runtime is defined by the <strong>ServingRuntime</strong> and <strong>InferenceService</strong> custom resource definitions (CRDs).</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_servingruntime\">ServingRuntime</h4>\n<div class=\"paragraph\">\n<p>The <strong>ServingRuntime</strong> CRD creates a serving runtime, an environment for deploying and managing a model. It creates the templates for pods that dynamically load and unload models of various formats and also exposes a service endpoint for inferencing requests.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following YAML configuration is an example of the <strong>vLLM ServingRuntime for KServe</strong> model-serving runtime. The configuration includes various flags, environment variables and command-line arguments.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations:\n    opendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]' <b class=\"conum\">(1)</b>\n    openshift.io/display-name: vLLM ServingRuntime for KServe <b class=\"conum\">(2)</b>\n  labels:\n    opendatahub.io/dashboard: \"true\"\n  name: vllm-runtime\nspec:\n     annotations:\n          prometheus.io/path: /metrics <b class=\"conum\">(3)</b>\n          prometheus.io/port: \"8080\" <b class=\"conum\">(4)</b>\n     containers :\n          - args:\n               - --port=8080\n               - --model=/mnt/models <b class=\"conum\">(5)</b>\n               - --served-model-name={{.Name}} <b class=\"conum\">(6)</b>\n             command: <b class=\"conum\">(7)</b>\n                  - python\n                  - '-m'\n                  - vllm.entrypoints.openai.api_server\n             env:\n                  - name: HF_HOME\n                     value: /tmp/hf_home\n             image: <b class=\"conum\">(8)</b>\nquay.io/modh/vllm@sha256:8a3dd8ad6e15fe7b8e5e471037519719d4d8ad3db9d69389f2beded36a6f5b21\n          name: kserve-container\n          ports:\n               - containerPort: 8080\n                   protocol: TCP\n    multiModel: false <b class=\"conum\">(9)</b>\n    supportedModelFormats: <b class=\"conum\">(10)</b>\n        - autoSelect: true\n           name: vLLM</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>The recommended accelerator to use with the runtime.</p>\n</li>\n<li>\n<p>The name with which the serving runtime is displayed.</p>\n</li>\n<li>\n<p>The endpoint used by Prometheus to scrape metrics for monitoring.</p>\n</li>\n<li>\n<p>The port used by Prometheus to scrape metrics for monitoring.</p>\n</li>\n<li>\n<p>The path to where the model files are stored in the runtime container.</p>\n</li>\n<li>\n<p>Passes the model name that is specified by the <code>{{.Name}}</code> template variable inside the runtime container specification to the runtime environment. The <code>{{.Name}}</code> variable maps to the <code>spec.predictor.name</code> field in the <code>InferenceService</code> metadata object.</p>\n</li>\n<li>\n<p>The entrypoint command that starts the runtime container.</p>\n</li>\n<li>\n<p>The runtime container image used by the serving runtime. This image differs depending on the type of accelerator used.</p>\n</li>\n<li>\n<p>Specifies that the runtime is used for single-model serving.</p>\n</li>\n<li>\n<p>Specifies the model formats supported by the runtime.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_inferenceservice\">InferenceService</h4>\n<div class=\"paragraph\">\n<p>The <strong>InferenceService</strong> CRD creates a server or inference service that processes inference queries, passes it to the model, and then returns the inference output.</p>\n</div>\n<div class=\"paragraph\">\n<p>The inference service also performs the following actions:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Specifies the location and format of the model.</p>\n</li>\n<li>\n<p>Specifies the serving runtime used to serve the model.</p>\n</li>\n<li>\n<p>Enables the passthrough route for gRPC or REST inference.</p>\n</li>\n<li>\n<p>Defines HTTP or gRPC endpoints for the deployed model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following example shows the InferenceService YAML configuration file that is generated when deploying a granite model with the vLLM runtime:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    openshift.io/display-name: granite\n    serving.knative.openshift.io/enablePassthrough: 'true'\n    sidecar.istio.io/inject: 'true'\n    sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n  name: granite\n  labels:\n    opendatahub.io/dashboard: 'true'\nspec:\n  predictor:\n    maxReplicas: 1\n    minReplicas: 1\n    model:\n      modelFormat:\n        name: vLLM\n      name: ''\n      resources:\n        limits:\n          cpu: '6'\n          memory: 24Gi\n          nvidia.com/gpu: '1'\n        requests:\n          cpu: '1'\n          memory: 8Gi\n          nvidia.com/gpu: '1'\n      runtime: vllm-runtime\n      storage:\n        key: aws-connection-my-storage\n        path: models/granite-7b-instruct/\n    tolerations:\n      - effect: NoSchedule\n        key: nvidia.com/gpu\n        operator: Exists</code></pre>\n</div>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kserve.github.io/website/0.11/modelserving/servingruntimes/\">Serving Runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</h3>\n<div class=\"paragraph\">\n<p>You can deploy models in either <strong>advanced</strong> or <strong>standard</strong> deployment mode.</p>\n</div>\n<div class=\"paragraph\">\n<p>Advanced deployment mode uses Knative Serverless. By default, KServe integrates with Red&#160;Hat OpenShift Serverless and Red Hat OpenShift Service Mesh to deploy models on the single-model serving platform. Red&#160;Hat Serverless is based on the open source <a href=\"https://knative.dev/docs/\" target=\"_blank\" rel=\"noopener\">Knative</a> project and requires the Red&#160;Hat OpenShift Serverless Operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, you can use standard deployment mode, which uses KServe RawDeployment mode and does not require the Red&#160;Hat OpenShift Serverless Operator, Red&#160;Hat OpenShift Service Mesh, or Authorino.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you configure KServe for <strong>advanced</strong> deployment mode, you can set up your data science project to serve models in both <strong>advanced</strong> and <strong>standard</strong> deployment mode. However, if you configure KServe for only <strong>standard</strong> deployment mode, you can only use <strong>standard</strong> deployment mode.</p>\n</div>\n<div class=\"paragraph\">\n<p>There are both advantages and disadvantages to using each of these deployment modes:</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_advanced_mode\">Advanced mode</h4>\n<div class=\"paragraph\">\n<p>Advantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Enables autoscaling based on request volume:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Resources scale up automatically when receiving incoming requests.</p>\n</li>\n<li>\n<p>Optimizes resource usage and maintains performance during peak times.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Supports scale down to and from zero using Knative:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Allows resources to scale down completely when there are no incoming requests.</p>\n</li>\n<li>\n<p>Saves costs by not running idle resources.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Disadvantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Has customization limitations:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Serverless is backed by Knative and implicitly inherits the same design choices, such as when mounting multiple volumes.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Dependency on Knative for scaling:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Introduces additional complexity in setup and management compared to traditional scaling methods.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Cluster scoped component:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If the cluster already has Serverless configured, you must manually configure the cluster to make it work with Open Data Hub.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_standard_mode\">Standard mode</h4>\n<div class=\"paragraph\">\n<p>Advantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Enables deployment with Kubernetes resources, such as <code>Deployment</code>, <code>Service</code>, <code>Route</code>, and <code>Horizontal Pod Autoscaler</code>, without additional dependencies like Red&#160;Hat Serverless, Red&#160;Hat Service Mesh, and Authorino.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The resulting model deployment has a smaller resource footprint compared to advanced mode.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Enables traditional Deployment/Pod configurations, such as mounting multiple volumes, which is not available using Knative.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Beneficial for applications requiring complex configurations or multiple storage mounts.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Disadvantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Does not support automatic scaling:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Does not support automatic scaling down to zero resources when idle.</p>\n</li>\n<li>\n<p>Might result in higher costs during periods of low traffic.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-kserve_serving-large-models\">Installing KServe</h3>\n<div class=\"paragraph _abstract\">\n<p>To learn how to perform both automated and manual installation of KServe, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main/docs#installation\" target=\"_blank\" rel=\"noopener\">Installation</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main/docs#installation\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models by using the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>On the single-model serving platform, each model is deployed on its own model server. This helps you to deploy, monitor, scale, and maintain large models that require increased resources.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you want to use the single-model serving platform to deploy a model from S3-compatible storage that uses a self-signed SSL certificate, you must install a certificate authority (CA) bundle on your OpenShift cluster. For more information, see <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#understanding-certificates_certs\">Understanding how Open Data Hub handles certificates</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-single-model-serving-platform_serving-large-models\">Enabling the single-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have installed KServe, you can use the Open Data Hub dashboard to enable the single-model serving platform. You can also use the dashboard to enable model-serving runtimes for the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You have installed KServe.</p>\n</li>\n<li>\n<p>Your cluster administrator has <em>not</em> edited the Open Data Hub dashboard configuration to disable the ability to select the single-model serving platform, which uses the KServe component. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2//html/managing_openshift_ai/customizing-the-dashboard#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Enable the single-model serving platform as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the left menu, click <strong>Settings</strong> &#8594; <strong>Cluster settings</strong>.</p>\n</li>\n<li>\n<p>Locate the <strong>Model serving platforms</strong> section.</p>\n</li>\n<li>\n<p>To enable the single-model serving platform for projects, select the <strong>Single-model serving platform</strong> checkbox.</p>\n</li>\n<li>\n<p>Click <strong>Save changes</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Enable preinstalled runtimes for the single-model serving platform as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page shows preinstalled runtimes and any custom runtimes that you have added.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about preinstalled runtimes, see <a href=\"https://opendatahub.io/docs/serving-models/#ref-supported-runtimes_serving-large-models\">Supported runtimes</a>.</p>\n</div>\n</li>\n<li>\n<p>Set the runtime that you want to use to <strong>Enabled</strong>.</p>\n<div class=\"paragraph\">\n<p>The single-model serving platform is now available for model deployments.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime adds support for a specified set of model frameworks and the model formats supported by those frameworks. You can use the <a href=\"https://opendatahub.io/docs/serving-models/#about-the-single-model-serving-platform_serving-large-models\">preinstalled runtimes</a> that are included with Open Data Hub. You can also add your own custom runtimes if the default runtimes do not meet your needs.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub interface to add and enable a custom model-serving runtime. You can then choose the custom runtime when you deploy a model on the single-model serving platform.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nRed&#160;Hat does not provide support for custom runtimes. You are responsible for ensuring that you are licensed to use any custom runtimes that you add, and for correctly configuring and maintaining them.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You have built your custom runtime and added the image to a container image repository such as <a href=\"https://quay.io\" target=\"_blank\" rel=\"noopener\">Quay</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a custom runtime, choose one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To start with an existing runtime (for example,\n<strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong>), click the action menu (&#8942;) next to the existing runtime and then click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>To add a new custom runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Single-model serving platform</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the API protocol this runtime supports</strong> list, select <strong>REST</strong> or <strong>gRPC</strong>.</p>\n</li>\n<li>\n<p>Optional: If you started a new runtime (rather than duplicating an existing one), add your code by choosing one of the following options:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>Enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIn many cases, creating a custom runtime will require adding new or custom parameters to the <code>env</code> section of the <code>ServingRuntime</code> specification.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the custom runtime that you added is automatically enabled. The API protocol that you specified when creating the runtime is shown.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The custom model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a tested and verified model-serving runtime for the single-model serving platform</h4>\n<div class=\"paragraph\">\n<p>In addition to preinstalled and custom model-serving runtimes, you can also use Red&#160;Hat tested and verified model-serving runtimes such as the <strong>NVIDIA Triton Inference Server</strong> to support your needs. For more information about Red&#160;Hat tested and verified runtimes, see <a href=\"https://access.redhat.com/articles/7089743\" target=\"_blank\" rel=\"noopener\">Tested and verified runtimes for Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use the Open Data Hub dashboard to add and enable the <strong>NVIDIA Triton Inference Server</strong> runtime for the single-model serving platform. You can then choose the runtime when you deploy a model on the single-model serving platform.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Add serving runtime</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Single-model serving platform</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the API protocol this runtime supports</strong> list, select <strong>REST</strong> or <strong>gRPC</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If you selected the <strong>REST</strong> API protocol, enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: triton-kserve-rest\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  containers:\n    - args:\n        - tritonserver\n        - --model-store=/mnt/models\n        - --grpc-port=9000\n        - --http-port=8080\n        - --allow-grpc=true\n        - --allow-http=true\n      image: nvcr.io/nvidia/tritonserver@sha256:xxxxx\n      name: kserve-container\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n      ports:\n        - containerPort: 8080\n          protocol: TCP\n  protocolVersions:\n    - v2\n    - grpc-v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: tensorrt\n      version: \"8\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - name: pytorch\n      version: \"1\"\n    - autoSelect: true\n      name: triton\n      version: \"2\"\n    - autoSelect: true\n      name: xgboost\n      version: \"1\"\n    - autoSelect: true\n      name: python\n      version: \"1\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you selected the <strong>gRPC</strong> API protocol, enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: triton-kserve-grpc\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  containers:\n    - args:\n        - tritonserver\n        - --model-store=/mnt/models\n        - --grpc-port=9000\n        - --http-port=8080\n        - --allow-grpc=true\n        - --allow-http=true\n      image: nvcr.io/nvidia/tritonserver@sha256:xxxxx\n      name: kserve-container\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n      volumeMounts:\n        - mountPath: /dev/shm\n          name: shm\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  protocolVersions:\n    - v2\n    - grpc-v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: tensorrt\n      version: \"8\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - name: pytorch\n      version: \"1\"\n    - autoSelect: true\n      name: triton\n      version: \"2\"\n    - autoSelect: true\n      name: xgboost\n      version: \"1\"\n    - autoSelect: true\n      name: python\n      version: \"1\"\nvolumes:\n  - emptyDir: null\n    medium: Memory\n    sizeLimit: 2Gi\n    name: shm</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, make sure that the value of the runtime you are adding does not match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To use a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: kserve-triton\n  annotations:\n    openshift.io/display-name: Triton ServingRuntime</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime that you added is automatically enabled. The API protocol that you specified when creating the runtime is shown.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit the runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#tested-verified-runtimes_serving-large-models\">Tested and verified model-serving runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-models-on-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the single-model serving platform, you can enable a preinstalled or custom model-serving runtime and deploy models on the platform.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use preinstalled model-serving runtimes to start serving models without modifying or defining the runtime yourself. For help adding a custom runtime, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have installed KServe.</p>\n</li>\n<li>\n<p>You have enabled the single-model serving platform.</p>\n</li>\n<li>\n<p>(Advanced deployments only) To enable token authentication and external model routes for deployed models, you have added Authorino as an authorization provider.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>For the model that you want to deploy, you know the associated URI in your S3-compatible object storage bucket or Open Container Initiative (OCI) container.</p>\n</li>\n<li>\n<p>To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process\" target=\"_blank\" rel=\"noopener\">Converting Hugging Face Hub models to Caikit format</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</li>\n<li>\n<p>To use the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime or use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>To use the VLLM runtime on IBM Z and IBM Power, use the <strong>vLLM CPU ServingRuntime for KServe</strong>. For IBM Z and IBM Power, vLLM runtime is supported only on CPU.</p>\n</li>\n<li>\n<p>To use the <strong>vLLM Intel Gaudi Accelerator ServingRuntime for KServe</strong> runtime, you have enabled support for hybrid processing units (HPUs) in Open Data Hub. This includes installing the Intel Gaudi Base Operator and configuring a hardware profile. For more information, see <a href=\"https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation\" target=\"_blank\" rel=\"noopener\">Setting up Gaudi for OpenShift</a> and <a href=\"https://opendatahub.io/docs/working-with-accelerators/#working-with-hardware-profiles_accelerators\" target=\"_blank\" rel=\"noopener\">Working with hardware profiles</a>.</p>\n</li>\n<li>\n<p>To use the <strong>vLLM AMD GPU ServingRuntime for KServe</strong> runtime, you have enabled support for AMD graphic processing units (GPUs) in Open Data Hub. This includes installing the AMD GPU Operator and configuring a hardware profile. For more information, see <a href=\"https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html\" target=\"_blank\" rel=\"noopener\">Deploying the AMD GPU operator on OpenShift</a> and <a href=\"https://opendatahub.io/docs/working-with-accelerators/#working-with-hardware-profiles_accelerators\" target=\"_blank\" rel=\"noopener\">Working with hardware profiles</a>.</p>\n</li>\n<li>\n<p>To deploy RHEL AI models:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You have enabled the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime.</p>\n</li>\n<li>\n<p>You have downloaded the model from the Red&#160;Hat container registry and uploaded it to S3-compatible object storage.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you see a <strong>​​Single-model serving platform</strong> tile, click <strong>Deploy model</strong> on the tile.</p>\n</li>\n<li>\n<p>If you do not see any tiles, click the <strong>Deploy model</strong> button.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model deployment name</strong> field, enter a unique name for the model that you are deploying.</p>\n</li>\n<li>\n<p>In the <strong>Serving runtime</strong> field, select an enabled runtime.\nIf project-scoped runtimes exist, the <strong>Serving runtime</strong> list includes subheadings to distinguish between global runtimes and project-scoped runtimes.</p>\n</li>\n<li>\n<p>From the <strong>Model framework (name - version)</strong> list, select a value.</p>\n</li>\n<li>\n<p>From the <strong>Deployment mode</strong> list, select standard or advanced. For more information about deployment modes, see <a href=\"https://opendatahub.io/docs/serving-models/#about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</a>.</p>\n</li>\n<li>\n<p>In the <strong>Number of model server replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>The following options are only available if you have created a hardware profile:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Hardware profile</strong> list, select a hardware profile.\nIf project-scoped hardware profiles exist, the <strong>Hardware profile</strong> list includes subheadings to distinguish between global hardware profiles and project-scoped hardware profiles.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, hardware profiles are hidden from appearing in the dashboard navigation menu and user interface. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the <strong>Settings &#8594; Hardware profiles</strong> option in the dashboard navigation menu and the user interface components associated with hardware profiles, set the <code>disableHardwareProfiles</code> value to <code>false</code> in the <code>OdhDashboardConfig</code> custom resource (CR) in OpenShift Container Platform.\nFor more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional To change these default values, click <strong>Customize resource requests and limit</strong> and enter new minimum (request) and maximum (limit) values. The hardware profile specifies the number of CPUs and the amount of memory allocated to the container, setting the guaranteed minimum (request) and maximum (limit) for both.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> checkbox to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>To require token authentication for inference requests to the deployed model, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Require token authentication</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Service account name</strong> field, enter the service account name that the token will be generated for.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To specify the location of your model, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To use an existing connection</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Existing connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a connection that you previously defined.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p><strong>For S3-compatible object storage</strong>: In the <strong>Path</strong> field, enter the folder path that contains the model in your specified data source.</p>\n</li>\n<li>\n<p><strong>For Open Container Image connections</strong>: In the <strong>OCI storage location</strong> field, enter the model URI where the model is located.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are deploying a registered model version with an existing S3, URI, or OCI data connection, some of your connection details might be autofilled. This depends on the type of data connection and the number of matching connections available in your data science project. For example, if only one matching connection exists, fields like the path, URI, endpoint, model URI, bucket, and region might populate automatically. Matching connections will be labeled as <strong>Recommended</strong>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To use a new connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>To define a new connection that your model can access, select <strong>New connection</strong>.</p>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the <strong>Add connection</strong> modal, select a <strong>Connection type</strong>. The <strong>OCI-compliant registry</strong>, <strong>S3 compatible object storage</strong>, and <strong>URI</strong> options are pre-installed connection types. Additional options might be available if your Open Data Hub administrator added them.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add connection</strong> form opens with fields specific to the connection type that you selected.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Fill in the connection detail fields.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) Customize the runtime parameters in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Modify the values in <strong>Additional serving runtime arguments</strong> to define how the deployed model behaves.</p>\n</li>\n<li>\n<p>Modify the values in <strong>Additional environment variables</strong> to define variables in the model&#8217;s environment.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configuration parameters</strong> section shows predefined serving runtime parameters, if any are available.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nDo not modify the port or model serving runtime arguments, because they require specific values to be set. Overwriting these parameters can cause the deployment to fail.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model deployments</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-models-using-multiple-gpu-nodes_serving-large-models\">Deploying models by using multiple GPU nodes</h4>\n<div class=\"paragraph _abstract\">\n<p>Deploy models across multiple GPU nodes to handle large models, such as large language models (LLMs).</p>\n</div>\n<div class=\"paragraph\">\n<p>You can serve models on Open Data Hub across multiple GPU nodes using the vLLM serving framework. Multi-node inferencing uses the <code>vllm-multinode-runtime</code> custom runtime. The <code>vllm-multinode-runtime</code> runtime uses the same image as the VLLM NVIDIA GPU ServingRuntime for KServe runtime and also includes information necessary for multi-GPU inferencing.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift Container Platform command-line interface (CLI). For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You have enabled the operators for your GPU type, such as Node Feature Discovery Operator, NVIDIA GPU Operator. For more information about enabling accelerators, see <a href=\"https://opendatahub.io/docs/working-with-accelerators\" target=\"_blank\" rel=\"noopener\">Working with accelerators</a>.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You are using an NVIDIA GPU (<code>nvidia.com/gpu</code>).</p>\n</li>\n<li>\n<p>You have specified the GPU type through either the <code>ServingRuntime</code> or <code>InferenceService</code>. If the GPU type specified in the <code>ServingRuntime</code> differs from what is set in the <code>InferenceService</code>, both GPU types are assigned to the resource and can cause errors.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have enabled KServe on your cluster.</p>\n</li>\n<li>\n<p>You have only one head pod in your setup. Do not adjust the replica count using the <code>min_replicas</code> or <code>max_replicas</code> settings in the <code>InferenceService</code>. Creating additional head pods can cause them to be excluded from the Ray cluster.</p>\n</li>\n<li>\n<p>You have a persistent volume claim (PVC) set up and configured for ReadWriteMany (RWX) access mode.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift Container Platform cluster as a cluster administrator, log in to the OpenShift Container Platform CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login &lt;openshift_cluster_url&gt; -u &lt;admin_username&gt; -p &lt;password&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Select or create a namespace for deploying the model. For example, you can create the <code>kserve-demo</code> namespace by running the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc new-project kserve-demo</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>From the namespace where you would like to deploy the model, create a PVC for model storage. Create a storage class using <code>Filesystem volumeMode</code>. Use this storage class for your PVC. The storage size must be larger than the size of the model files on disk. For example:</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you have already configured a PVC, you can skip this step.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: granite-8b-code-base-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: &lt;model size&gt;\n  storageClassName: &lt;storage class&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>To download the model to the PVC, modify the sample YAML provided:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: download-granite-8b-code\n  labels:\n    name: download-granite-8b-code\nspec:\n  volumes:\n    - name: model-volume\n      persistentVolumeClaim:\n        claimName: granite-8b-code-base-pvc\n  restartPolicy: Never\n  initContainers:\n    - name: fix-volume-permissions\n      image: quay.io/quay/busybox@sha256:92f3298bf80a1ba949140d77987f5de081f010337880cd771f7e7fc928f8c74d\n      command: [\"sh\"]\n      args: [\"-c\", \"mkdir -p /mnt/models/$(MODEL_PATH) &amp;&amp; chmod -R 777 /mnt/models\"] <b class=\"conum\">(1)</b>\n      volumeMounts:\n        - mountPath: \"/mnt/models/\"\n          name: model-volume\n      env:\n        - name: MODEL_PATH\n          value: &lt;model path&gt; <b class=\"conum\">(2)</b>\n  containers:\n    - resources:\n        requests:\n          memory: 40Gi\n      name: download-model\n      imagePullPolicy: IfNotPresent\n      image: quay.io/opendatahub/kserve-storage-initializer:v0.14 <b class=\"conum\">(3)</b>\n      args:\n        - 's3://$(BUCKET_NAME)/$(MODEL_PATH)/'\n        - /mnt/models/$(MODEL_PATH)\n      env:\n        - name: AWS_ACCESS_KEY_ID\n          value: &lt;id&gt; <b class=\"conum\">(4)</b>\n        - name: AWS_SECRET_ACCESS_KEY\n          value: &lt;secret&gt; <b class=\"conum\">(5)</b>\n        - name: BUCKET_NAME\n          value: &lt;bucket_name&gt; <b class=\"conum\">(6)</b>\n        - name: MODEL_PATH\n          value: &lt;model path&gt; <b class=\"conum\">(2)</b>\n        - name: S3_USE_HTTPS\n          value: \"1\"\n        - name: AWS_ENDPOINT_URL\n          value: &lt;AWS endpoint&gt; <b class=\"conum\">(7)</b>\n        - name: awsAnonymousCredential\n          value: 'false'\n        - name: AWS_DEFAULT_REGION\n          value: &lt;region&gt; <b class=\"conum\">(8)</b>\n        - name: S3_VERIFY_SSL\n          value: 'true' <b class=\"conum\">(9)</b>\n      volumeMounts:\n        - mountPath: \"/mnt/models/\"\n          name: model-volume</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>The <code>chmod</code> operation is permitted only if your pod is running as root. Remove`chmod -R 777` from the arguments if you are not running the pod as root.</p>\n</li>\n<li>\n<p>Specify the path to the model.</p>\n</li>\n<li>\n<p>The value for <code>containers.image</code>, located in your <code>donwload-model</code> container. To access this value, run the following command: <code>oc get configmap inferenceservice-config -n opendatahub -oyaml | grep kserve-storage-initializer:</code></p>\n</li>\n<li>\n<p>The access key ID to your S3 bucket.</p>\n</li>\n<li>\n<p>The secret access key to your S3 bucket.</p>\n</li>\n<li>\n<p>The name of your S3 bucket.</p>\n</li>\n<li>\n<p>The endpoint to your S3 bucket.</p>\n</li>\n<li>\n<p>The region for your S3 bucket if using an AWS S3 bucket. If using other S3-compatible storage, such as ODF or Minio, you can remove the <code>AWS_DEFAULT_REGION</code> environment variable.</p>\n</li>\n<li>\n<p>If you encounter SSL errors, change <code>S3_VERIFY_SSL</code> to <code>false</code>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create the <code>vllm-multinode-runtime</code> custom runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc process vllm-multinode-runtime-template -n opendatahub|oc apply  -f -</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the model using the following <code>InferenceService</code> configuration:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    serving.kserve.io/deploymentMode: RawDeployment\n    serving.kserve.io/autoscalerClass: external\n  name: &lt;inference service name&gt;\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: vLLM\n      runtime: vllm-multinode-runtime\n      storageUri: pvc://&lt;pvc name&gt;/&lt;model path&gt;\n    workerSpec: {}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The following configuration can be added to the <code>InferenceService</code>:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>workerSpec.tensorParallelSize</code>: Determines how many GPUs are used per node. The GPU type count in both the head and worker node deployment resources is updated automatically. Ensure that the value of <code>workerSpec.tensorParallelSize</code> is at least <code>1</code>.</p>\n</li>\n<li>\n<p><code>workerSpec.pipelineParallelSize</code>: Determines how many nodes are used to balance the model in deployment. This variable represents the total number of nodes, including both the head and worker nodes. Ensure that the value of <code>workerSpec.pipelineParallelSize</code> is at least <code>2</code>. Do not modify this value in production environments.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou may need to specify additional arguments, depending on your environment and model size.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>To confirm that you have set up your environment to deploy models on multiple GPU nodes, check the GPU resource status, the <code>InferenceService</code> status, the Ray cluster status, and send a request to the model.</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Check the GPU resource status:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Retrieve the pod names for the head and worker nodes:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code># Get pod name\npodName=$(oc get pod -l app=isvc.granite-8b-code-base-pvc-predictor --no-headers|cut -d' ' -f1)\nworkerPodName=$(oc get pod -l app=isvc.granite-8b-code-base-pvc-predictor-worker --no-headers|cut -d' ' -f1)\n\noc wait --for=condition=ready pod/${podName} --timeout=300s\n# Check the GPU memory size for both the head and worker pods:\necho \"### HEAD NODE GPU Memory Size\"\nkubectl exec $podName -- nvidia-smi\necho \"### Worker NODE GPU Memory Size\"\nkubectl exec $workerPodName -- nvidia-smi</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample response</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n|  0%   33C    P0             71W /  300W |19031MiB /  23028MiB &lt;1&gt;|      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n         ...\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n|  0%   30C    P0             69W /  300W |18959MiB /  23028MiB &lt;2&gt;|      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Confirm that the model loaded properly by checking the values of &lt;1&gt; and &lt;2&gt;. If the model did not load, the value of these fields is <code>0MiB</code>.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Verify the status of your <code>InferenceService</code> using the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc wait --for=condition=ready pod/${podName} -n $DEMO_NAMESPACE --timeout=300s\nexport MODEL_NAME=granite-8b-code-base-pvc</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Sample response</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>   NAME                 URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                          AGE\n   granite-8b-code-base-pvc   http://granite-8b-code-base-pvc.default.example.com</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Send a request to the model to confirm that the model is available for inference:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc wait --for=condition=ready pod/${podName} -n vllm-multinode --timeout=300s\n\noc port-forward $podName 8080:8080 &amp;\n\ncurl http://localhost:8080/v1/completions \\\n       -H \"Content-Type: application/json\" \\\n       -d \"{\n            'model': \"$MODEL_NAME\",\n            'prompt': 'At what temperature does Nitrogen boil?',\n            'max_tokens': 100,\n            'temperature': 0\n        }\"</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"setting-timeout-for-kserve_serving-large-models\">Setting a timeout for KServe</h4>\n<div class=\"paragraph _abstract\">\n<p>When deploying large models or using node autoscaling with KServe, the operation may time out before a model is deployed because the default <code>progress-deadline</code> that KNative Serving sets is 10 minutes.</p>\n</div>\n<div class=\"paragraph\">\n<p>If a pod using KNative Serving takes longer than 10 minutes to deploy, the pod might be automatically marked as failed. This can happen if you are deploying large models that take longer than 10 minutes to pull from S3-compatible object storage or if you are using node autoscaling to reduce the consumption of GPU nodes.</p>\n</div>\n<div class=\"paragraph\">\n<p>To resolve this issue, you can set a custom <code>progress-deadline</code> in the KServe <code>InferenceService</code> for your application.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have namespace edit access for your OpenShift Container Platform cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform console as a cluster administrator.</p>\n</li>\n<li>\n<p>Select the project where you have deployed the model.</p>\n</li>\n<li>\n<p>In the <strong>Administrator</strong> perspective, click <strong>Home</strong> &#8594; <strong>Search</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Resources</strong> dropdown menu, search for <code>InferenceService</code>.</p>\n</li>\n<li>\n<p>Under <code>spec.predictor.annotations</code>, modify the <code>serving.knative.dev/progress-deadline</code> with the new timeout:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: InferenceService\nmetadata:\n  name: my-inference-service\nspec:\n  predictor:\n    annotations:\n      serving.knative.dev/progress-deadline: 30m</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Ensure that you set the <code>progress-deadline</code> on the <code>spec.predictor.annotations</code> level, so that the KServe <code>InferenceService</code> can copy the <code>progress-deadline</code> back to the KNative Service object.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"customizing-parameters-serving-runtime_serving-large-models\">Customizing the parameters of a deployed model-serving runtime</h4>\n<div class=\"paragraph _abstract\">\n<p>You might need additional parameters beyond the default ones to deploy specific models or to enhance an existing model deployment. In such cases, you can modify the parameters of an existing runtime to suit your deployment needs.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nCustomizing the parameters of a runtime only affects the selected model deployment.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You have deployed a model on the single-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model deployments</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (⋮) next to the name of the model you want to customize and select <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configuration parameters</strong> section shows predefined serving runtime parameters, if any are available.</p>\n</div>\n</li>\n<li>\n<p>Customize the runtime parameters in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Modify the values in <strong>Additional serving runtime arguments</strong> to define how the deployed model behaves.</p>\n</li>\n<li>\n<p>Modify the values in <strong>Additional environment variables</strong> to define variables in the model&#8217;s environment.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nDo not modify the port or model serving runtime arguments, because they require specific values to be set. Overwriting these parameters can cause the deployment to fail.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>After you are done customizing the runtime parameters, click <strong>Redeploy</strong> to save and deploy the model with your changes.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model deployments</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n<li>\n<p>Confirm that the arguments and variables that you set appear in <code>spec.predictor.model.args</code> and <code>spec.predictor.model.env</code> by one of the following methods:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Checking the InferenceService YAML from the OpenShift Container Platform Console.</p>\n</li>\n<li>\n<p>Using the following command in the OpenShift Container Platform CLI:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get -o json inferenceservice &lt;inferenceservicename/modelname&gt; -n &lt;projectname&gt;</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"customizable-model-serving-runtime-parameters_serving-large-models\">Customizable model serving runtime parameters</h4>\n<div class=\"paragraph _abstract\">\n<p>You can modify the parameters of an existing model serving runtime to suit your deployment needs.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about parameters for each of the supported serving runtimes, see the following table:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 50%;\">\n<col style=\"width: 50%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Serving runtime</th>\n<th class=\"tableblock halign-left valign-top\">Resource</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">NVIDIA Triton Inference Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/docs/model_config.html?#model-configuration\">NVIDIA Triton Inference Server: Model Parameters</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation Inference Server (Caikit-TGIS) ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://github.com/opendatahub-io/caikit-nlp?tab=readme-ov-file#configuration\">Caikit NLP: Configuration</a><br>\n<a href=\"https://github.com/IBM/text-generation-inference?tab=readme-ov-file#model-configuration\">TGIS: Model configuration</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://github.com/opendatahub-io/caikit-nlp?tab=readme-ov-file#configuration\">Caikit NLP: Configuration</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">OpenVINO Model Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_dynamic_input.html\">OpenVINO Model Server Features: Dynamic Input Parameters</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">[Deprecated] Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://github.com/IBM/text-generation-inference?tab=readme-ov-file#model-configuration\">TGIS: Model configuration</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM NVIDIA GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/stable/serving/engine_args.html\">vLLM: Engine Arguments</a><br>\n<a href=\"https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html\">OpenAI-Compatible Server</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM AMD GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/stable/serving/engine_args.html\">vLLM: Engine Arguments</a><br>\n<a href=\"https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html\">OpenAI-Compatible Server</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM Intel Gaudi Accelerator ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/stable/serving/engine_args.html\">vLLM: Engine Arguments</a><br>\n<a href=\"https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html\">OpenAI-Compatible Server</a></p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#customizing-parameters-serving-runtime_serving-large-models\">Customizing the parameters of a deployed model serving runtime</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-accelerators-with-vllm_serving-large-models\">Using accelerators with vLLM</h4>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub includes support for NVIDIA, AMD and Intel Gaudi accelerators. Open Data Hub also includes preinstalled model-serving runtimes that provide accelerator support.</p>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_nvidia_gpus\">NVIDIA GPUs</h5>\n<div class=\"paragraph\">\n<p>You can serve models with NVIDIA graphics processing units (GPUs) by using the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime. To use the runtime, you must enable GPU support in Open Data Hub. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_intel_gaudi_accelerators\">Intel Gaudi accelerators</h5>\n<div class=\"paragraph\">\n<p>You can serve models with Intel Gaudi accelerators by using the <strong>vLLM Intel Gaudi Accelerator ServingRuntime for KServe</strong> runtime. To use the runtime, you must enable hybrid processing support (HPU) support in Open Data Hub. This includes installing the Intel Gaudi AI accelerator operator and configuring a hardware profile. For more information, see <a href=\"https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation\" target=\"_blank\" rel=\"noopener\">Setting up Gaudi for OpenShift</a> and <a href=\"https://opendatahub.io/docs/working-with-accelerators/#working-with-hardware-profiles_accelerators\" target=\"_blank\" rel=\"noopener\">Working with hardware profiles</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about recommended vLLM parameters, environment variables, supported configurations and more, see <a href=\"https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md\" target=\"_blank\" rel=\"noopener\">vLLM with Intel® Gaudi® AI Accelerators</a>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Warm-up is a model initialization and performance optimization step that is useful for reducing cold-start delays and first-inference latency. Depending on the model size, warm-up can lead to longer model loading times.</p>\n</div>\n<div class=\"paragraph\">\n<p>While highly recommended in production environments to avoid performance limitations, you can choose to skip warm-up for non-production environments to reduce model loading times and accelerate model development and testing cycles.\nTo skip warm-up, follow the steps described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models\">Customizing the parameters of a deployed model-serving runtime</a> to add the following environment variable in the <strong>Configuration parameters</strong> section of your model deployment:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>`VLLM_SKIP_WARMUP=\"true\"`</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_amd_gpus\">AMD GPUs</h5>\n<div class=\"paragraph\">\n<p>You can serve models with AMD GPUs by using the <strong>vLLM AMD GPU ServingRuntime for KServe</strong> runtime. To use the runtime, you must enable support for AMD graphic processing units (GPUs) in Open Data Hub. This includes installing the AMD GPU operator and configuring a hardware profile. For more information, see <a href=\"https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html\" target=\"_blank\" rel=\"noopener\">Deploying the AMD GPU operator on OpenShift</a> and <a href=\"https://opendatahub.io/docs/working-with-accelerators/#working-with-hardware-profiles_accelerators\" target=\"_blank\" rel=\"noopener\">Working with hardware profiles</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#supported-model-serving-runtimes_serving-large-models\" target=\"_blank\" rel=\"noopener\">Supported model-serving runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-oci-containers-for-model-storage_serving-large-models\">Using OCI containers for model storage</h4>\n<div class=\"paragraph _abstract\">\n<p>As an alternative to storing a model in an S3 bucket or URI, you can upload models to Open Container Initiative (OCI) containers. Deploying models from OCI containers is also known as modelcars in KServe.</p>\n</div>\n<div class=\"paragraph\">\n<p>Using OCI containers for model storage can help you:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Reduce startup times by avoiding downloading the same model multiple times.</p>\n</li>\n<li>\n<p>Reduce disk space usage by reducing the number of models downloaded locally.</p>\n</li>\n<li>\n<p>Improve model performance by allowing pre-fetched images.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Using OCI containers for model storage involves the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Storing a model in an OCI image.</p>\n</li>\n<li>\n<p>Deploying a model from an OCI image by using either the user interface or the command line interface. To deploy a model by using:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The user interface, see <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>The command line interface, see <a href=\"https://opendatahub.io/docs/serving-models/#deploying-model-stored-in-oci-image_serving-large-models\">Deploying a model stored in an OCI image by using the CLI</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kserve.github.io/website/latest/modelserving/storage/oci/\">Serving models with OCI images</a></p>\n</li>\n</ul>\n</div>\n<div class=\"sect4\">\n<h5 id=\"storing-a-model-in-oci-image_serving-large-models\">Storing a model in an OCI image</h5>\n<div class=\"paragraph _abstract\">\n<p>You can store a model in an OCI image. The following procedure uses the example of storing a MobileNet v2-7 model in ONNX format.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have a model in the ONNX format. The example in this procedure uses the MobileNet v2-7 model in ONNX format.</p>\n</li>\n<li>\n<p>You have installed the Podman tool.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window on your local machine, create a temporary directory for storing both the model and the support files that you need to create the OCI image:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>cd $(mktemp -d)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a <code>models</code> folder inside the temporary directory:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>mkdir -p models/1</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>This example command specifies the subdirectory <code>1</code> because OpenVINO requires numbered subdirectories for model versioning. If you are not using OpenVINO, you do not need to create the <code>1</code> subdirectory to use OCI container images.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Download the model and support files:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>DOWNLOAD_URL=https://github.com/onnx/models/raw/main/validated/vision/classification/mobilenet/model/mobilenetv2-7.onnx\ncurl -L $DOWNLOAD_URL -O --output-dir models/1/</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use the <code>tree</code> command to confirm that the model files are located in the directory structure as expected:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>tree</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>tree</code> command should return a directory structure similar to the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>.\n├── Containerfile\n└── models\n    └── 1\n        └── mobilenetv2-7.onnx</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a Docker file named <code>Containerfile</code>:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Specify a base image that provides a shell. In the following example, <code>ubi9-micro</code> is the base container image. You cannot specify an empty image that does not provide a shell, such as <code>scratch</code>, because KServe uses the shell to ensure the model files are accessible to the model server.</p>\n</li>\n<li>\n<p>Change the ownership of the copied model files and grant read permissions to the root group to ensure that the model server can access the files. OpenShift runs containers with a random user ID and the root group ID.</p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>FROM registry.access.redhat.com/ubi9/ubi-micro:latest\nCOPY --chown=0:0 models /models\nRUN chmod -R a=rX /models\n\n# nobody user\nUSER 65534</code></pre>\n</div>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>podman build</code> commands to create the OCI container image and upload it to a registry. The following commands use Quay as the registry.</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If your repository is private, ensure that you are authenticated to the registry before uploading your container image.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>podman build --format=oci -t quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt; .\npodman push quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;</code></pre>\n</div>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"deploying-model-stored-in-oci-image_serving-large-models\">Deploying a model stored in an OCI image by using the CLI</h5>\n<div class=\"paragraph _abstract\">\n<p>You can deploy a model that is stored in an OCI image from the command line interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following procedure uses the example of deploying a MobileNet v2-7 model in ONNX format, stored in an OCI image on an OpenVINO model server.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default in KServe, models are exposed outside the cluster and not protected with authentication.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have stored a model in an OCI image as described in <a href=\"https://opendatahub.io/docs/serving-models/storing-a-model-in-oci-image\">Storing a model in an OCI image</a>.</p>\n</li>\n<li>\n<p>If you want to deploy a model that is stored in a private OCI repository, you must configure an image pull secret. For more information about creating an image pull secret, see <a href=\"https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html\" target=\"_blank\" rel=\"noopener\">Using image pull secrets</a>.</p>\n</li>\n<li>\n<p>You are logged in to your OpenShift cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a project to deploy the model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc new-project oci-model-example</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use the Open Data Hub project <code>kserve-ovms</code> template to create a <code>ServingRuntime</code> resource and configure the OpenVINO model server in the new project:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc process -n opendatahub -o yaml kserve-ovms | oc apply -f -</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Verify that the <code>ServingRuntime</code> named <code>kserve-ovms</code> is created:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get servingruntimes</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command should return output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>NAME          DISABLED   MODELTYPE     CONTAINERS         AGE\nkserve-ovms              openvino_ir   kserve-container   1m</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create an <code>InferenceService</code> YAML resource, depending on whether the model is stored from a private or a public OCI repository:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>For a model stored in a public OCI repository, create an <code>InferenceService</code> YAML file with the following values, replacing <code>&lt;user_name&gt;</code>, <code>&lt;repository_name&gt;</code>, and <code>&lt;tag_name&gt;</code> with values specific to your environment:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sample-isvc-using-oci\nspec:\n  predictor:\n    model:\n      runtime: kserve-ovms # Ensure this matches the name of the ServingRuntime resource\n      modelFormat:\n        name: onnx\n      storageUri: oci://quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;\n      resources:\n        requests:\n          memory: 500Mi\n          cpu: 100m\n          # nvidia.com/gpu: \"1\" # Only required if you have GPUs available and the model and runtime will use it\n        limits:\n          memory: 4Gi\n          cpu: 500m\n          # nvidia.com/gpu: \"1\" # Only required if you have GPUs available and the model and runtime will use it</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>For a model stored in a private OCI repository, create an <code>InferenceService</code> YAML file that specifies your pull secret in the <code>spec.predictor.imagePullSecrets</code> field, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sample-isvc-using-private-oci\nspec:\n  predictor:\n    model:\n      runtime: kserve-ovms # Ensure this matches the name of the ServingRuntime resource\n      modelFormat:\n        name: onnx\n      storageUri: oci://quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;\n      resources:\n        requests:\n          memory: 500Mi\n          cpu: 100m\n          # nvidia.com/gpu: \"1\" # Only required if you have GPUs available and the model and runtime will use it\n        limits:\n          memory: 4Gi\n          cpu: 500m\n          # nvidia.com/gpu: \"1\" # Only required if you have GPUs available and the model and runtime will use it\n    imagePullSecrets: # Specify image pull secrets to use for fetching container images, including OCI model images\n    - name: &lt;pull-secret-name&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After you create the <code>InferenceService</code> resource, KServe deploys the model stored in the OCI image referred to by the <code>storageUri</code> field.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the deployment:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get inferenceservice</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command should return output that includes information, such as the URL of the deployed model and its readiness state.</p>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-authentication-token-for-deployed-model_serving-large-models\">Accessing the authentication token for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>If you secured your model inference endpoint by enabling token authentication, you must know how to access your authentication token so that you can specify it in your inference requests.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model by using the single-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains your deployed model.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> list, expand the section for your model.</p>\n<div class=\"paragraph\">\n<p>Your authentication token is shown in the <strong>Token authentication</strong> section, in the <strong>Token secret</strong> field.</p>\n</div>\n</li>\n<li>\n<p>Optional: To copy the authentication token for use in an inference request, click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) next to the token value.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-inference-endpoint-for-deployed-model_serving-large-models\">Accessing the inference endpoint for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>To make inference requests to your deployed model, you must know how to access the inference endpoint that is available.</p>\n</div>\n<div class=\"paragraph\">\n<p>For a list of paths to use with the supported runtimes and example commands, see <a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model by using the single-model serving platform.</p>\n</li>\n<li>\n<p>If you enabled token authentication for your deployed model, you have the associated token value.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n<div class=\"paragraph\">\n<p>The inference endpoint for the model is shown in the <strong>Inference endpoint</strong> field.</p>\n</div>\n</li>\n<li>\n<p>Depending on what action you want to perform with the model (and if the model supports that action), copy the inference endpoint and then add a path to the end of the URL.</p>\n</li>\n<li>\n<p>Use the endpoint to make API requests to your deployed model.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a></p>\n</li>\n<li>\n<p><a href=\"https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html\" target=\"_blank\" rel=\"noopener\">Caikit API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/caikit/caikit-nlp/tree/main\" target=\"_blank\" rel=\"noopener\">Caikit NLP GitHub project</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html\" target=\"_blank\" rel=\"noopener\">OpenVINO KServe-compatible REST API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://platform.openai.com/docs/api-reference/introduction\">OpenAI API documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The single-model serving platform includes metrics for <a href=\"https://opendatahub.io/docs/serving-models/#about-the-single-model-serving-platform_serving-large-models\">supported runtimes</a> of the KServe component. KServe does not generate its own metrics, and relies on the underlying model-serving runtimes to provide them. The set of available metrics for a deployed model depends on its model-serving runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to runtime metrics for KServe, you can also configure monitoring for OpenShift Service Mesh. The OpenShift Service Mesh metrics help you to understand dependencies and traffic flow between components in the mesh.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have created OpenShift Service Mesh and Knative Serving instances and installed KServe.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-core-platform-monitoring#preparing-to-configure-the-monitoring-stack\">creating a config map</a> for monitoring a user-defined workflow. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm\">enabling monitoring</a> for user-defined projects in OpenShift. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You have <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm\">assigned</a> the <code>monitoring-rules-view</code> role to users that will monitor metrics.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-conf.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>user-workload-monitoring-config</code> object configures the components that monitor user-defined projects.  Observe that the retention time is set to the recommended value of 15 days.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>user-workload-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-conf.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define another <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-enable.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>cluster-monitoring-config</code> object enables monitoring for user-defined projects.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-enable.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create <code>ServiceMonitor</code> and <code>PodMonitor</code> objects to monitor metrics in the service mesh control plane as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create an <code>istiod-monitor.yaml</code> YAML file with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: istiod-monitor\n  namespace: istio-system\nspec:\n  targetLabels:\n  - app\n  selector:\n    matchLabels:\n      istio: pilot\n  endpoints:\n  - port: http-monitoring\n    interval: 30s</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>ServiceMonitor</code> CR in the specified <code>istio-system</code> namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f istiod-monitor.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>servicemonitor.monitoring.coreos.com/istiod-monitor created</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create an <code>istio-proxies-monitor.yaml</code> YAML file with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: istio-proxies-monitor\n  namespace: istio-system\nspec:\n  selector:\n    matchExpressions:\n    - key: istio-prometheus-ignore\n      operator: DoesNotExist\n  podMetricsEndpoints:\n  - path: /stats/prometheus\n    interval: 30s</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>PodMonitor</code> CR in the specified <code>istio-system</code> namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f istio-proxies-monitor.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>podmonitor.monitoring.coreos.com/istio-proxies-monitor created</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-metrics-for-the-single-model-serving-platform_serving-large-models\">Viewing model-serving runtime metrics for the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>When a cluster administrator has configured monitoring for the single-model serving platform, non-admin users can use the OpenShift web console to view model-serving runtime metrics for the KServe component.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>As described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>, use the web console to run queries for model-serving runtime metrics. You can also run queries for metrics that are related to OpenShift Service Mesh. Some examples are shown.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the vLLM runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(vllm:request_success_total{namespace=<em>${namespace}</em>,model_name=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Certain vLLM metrics are available only after an inference request is processed by a deployed model. To generate and view these metrics, you must first make an inference request to the model.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the standalone TGIS runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(tgi_request_success{namespace=${namespace}, pod=~<em>${model_name}-predictor-.*</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the Caikit Standalone runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(predict_rpc_count_total{namespace=<em>${namespace}</em>,code=<em>OK</em>,model_id=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the OpenVINO Model Server runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(ovms_requests_success{namespace=<em>${namespace}</em>,name=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.openvino.ai/2024/ovms_docs_metrics.html#available-metrics-families\" target=\"_blank\" rel=\"noopener\">OVMS metrics</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference?tab=readme-ov-file#metrics\" target=\"_blank\" rel=\"noopener\">TGIS metrics</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/stable/design/v1/metrics.html\" target=\"_blank\" rel=\"noopener\">vLLM metrics</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_monitoring_model_performance\">Monitoring model performance</h3>\n<div class=\"paragraph\">\n<p>In the single-model serving platform, you can view performance metrics for a specific model that is deployed on the platform.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-performance-metrics-for-deployed-model_serving-large-models\">Viewing performance metrics for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can monitor the following metrics for a specific model that is deployed on the single-model serving platform:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Number of requests</strong> - The number of requests that have failed or succeeded for a specific model.</p>\n</li>\n<li>\n<p><strong>Average response time (ms)</strong> - The average time it takes a specific model to respond to requests.</p>\n</li>\n<li>\n<p><strong>CPU utilization (%)</strong> - The percentage of the CPU limit per model replica that is currently utilized by a specific model.</p>\n</li>\n<li>\n<p><strong>Memory utilization (%)</strong> - The percentage of the memory limit per model replica that is utilized by a specific model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, when the peak usage hours are and how the model is performing at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>A cluster admin has enabled user workload monitoring (UWM) for user-defined projects on your OpenShift cluster. For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm\">Enabling monitoring for user-defined projects</a> and <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>The following dashboard configuration options are set to the default values as shown:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disablePerformanceMetrics:false\ndisableKServeMetrics:false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have deployed a model on the single-model serving platform by using a preinstalled runtime.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Metrics are only supported for models deployed by using a preinstalled model-serving runtime or a custom runtime that is duplicated from a preinstalled runtime.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the data science models that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Select the model that you are interested in.</p>\n</li>\n<li>\n<p>On the <strong>Endpoint performance</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for number of requests, average response time, CPU utilization, and memory utilization.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>Endpoint performance</strong> tab shows graphs of metrics for the model.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"Deploying-a-grafana-metrics-dashboard_serving-large-models\">Deploying a Grafana metrics dashboard</h4>\n<div class=\"paragraph _abstract\">\n<p>You can deploy a Grafana metrics dashboard for User Workload Monitoring (UWM) to monitor performance and resource usage metrics for models deployed on the single-model serving platform.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can create a Kustomize overlay, similar to <a href=\"https://github.com/rh-aiservices-bu/rhoai-uwm/tree/main/rhoai-uwm-grafana/overlays/rhoai-uwm-user-grafana-app\">this example</a>. Use the overlay to deploy preconfigured metrics dashboards for models deployed with OpenVino Model Server (OVMS) and vLLM.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster admin privileges for your OpenShift cluster.</p>\n</li>\n<li>\n<p>A cluster admin has enabled user workload monitoring (UWM) for user-defined projects on your OpenShift cluster. For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm\">Enabling monitoring for user-defined projects</a> and <a href=\"https://opendatahub.io/docs/serving-models/#configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>You have created an overlay to deploy a Grafana instance, similar to <a href=\"https://github.com/rh-aiservices-bu/rhoai-uwm/tree/main/rhoai-uwm-grafana/overlays/rhoai-uwm-user-grafana-app\">this example</a>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To view GPU metrics, you must enable the NVIDIA GPU monitoring dashboard as described in <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/24.9.2/enable-gpu-monitoring-dashboard.html\">Enabling the GPU monitoring dashboard</a>. The GPU monitoring dashboard provides a comprehensive view of GPU utilization, memory usage, and other metrics for your GPU nodes.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, log in to the OpenShift CLI as a cluster administrator.</p>\n</li>\n<li>\n<p>If you have not already created the overlay to install the Grafana operator and metrics dashboards, refer to the <a href=\"https://github.com/rh-aiservices-bu/rhoai-uwm/tree/main/rhoai-uwm-grafana/overlays/rhoai-uwm-user-grafana-app\">RHOAI UWM repository</a> to create it.</p>\n</li>\n<li>\n<p>Install the Grafana instance and metrics dashboards on your OpenShift cluster with the overlay that you created. Replace <code>&lt;overlay-name&gt;</code> with the name of your overlay.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -k overlays/&lt;overlay-name&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Retrieve the URL of the Grafana instance. Replace <code>&lt;namespace&gt;</code> with the namespace that contains the Grafana instance.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get route -n &lt;namespace&gt; grafana-route -o jsonpath='{.spec.host}'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following example. Use the URL to access the Grafana instance:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>grafana-&lt;namespace&gt;.apps.example-openshift.com</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can access the preconfigured dashboards available for KServe, vLLM and OVMS on the Grafana instance.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-vllm-gpu-metrics-dashboard-grafana_serving-large-models\">Deploying a vLLM/GPU metrics dashboard on a Grafana instance</h4>\n<div class=\"paragraph _abstract\">\n<p>Deploy Grafana boards to monitor accelerator and vLLM performance metrics.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have deployed a Grafana metrics dashboard, as described in <a href=\"https://opendatahub.io/docs/serving-models/#Deploying-a-grafana-metrics-dashboard\">Deploying a Grafana metrics dashboard</a>.</p>\n</li>\n<li>\n<p>You can access a Grafana instance.</p>\n</li>\n<li>\n<p>You have installed <code>envsubst</code>, a command-line tool used to substitute environment variables in configuration files. For more information, see the <a href=\"https://www.gnu.org/software/gettext/\">GNU gettext documentation</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Define a <code>GrafanaDashboard</code> object in a YAML file, similar to the following examples:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>To monitor accelerator metrics, see <a href=\"https://github.com/rh-aiservices-bu/rhoai-uwm/tree/main/rhoai-uwm-grafana/overlays/rhoai-uwm-user-grafana-app/nvidia-vllm-dashboard.yaml\"><code>nvidia-vllm-dashboard.yaml</code></a>.</p>\n</li>\n<li>\n<p>To monitor vLLM metrics, see <a href=\"https://github.com/rh-aiservices-bu/rhoai-uwm/tree/main/rhoai-uwm-grafana/overlays/rhoai-uwm-user-grafana-app/grafana-vllm-dashboard.yaml\"><code>grafana-vllm-dashboard.yaml</code></a>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create an <code>inputs.env</code> file similar to the following example. Replace the <code>NAMESPACE</code> and <code>MODEL_NAME</code> parameters with your own values:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>NAMESPACE=&lt;namespace&gt; <b class=\"conum\">(1)</b>\nMODEL_NAME=&lt;model-name&gt; <b class=\"conum\">(2)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p><strong>NAMESPACE</strong> is the target namespace where the model will be deployed.</p>\n</li>\n<li>\n<p><strong>MODEL_NAME</strong> is the model name as defined in your InferenceService. The model name is also used to filter the pod name in the Grafana dashboard.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Replace the <code>NAMESPACE</code> and <code>MODEL_NAME</code> parameters in your YAML file with the values from the <code>input.env</code> file by performing the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Export the parameters described in the <code>inputs.env</code> as environment variables:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>export $(cat inputs.env | xargs)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the  ‘$NAMESPACE’ and ‘${MODEL_NAME)’variables in the YAML file with the values of the exported environment variables:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>envsubst '${NAMESPACE} ${MODEL_NAME}' &lt; nvidia-vllm-dashboard.yaml &gt; nvidia-vllm-dashboard-replaced.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Confirm that your YAML file contains updated values.</p>\n</li>\n<li>\n<p>Deploy the dashboard object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc create -f nvidia-vllm-dashboard-replaced.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>You can see the accelerator and vLLM metrics dashboard on your Grafana instance.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"ref-grafana-metrics_serving-large-models\">Grafana metrics</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use Grafana boards to monitor the accelerator and vLLM performance metrics. The <code>datasource</code>, <code>instance</code> and <code>gpu</code> are variables defined inside the board.</p>\n</div>\n<div class=\"sect4\">\n<h5 id=\"ref-accelerator-metrics_serving-large-models\">Accelerator metrics</h5>\n<div class=\"paragraph _abstract\">\n<p>Track metrics on your accelerators to ensure the health of the hardware.</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">NVIDIA GPU utilization</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Tracks the percentage of time the GPU is actively processing tasks, indicating GPU workload levels.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">DCGM_FI_DEV_GPU_UTIL{instance=~\"$instance\", gpu=~\"$gpu\"}</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">NVIDIA GPU memory utilization</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Compares memory usage against free memory, which is critical for identifying memory bottlenecks in GPU-heavy workloads.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">DCGM_FI_DEV_POWER_USAGE{instance=~\"$instance\", gpu=~\"$gpu\"}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><em>Sum</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sum(DCGM_FI_DEV_POWER_USAGE{instance=~\"$instance\", gpu=~\"$gpu\"})</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">NVIDIA GPU temperature</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Ensures the GPU operates within safe thermal limits to prevent hardware degradation.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">DCGM_FI_DEV_GPU_TEMP{instance=~\"$instance\", gpu=~\"$gpu\"}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><em>Avg</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">avg(DCGM_FI_DEV_GPU_TEMP{instance=~\"$instance\", gpu=~\"$gpu\"})</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">NVIDIA GPU throttling</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>GPU throttling occurs when the GPU automatically reduces the clock to avoid damage from overheating.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the following metrics to identify GPU throttling:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>GPU temperature</strong>: Monitor the GPU temperature. Throttling often occurs when the GPU reaches a certain temperature, for example, 85-90°C.</p>\n</li>\n<li>\n<p><strong>SM clock speed</strong>: Monitor the core clock speed. A significant drop in the clock speed while the GPU is under load indicates throttling.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"ref-cpu-metrics_serving-large-models\">CPU metrics</h5>\n<div class=\"paragraph _abstract\">\n<p>You can track metrics on your CPU to ensure the health of the hardware.</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">CPU utilization</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Tracks CPU usage to identify workloads that are CPU-bound.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sum(rate(container_cpu_usage_seconds_total{namespace=\"$namespace\", pod=~\"$model_name.*\"}[5m])) by (namespace)</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">CPU-GPU bottlenecks</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>A combination of CPU throttling and GPU usage metrics to identify resource allocation inefficiencies. The following table outlines the combination of CPU throttling and GPU utilizations, and what these metrics mean for your environment:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">CPU throttling</th>\n<th class=\"tableblock halign-left valign-top\">GPU utilization</th>\n<th class=\"tableblock halign-left valign-top\">Meaning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Low</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">High</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">System well-balanced. GPU is fully used without CPU constraints.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">High</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Low</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">CPU resources are constrained. The CPU is unable to keep up with the GPU&#8217;s processing demands, and the GPU may be underused.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">High</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">High</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Workload is increasing for both CPU and GPU, and you might need to scale up resources.</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sum(rate(container_cpu_cfs_throttled_seconds_total{namespace=\"$namespace\", pod=~\"$model_name.*\"}[5m])) by (namespace)\navg_over_time(DCGM_FI_DEV_GPU_UTIL{instance=~\"$instance\", gpu=~\"$gpu\"}[5m])</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"ref-vllm-metrics_serving-large-models\">vLLM metrics</h5>\n<div class=\"paragraph _abstract\">\n<p>You can track metrics related to your vLLM model.</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">GPU and CPU cache utilization</dt>\n</dl>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>Tracks the percentage of GPU memory used by the vLLM model, providing insights into memory efficiency.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sum_over_time(vllm:gpu_cache_usage_perc{namespace=\"${namespace}\",pod=~\"$model_name.*\"}[24h])</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Running requests</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The number of requests actively being processed. Helps monitor workload concurrency.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">num_requests_running{namespace=\"$namespace\", pod=~\"$model_name.*\"}</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Waiting requests</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Tracks requests in the queue, indicating system saturation.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">num_requests_waiting{namespace=\"$namespace\", pod=~\"$model_name.*\"}</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Prefix cache hit rates</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>High hit rates imply efficient reuse of cached computations, optimizing resource usage.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Queries</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">vllm:gpu_cache_usage_perc{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}\nvllm:cpu_cache_usage_perc{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Request total count</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">vllm:request_success_total{finished_reason=\"length\",namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The request ended because it reached the maximum token limit set for the model inference.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">vllm:request_success_total{finished_reason=\"stop\",namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The request completed naturally based on the model&#8217;s output or a stop condition, for example, the end of a sentence or token completion.</p>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">End-to-end latency</dt>\n<dd>\n<p>Measures the overall time to process a request for an optimal user experience.</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p><em>Histogram queries</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">histogram_quantile(0.99, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.95, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.9, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.5, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nrate(vllm:e2e_request_latency_seconds_sum{namespace=\"$namespace\", pod=~\"$model_name.*\",model_name=\"$model_name\"}[5m])\nrate(vllm:e2e_request_latency_seconds_count{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Time to first token (TTFT) latency</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The time taken to generate the first token in a response.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Histogram queries</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">histogram_quantile(0.99, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.95, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.9, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.5, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nrate(vllm:time_to_first_token_seconds_sum{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])\nrate(vllm:time_to_first_token_seconds_count{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Time per output token (TPOT) latency</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The average time taken to generate each output token.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Histogram queries</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">histogram_quantile(0.99, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.95, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.9, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nhistogram_quantile(0.5, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])))\nrate(vllm:time_per_output_token_seconds_sum{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])\nrate(vllm:time_per_output_token_seconds_count{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Prompt token throughput and generation throughput</dt>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Tracks the speed of processing prompt tokens for LLM optimization.</p>\n</div>\n<div class=\"paragraph\">\n<p><em>Queries</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">rate(vllm:prompt_tokens_total{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])\nrate(vllm:generation_tokens_total{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"}[5m])</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Total tokens generated</dt>\n<dd>\n<p>Measures the efficiency of generating response tokens, critical for real-time applications.</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p><em>Query</em></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sum(vllm:generation_tokens_total{namespace=\"$namespace\", pod=~\"$model_name.*\", model_name=\"$model_name\"})</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_optimizing_model_serving_runtimes\">Optimizing model-serving runtimes</h3>\n<div class=\"paragraph\">\n<p>You can optionally enhance the preinstalled model-serving runtimes available in Open Data Hub to leverage additional benefits and capabilities, such as optimized inferencing, reduced latency, and fine-tuned resource allocation.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-speculative-decoding-and-multi-modal-inferencing_serving-large-models\">Enabling speculative decoding and multi-modal inferencing</h4>\n<div class=\"paragraph\">\n<p>You can configure the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime to use speculative decoding, a parallel processing technique to optimize inferencing time for large language models (LLMs).</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure the runtime to support inferencing for vision-language models (VLMs). VLMs are a subset of multi-modal models that integrate both visual and textual data.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following procedure describes customizing the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime for speculative decoding and multi-modal inferencing.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>If you are using the vLLM model-serving runtime for speculative decoding with a draft model, you have stored the original model and the speculative model in the same folder within your S3-compatible object storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Follow the steps to deploy a model as described in <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#deploying-models-on-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>In the <strong>Serving runtime</strong> field, select the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> runtime.</p>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for speculative decoding by matching n-grams in the prompt, add the following arguments under <strong>Additional serving runtime arguments</strong> in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>--speculative-model=[ngram]\n--num-speculative-tokens=&lt;NUM_SPECULATIVE_TOKENS&gt;\n--ngram-prompt-lookup-max=&lt;NGRAM_PROMPT_LOOKUP_MAX&gt;\n--use-v2-block-manager</code></pre>\n</div>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Replace <code>&lt;NUM_SPECULATIVE_TOKENS&gt;</code> and <code>&lt;NGRAM_PROMPT_LOOKUP_MAX&gt;</code> with your own values.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Inferencing throughput varies depending on the model used for speculating with n-grams.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for speculative decoding with a draft model, add the following arguments under <strong>Additional serving runtime arguments</strong> in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>--port=8080\n--served-model-name={{.Name}}\n--distributed-executor-backend=mp\n--model=/mnt/models/&lt;path_to_original_model&gt;\n--speculative-model=/mnt/models/&lt;path_to_speculative_model&gt;\n--num-speculative-tokens=&lt;NUM_SPECULATIVE_TOKENS&gt;\n--use-v2-block-manager</code></pre>\n</div>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Replace <code>&lt;path_to_speculative_model&gt;</code> and <code>&lt;path_to_original_model&gt;</code> with the paths to the speculative model and original model on your S3-compatible object storage.</p>\n</li>\n<li>\n<p>Replace <code>&lt;NUM_SPECULATIVE_TOKENS&gt;</code> with your own value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for multi-modal inferencing, add the following arguments under <strong>Additional serving runtime arguments</strong> in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>--trust-remote-code</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Only use the <code>--trust-remote-code</code> argument with models from trusted sources.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>If you have configured the vLLM model-serving runtime for speculative decoding, use the following example command to verify API requests to your deployed model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer &lt;token&gt;\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you have configured the vLLM model-serving runtime for multi-modal inferencing, use the following example command to verify API requests to the vision-language model (VLM) that you have deployed:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer &lt;token&gt;\"\n-d '{\"model\":\"&lt;model_name&gt;\",\n     \"messages\":\n        [{\"role\":\"&lt;role&gt;\",\n          \"content\":\n             [{\"type\":\"text\", \"text\":\"&lt;text&gt;\"\n              },\n              {\"type\":\"image_url\", \"image_url\":\"&lt;image_url_link&gt;\"\n              }\n             ]\n         }\n        ]\n    }'</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/stable/serving/engine_args.html\">vLLM: Engine Arguments</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\">OpenAI Compatible Server</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_performance_tuning_on_the_single_model_serving_platform\">Performance tuning on the single-model serving platform</h3>\n<div class=\"paragraph\">\n<p>Certain performance issues might require you to tune the parameters of your inference service or model-serving runtime.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models\">Resolving CUDA out-of-memory errors</h4>\n<div class=\"paragraph _abstract\">\n<p>In certain cases, depending on the model and hardware accelerator used, the TGIS memory auto-tuning algorithm might underestimate the amount of GPU memory needed to process long sequences. This miscalculation can lead to Compute Unified Architecture (CUDA) out-of-memory (OOM) error responses from the model server. In such cases, you must update or add additional parameters in the TGIS model-serving runtime, as described in the following procedure.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <strong>Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</strong> is deprecated. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/release_notes/index\">Open Data Hub release notes</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>Based on the runtime that you used to deploy your model, perform one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you used the preinstalled <strong>Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</strong> runtime, duplicate the runtime to create a custom version and then follow the remainder of this procedure. For more information about duplicating the pre-installed TGIS runtime, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>If you were already using a custom TGIS runtime, click the action menu (&#8942;) next to the runtime and select <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the custom model-serving runtime.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Add or update the <code>BATCH_SAFETY_MARGIN</code> environment variable and set the value to 30. Similarly, add or update the <code>ESTIMATE_MEMORY_BATCH_SIZE</code> environment variable and set the value to 8.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  containers:\n    env:\n    - name: BATCH_SAFETY_MARGIN\n      value: 30\n    - name: ESTIMATE_MEMORY_BATCH\n      value: 8</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>BATCH_SAFETY_MARGIN</code> parameter sets a percentage of free GPU memory to hold back as a safety margin to avoid OOM conditions. The default value of <code>BATCH_SAFETY_MARGIN</code> is <code>20</code>. The <code>ESTIMATE_MEMORY_BATCH_SIZE</code> parameter sets the batch size used in the memory auto-tuning algorithm. The default value of <code>ESTIMATE_MEMORY_BATCH_SIZE</code>  is <code>16</code>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the list of runtimes that are installed. Observe that the custom model-serving runtime you updated is shown.</p>\n</div>\n</li>\n<li>\n<p>To redeploy the model for the parameter updates to take effect, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n</li>\n<li>\n<p>Find the model you want to redeploy, click the action menu (⋮) next to the model, and select <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>Redeploy the model as described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-on-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</a>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You receive successful responses from the model server and no longer see CUDA OOM errors.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"supported-model-serving-runtimes_serving-large-models\">Supported model-serving runtimes</h3>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub includes several preinstalled model-serving runtimes. You can use preinstalled model-serving runtimes to start serving models without modifying or defining the runtime yourself. You can also add a custom runtime to support a model.</p>\n</div>\n<div class=\"paragraph\">\n<p>For help adding a custom runtime, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Model-serving runtimes</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n<th class=\"tableblock halign-left valign-top\">Exported model format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation Inference Server (Caikit-TGIS) ServingRuntime for KServe (1)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A composite runtime for serving models in the Caikit format</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Standalone ServingRuntime for KServe (2)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A runtime for serving models in the Caikit embeddings format for embeddings tasks</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Embeddings</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">OpenVINO Model Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A scalable, high-performance runtime for serving models that are optimized for Intel architectures</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch, TensorFlow, OpenVINO IR, PaddlePaddle, MXNet, Caffe, Kaldi</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">[Deprecated] Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe (3)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A runtime for serving TGI-enabled models</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch Model Formats</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM NVIDIA GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A high-throughput and memory-efficient inference and serving runtime for large language models that supports NVIDIA GPU accelerators</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\" target=\"_blank\" rel=\"noopener\">Supported models</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM Intel Gaudi Accelerator ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A high-throughput and memory-efficient inference and serving runtime that supports Intel Gaudi accelerators</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\" target=\"_blank\" rel=\"noopener\">Supported models</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM AMD GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A high-throughput and memory-efficient inference and serving runtime that supports AMD GPU accelerators</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\" target=\"_blank\" rel=\"noopener\">Supported models</a></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM CPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A high-throughput and memory-efficient inference and serving runtime that supports IBM Power (ppc64le) and IBM Z (s390x).</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\" target=\"_blank\" rel=\"noopener\">Supported models</a></p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>The composite Caikit-TGIS runtime is based on <a href=\"https://github.com/opendatahub-io/caikit\" target=\"_blank\" rel=\"noopener\">Caikit</a> and <a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a>. To use this runtime, you must convert your models to Caikit format. For an example, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process\" target=\"_blank\" rel=\"noopener\">Converting Hugging Face Hub models to Caikit format</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</li>\n<li>\n<p>The Caikit Standalone runtime is based on <a href=\"https://github.com/caikit/caikit-nlp/tree/main\" target=\"_blank\" rel=\"noopener\">Caikit NLP</a>. To use this runtime, you must convert your models to the Caikit embeddings format. For an example, see <a href=\"https://github.com/caikit/caikit-nlp/blob/main/tests/modules/text_embedding/test_embedding.py\" target=\"_blank\" rel=\"noopener\">Tests for text embedding module</a>.</p>\n</li>\n<li>\n<p>The <strong>Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</strong> is deprecated. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/release_notes/index\">Open Data Hub release notes</a>.</p>\n</li>\n</ol>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. Deployment requirements</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Default protocol</th>\n<th class=\"tableblock halign-left valign-top\">Additonal protocol</th>\n<th class=\"tableblock halign-left valign-top\">Model mesh support</th>\n<th class=\"tableblock halign-left valign-top\">Single node OpenShift support</th>\n<th class=\"tableblock halign-left valign-top\">Deployment mode</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation Inference Server (Caikit-TGIS) ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">OpenVINO Model Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">[Deprecated] Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM NVIDIA GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM Intel Gaudi Accelerator ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM AMD GPU ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM CPU ServingRuntime for KServe[1]</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p><sup class=\"footnote\">[<a id=\"_footnoteref_1\" class=\"footnote\" href=\"#_footnotedef_1\" title=\"View footnote.\">1</a>]</sup> If you are using IBM Z and IBM Power architecture, you can only deploy models in standard deployment mode.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tested-verified-runtimes_serving-large-models\">Tested and verified model-serving runtimes</h3>\n<div class=\"paragraph _abstract\">\n<p>Tested and verified runtimes are community versions of model-serving runtimes that have been tested and verified against specific versions of Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Red&#160;Hat tests the current version of a tested and verified runtime each time there is a new version of Open Data Hub. If a new version of a tested and verified runtime is released in the middle of an Open Data Hub release cycle, it will be tested and verified in an upcoming release.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Tested and verified runtimes are not directly supported by Red&#160;Hat. You are responsible for ensuring that you are licensed to use any tested and verified runtimes that you add, and for correctly configuring and maintaining them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 3. Model-serving runtimes</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n<th class=\"tableblock halign-left valign-top\">Exported model format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">NVIDIA Triton Inference Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">An open-source inference-serving software for fast and scalable AI in applications.</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more</p></td>\n</tr>\n</tbody>\n</table>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 4. Deployment requirements</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Default protocol</th>\n<th class=\"tableblock halign-left valign-top\">Additonal protocol</th>\n<th class=\"tableblock halign-left valign-top\">Model mesh support</th>\n<th class=\"tableblock halign-left valign-top\">Single node OpenShift support</th>\n<th class=\"tableblock halign-left valign-top\">Deployment mode</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">NVIDIA Triton Inference Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"inference-endpoints_serving-large-models\">Inference endpoints</h3>\n<div class=\"paragraph _abstract\">\n<p>These examples show how to use inference endpoints to query the model.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you enabled token authentication when deploying the model, add the <code>Authorization</code> header and specify a token value.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_caikit_tgis_servingruntime_for_kserve\">Caikit TGIS ServingRuntime for KServe</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443/api/v1/task/text-generation</code></p>\n</li>\n<li>\n<p><code>:443/api/v1/task/server-streaming-text-generation</code></p>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl --json '{\"model_id\": \"&lt;model_name&gt;\", \"inputs\": \"&lt;text&gt;\"}' \\\nhttps://&lt;inference_endpoint_url&gt;:443/api/v1/task/server-streaming-text-generation \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_caikit_standalone_servingruntime_for_kserve\">Caikit Standalone ServingRuntime for KServe</h4>\n<div class=\"paragraph\">\n<p>If you are serving multiple models, you can query <code>/info/models</code> or <code>:443 caikit.runtime.info.InfoService/GetModelsInfo</code> to view a list of served models.</p>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">REST endpoints</div>\n<ul>\n<li>\n<p><code>/api/v1/task/embedding</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/embedding-tasks</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/sentence-similarity</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/sentence-similarity-tasks</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/rerank</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/rerank-tasks</code></p>\n</li>\n<li>\n<p><code>/info/models</code></p>\n</li>\n<li>\n<p><code>/info/version</code></p>\n</li>\n<li>\n<p><code>/info/runtime</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">gRPC endpoints</div>\n<ul>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/EmbeddingTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/RerankTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/RerankTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.info.InfoService/GetModelsInfo</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.info.InfoService/GetRuntimeInfo</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>An example manifest is available in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving GitHub repository</a>.</p>\n</div>\n<div class=\"openblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p><strong>REST</strong></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -H 'Content-Type: application/json' -d '{\"inputs\": \"&lt;text&gt;\", \"model_id\": \"&lt;model_id&gt;\"}' &lt;inference_endpoint_url&gt;/api/v1/task/embedding -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>gRPC</strong></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -d '{\"text\": \"&lt;text&gt;\"}' -H \\\"mm-model-id: &lt;model_id&gt;\\\" &lt;inference_endpoint_url&gt;:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_tgis_standalone_servingruntime_for_kserve\">TGIS Standalone ServingRuntime for KServe</h4>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\nThe <strong>Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</strong> is deprecated. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/release_notes/index\">Open Data Hub release notes</a>.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443 fmaas.GenerationService/Generate</code></p>\n</li>\n<li>\n<p><code>:443 fmaas.GenerationService/GenerateStream</code></p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To query the endpoint for the TGIS standalone runtime, you must also download the files in the <a href=\"https://github.com/opendatahub-io/text-generation-inference/blob/main/proto\" target=\"_blank\" rel=\"noopener\">proto</a> directory of the Open Data Hub <code>text-generation-inference</code> repository.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -proto text-generation-inference/proto/generation.proto -d \\\n'{\"requests\": [{\"text\":\"&lt;text&gt;\"}]}' \\\n-insecure &lt;inference_endpoint_url&gt;:443 fmaas.GenerationService/Generate \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_openvino_model_server\">OpenVINO Model Server</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>/v2/models/&lt;model-name&gt;/infer</code></p>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -ks &lt;inference_endpoint_url&gt;/v2/models/&lt;model_name&gt;/infer -d \\\n'{ \"model_name\": \"&lt;model_name&gt;\", \\\n\"inputs\": [{ \"name\": \"&lt;name_of_model_input&gt;\", \"shape\": [&lt;shape&gt;], \"datatype\": \"&lt;data_type&gt;\", \"data\": [&lt;data&gt;] }]}' \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_vllm_nvidia_gpu_servingruntime_for_kserve\">vLLM NVIDIA GPU ServingRuntime for KServe</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443/version</code></p>\n</li>\n<li>\n<p><code>:443/docs</code></p>\n</li>\n<li>\n<p><code>:443/v1/models</code></p>\n</li>\n<li>\n<p><code>:443/v1/chat/completions</code></p>\n</li>\n<li>\n<p><code>:443/v1/completions</code></p>\n</li>\n<li>\n<p><code>:443/v1/embeddings</code></p>\n</li>\n<li>\n<p><code>:443/tokenize</code></p>\n</li>\n<li>\n<p><code>:443/detokenize</code></p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The vLLM runtime is compatible with the OpenAI REST API. For a list of models that the vLLM runtime supports, see <a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\">Supported models</a>.</p>\n</li>\n<li>\n<p>To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see <a href=\"https://github.com/vllm-project/vllm/pull/3734\">Supported embeddings models in vLLM</a>.</p>\n</li>\n<li>\n<p>As of vLLM v0.5.5, you must provide a chat template while querying a model using the <code>/v1/chat/completions</code> endpoint. If your model does not include a predefined chat template, you can use the <code>chat-template</code> command-line parameter to specify a chat template in your custom vLLM runtime, as shown in the example. Replace <code>&lt;CHAT_TEMPLATE&gt;</code> with the path to your template.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --chat-template=&lt;CHAT_TEMPLATE&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You can use the chat templates that are available as <code>.jinja</code> files <a href=\"https://github.com/opendatahub-io/vllm/tree/main/examples\">here</a> or with the vLLM image under <code>/app/data/template</code>. For more information, see <a href=\"https://huggingface.co/docs/transformers/main/chat_templating\">Chat templates</a>.</p>\n</div>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions -H \\\n\"Content-Type: application/json\" -d '{ \\\n\"messages\": [{ \\\n\"role\": \"&lt;role&gt;\", \\\n\"content\": \"&lt;content&gt;\" \\\n}] -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve\">vLLM Intel Gaudi Accelerator ServingRuntime for KServe</h4>\n<div class=\"paragraph\">\n<p>See <a href=\"https://opendatahub.io/docs/serving-models/#_vllm_servingruntime_for_kserve\" target=\"_blank\" rel=\"noopener\">vLLM NVIDIA GPU ServingRuntime for KServe</a>.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_vllm_amd_gpu_servingruntime_for_kserve\">vLLM AMD GPU ServingRuntime for KServe</h4>\n<div class=\"paragraph\">\n<p>See <a href=\"https://opendatahub.io/docs/serving-models/#_vllm_servingruntime_for_kserve\" target=\"_blank\" rel=\"noopener\">vLLM NVIDIA GPU ServingRuntime for KServe</a>.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_nvidia_triton_inference_server\">NVIDIA Triton Inference Server</h4>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">REST endpoints</div>\n<ul>\n<li>\n<p><code>v2/models/[/versions/&lt;model_version&gt;]/infer</code></p>\n</li>\n<li>\n<p><code>v2/models/&lt;model_name&gt;[/versions/&lt;model_version&gt;]</code></p>\n</li>\n<li>\n<p><code>v2/health/ready</code></p>\n</li>\n<li>\n<p><code>v2/health/live</code></p>\n</li>\n<li>\n<p><code>v2/models/&lt;model_name&gt;[/versions/]/ready</code></p>\n</li>\n<li>\n<p><code>v2</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>ModelMesh does not support the following REST endpoints:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>v2/health/live</code></p>\n</li>\n<li>\n<p><code>v2/health/ready</code></p>\n</li>\n<li>\n<p><code>v2/models/&lt;model_name&gt;[/versions/]/ready</code></p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -ks &lt;inference_endpoint_url&gt;/v2/models/&lt;model_name&gt;/infer -d /\n'{ \"model_name\": \"&lt;model_name&gt;\", \\\n   \"inputs\": \\\n\t[{ \"name\": \"&lt;name_of_model_input&gt;\", \\\n           \"shape\": [&lt;shape&gt;], \\\n           \"datatype\": \"&lt;data_type&gt;\", \\\n           \"data\": [&lt;data&gt;] \\\n         }]}' -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">gRPC endpoints</div>\n<ul>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelInfer</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelReady</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelMetadata</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerReady</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerLive</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerMetadata</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -cacert ./openshift_ca_istio_knative.crt \\\n        -proto ./grpc_predict_v2.proto \\\n        -d @ \\\n        -H \"Authorization: Bearer &lt;token&gt;\" \\\n        &lt;inference_endpoint_url&gt;:443 \\\n        inference.GRPCInferenceService/ModelMetadata</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3 _additional-resources\">\n<h4 id=\"_additional_resources\">Additional resources</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a></p>\n</li>\n<li>\n<p><a href=\"https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html\" target=\"_blank\" rel=\"noopener\">Caikit API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/caikit/caikit-nlp\" target=\"_blank\" rel=\"noopener\">Caikit NLP GitHub project</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html\" target=\"_blank\" rel=\"noopener\">OpenVINO KServe-compatible REST API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://platform.openai.com/docs/api-reference/introduction\" target=\"_blank\" rel=\"noopener\">OpenAI API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://kserve.github.io/website/master/modelserving/data_plane/v2_protocol/\">Open Inference Protocol</a></p>\n</li>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#supported-runtimes_serving-large-models\">Supported model-serving runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">About the NVIDIA NIM model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>You can deploy models using NVIDIA NIM inference services on the <strong>NVIDIA NIM model serving platform</strong>.</p>\n</div>\n<div class=\"paragraph\">\n<p>NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.nvidia.com/nim/index.html\">NVIDIA NIM</a></p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-nvidia-nim-model-serving-platform_serving-large-models\">Enabling the NVIDIA NIM model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>As an administrator, you can use the Open Data Hub dashboard to enable the NVIDIA NIM model serving platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as an administrator.</p>\n</li>\n<li>\n<p>You have enabled the single-model serving platform. You do not need to enable a preinstalled runtime. For more information about enabling the single-model serving platform, see <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models\" target=\"_blank\" rel=\"noopener\">Enabling the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>The <code>disableNIMModelServing</code> Open Data Hub dashboard configuration is set to <code>false</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disableNIMModelServing: false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-resources/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have an NVIDIA Cloud Account (NCA) and can access the NVIDIA GPU Cloud (NGC) portal. For more information, see <a href=\"https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html\">NVIDIA GPU Cloud user guide</a>.</p>\n</li>\n<li>\n<p>Your NCA account is associated with the NVIDIA AI Enterprise Viewer role.</p>\n</li>\n<li>\n<p>You have generated an NGC API key on the NGC portal. For more information, see <a href=\"https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#ngc-api-keys\">NGC API keys</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to Open Data Hub.</p>\n</li>\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Applications</strong> &#8594; <strong>Explore</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Explore</strong> page, find the <strong>NVIDIA NIM</strong> tile.</p>\n</li>\n<li>\n<p>Click <strong>Enable</strong> on the application tile.</p>\n</li>\n<li>\n<p>Enter the NGC API key and then click <strong>Submit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The NVIDIA NIM application that you enabled appears on the <strong>Enabled</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">Deploying models on the NVIDIA NIM model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the <strong>NVIDIA NIM model serving platform</strong>, you can start to deploy NVIDIA-optimized models on the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have enabled the <strong>NVIDIA NIM model serving platform</strong>.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have enabled support for graphic processing units (GPUs) in Open Data Hub. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>In the <strong>Models</strong> section, perform one of the following actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On the <strong>​​NVIDIA NIM model serving platform</strong> tile, click <strong>Select NVIDIA NIM</strong> on the tile, and then click <strong>Deploy model</strong>.</p>\n</li>\n<li>\n<p>If you have previously selected the NVIDIA NIM model serving type, the <strong>Models</strong> page displays <strong>NVIDIA model serving enabled</strong> on the upper-right corner, along with the <strong>Deploy model</strong> button. To proceed, click <strong>Deploy model</strong>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Configure properties for deploying your model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model deployment name</strong> field, enter a unique name for the deployment.</p>\n</li>\n<li>\n<p>From the <strong>NVIDIA NIM</strong> list, select the NVIDIA NIM model that you want to deploy. For more information, see <a href=\"https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html\" target=\"_blank\" rel=\"noopener\">Supported Models</a></p>\n</li>\n<li>\n<p>In the <strong>NVIDIA NIM storage size</strong> field, specify the size of the cluster storage instance that will be created to store the NVIDIA NIM model.</p>\n</li>\n<li>\n<p>In the <strong>Number of model server replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>From the <strong>Hardware profile</strong> list, select a hardware profile.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, hardware profiles are hidden from appearing in the dashboard navigation menu and user interface. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the <strong>Settings &#8594; Hardware profiles</strong> option in the dashboard navigation menu and the user interface components associated with hardware profiles, set the <code>disableHardwareProfiles</code> value to <code>false</code> in the <code>OdhDashboardConfig</code> custom resource (CR) in OpenShift Container Platform.\nFor more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: Click <strong>Customize resource requests and limit</strong> and update the following values:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>CPUs requests</strong> field, specify the number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limits</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requests</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limits</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> checkbox to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>To require token authentication for inference requests to the deployed model, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Require token authentication</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Service account name</strong> field, enter the service account name that the token will be generated for.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model deployments</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.nvidia.com/nim/large-language-models/latest/api-reference.html\" target=\"_blank\" rel=\"noopener\">NVIDIA NIM API reference</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html\" target=\"_blank\" rel=\"noopener\">Supported Models</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"Customizing-model-selection-options_serving-large-models\">Customizing model selection options for the NVIDIA NIM model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>The NVIDIA NIM model serving platform provides access to all available NVIDIA NIM models from the NVIDIA GPU Cloud (NGC). You can deploy a NIM model by selecting it from the <strong>NVIDIA NIM</strong> list in the <strong>Deploy model</strong> dialog. To customize the models that appear in the list, you can create a <code>ConfigMap</code> object specifying your preferred models.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have an NVIDIA Cloud Account (NCA) and can access the NVIDIA GPU Cloud (NGC) portal.</p>\n</li>\n<li>\n<p>You know the IDs of the NVIDIA NIM models that you want to make available for selection on the NVIDIA NIM model serving platform.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You can find the model ID from the <a href=\"https://catalog.ngc.nvidia.com/\">NGC Catalog</a>. The ID is usually part of the URL path.</p>\n</li>\n<li>\n<p>You can also find the model ID by using the NGC CLI.  For more information, see <a href=\"https://docs.ngc.nvidia.com/cli/cmd_registry.html#model\">NGC CLI reference</a>.</p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>You know the name and namespace of your <code>Account</code> custom resource (CR).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, log in to the OpenShift Container Platform CLI as a cluster administrator as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-console\" data-lang=\"console\">oc login &lt;openshift_cluster_url&gt; -u &lt;admin_username&gt; -p &lt;password&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file, similar to the one in the following example, containing the model IDs that you want to make available for selection on the NVIDIA NIM model serving platform:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: nvidia-nim-enabled-models\ndata:\n models: |-\n    [\n    \"mistral-nemo-12b-instruct\",\n    \"llama3-70b-instruct\",\n    \"phind-codellama-34b-v2-instruct\",\n    \"deepseek-r1\",\n    \"qwen-2.5-72b-instruct\"\n    ]</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Confirm the name and namespace of your <code>Account</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-console\" data-lang=\"console\">oc get account -A</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-console\" data-lang=\"console\">NAMESPACE         NAME       TEMPLATE  CONFIGMAP  SECRET\nredhat-ods-applications  odh-nim-account</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>ConfigMap</code> object in the same namespace as your <code>Account</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">oc apply -f &lt;configmap-name&gt; -n &lt;namespace&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>&lt;configmap-name&gt;</em> with the name of your YAML file, and <em>&lt;namespace&gt;</em> with the namespace of your <code>Account</code> CR.</p>\n</div>\n</li>\n<li>\n<p>Add the <code>ConfigMap</code> object that you previously created to the <code>spec.modelListConfig</code> section of your <code>Account</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-console\" data-lang=\"console\">oc patch account &lt;account-name&gt; \\\n  --type='merge' \\\n  \t-p '{\"spec\": {\"modelListConfig\": {\"name\": \"&lt;configmap-name&gt;\"}}}'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>&lt;account-name&gt;</em> with the name of your <code>Account</code> CR, and <em>&lt;configmap-name&gt;</em> with your <code>ConfigMap</code> object.</p>\n</div>\n</li>\n<li>\n<p>Confirm that the <code>ConfigMap</code> object is added to your <code>Account</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-console\" data-lang=\"console\">oc get account &lt;account-name&gt; -o yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the <code>ConfigMap</code> object in the <code>spec.modelListConfig</code> section of your <code>Account</code> CR, similar to the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">spec:\n enabledModelsConfig:\n modelListConfig:\n  name: &lt;configmap-name&gt;</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Follow the steps to deploy a model as described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">Deploying models on the NVIDIA NIM model serving platform to deploy a NIM model</a>. You see that the <strong>NVIDIA NIM</strong> list in the <strong>Deploy model</strong> dialog displays your preferred list of models instead of all the models available in the NGC catalog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models\">Enabling NVIDIA NIM metrics for an existing NIM deployment</h4>\n<div class=\"paragraph _abstract\">\n<p>If you have previously deployed a NIM model in Open Data Hub, and then upgraded to the latest version, you must manually enable NIM metrics for your existing deployment by adding annotations to enable metrics collection and graph generation.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>NIM metrics and graphs are automatically enabled for new deployments in the latest version of Open Data Hub.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_enabling_graph_generation_for_an_existing_nim_deployment\">Enabling graph generation for an existing NIM deployment</h5>\n<div class=\"paragraph\">\n<p>The following procedure describes how to enable graph generation for an existing NIM deployment.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You have an existing NIM deployment in Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift Container Platform cluster as a cluster administrator, log in to the OpenShift CLI.</p>\n</li>\n<li>\n<p>Confirm the name of the <code>ServingRuntime</code> associated with your NIM deployment:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get servingruntime -n &lt;namespace&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <code>&lt;namespace&gt;</code> with the namespace of the project where your NIM model is deployed.</p>\n</div>\n</li>\n<li>\n<p>Check for an existing <code>metadata.annotations</code> section in the <code>ServingRuntime</code> configuration:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get servingruntime -n  &lt;namespace&gt; &lt;servingruntime-name&gt; -o json | jq '.metadata.annotations'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace &lt;servingruntime-name&gt; with the name of the <code>ServingRuntime</code> from the previous step.</p>\n</div>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If the <code>metadata.annotations</code> section is not present in the configuration, add the section with the required annotations:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch servingruntime -n &lt;namespace&gt; &lt;servingruntime-name&gt; --type json --patch \\\n '[{\"op\": \"add\", \"path\": \"/metadata/annotations\", \"value\": {\"runtimes.opendatahub.io/nvidia-nim\": \"true\"}}]'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>servingruntime.serving.kserve.io/nim-serving-runtime patched</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If there is an existing <code>metadata.annotations</code> section, add the required annotations to the section:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch servingruntime -n &lt;project-namespace&gt; &lt;runtime-name&gt; --type json --patch \\\n '[{\"op\": \"add\", \"path\": \"/metadata/annotations/runtimes.opendatahub.io~1nvidia-nim\", \"value\": \"true\"}]'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>servingruntime.serving.kserve.io/nim-serving-runtime patched</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the annotation has been added to the <code>ServingRuntime</code> of your existing NIM deployment.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get servingruntime -n &lt;namespace&gt; &lt;servingruntime-name&gt; -o json | jq '.metadata.annotations'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The annotation that you added appears in the output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>...\n\"runtimes.opendatahub.io/nvidia-nim\": \"true\"</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>For metrics to be available for graph generation, you must also enable metrics collection for your deployment. Please see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#enabling_metrics_collection_for_an_existing_nim_deployment\">Enabling metrics collection for an existing NIM deployment</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_enabling_metrics_collection_for_an_existing_nim_deployment\">Enabling metrics collection for an existing NIM deployment</h5>\n<div class=\"paragraph\">\n<p>To enable metrics collection for your existing NIM deployment, you must manually add the Prometheus endpoint and port annotations to the <code>InferenceService</code> of your deployment.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following procedure describes how to add the required Prometheus annotations to the <code>InferenceService</code> of your NIM deployment.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You have an existing NIM deployment in Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift Container Platform cluster as a cluster administrator, log in to the OpenShift CLI.</p>\n</li>\n<li>\n<p>Confirm the name of the <code>InferenceService</code> associated with your NIM deployment:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get inferenceservice -n &lt;namespace&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <code>&lt;namespace&gt;</code> with the namespace of the project where your NIM model is deployed.</p>\n</div>\n</li>\n<li>\n<p>Check if there is an existing <code>spec.predictor.annotations</code> section in the <code>InferenceService</code> configuration:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get inferenceservice -n &lt;namespace&gt; &lt;inferenceservice-name&gt; -o json | jq '.spec.predictor.annotations'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace &lt;inferenceservice-name&gt; with the name of the <code>InferenceService</code> from the previous step.</p>\n</div>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If the <code>spec.predictor.annotations</code> section does not exist in the configuration, add the section and required annotations:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch inferenceservice -n &lt;namespace&gt; &lt;inference-name&gt; --type json --patch \\\n '[{\"op\": \"add\", \"path\": \"/spec/predictor/annotations\", \"value\": {\"prometheus.io/path\": \"/metrics\", \"prometheus.io/port\": \"8000\"}}]'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The annotation that you added appears in the output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>inferenceservice.serving.kserve.io/nim-serving-runtime patched</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If there is an existing <code>spec.predictor.annotations</code> section, add the Prometheus annotations to the section:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch inferenceservice -n &lt;namespace&gt; &lt;inference-service-name&gt; --type json --patch \\\n '[{\"op\": \"add\", \"path\": \"/spec/predictor/annotations/prometheus.io~1path\", \"value\": \"/metrics\"},\n {\"op\": \"add\", \"path\": \"/spec/predictor/annotations/prometheus.io~1port\", \"value\": \"8000\"}]'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The annotations that you added appears in the output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>inferenceservice.serving.kserve.io/nim-serving-runtime patched</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the annotations have been added to the <code>InferenceService</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get inferenceservice -n &lt;namespace&gt; &lt;inferenceservice-name&gt; -o json | jq '.spec.predictor.annotations'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the annotation that you added in the output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>{\n  \"prometheus.io/path\": \"/metrics\",\n  \"prometheus.io/port\": \"8000\"\n}</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-nvidia-nim-metrics-for-a-nim-model_serving-large-models\">Viewing NVIDIA NIM metrics for a NIM model</h4>\n<div class=\"paragraph _abstract\">\n<p>In Open Data Hub, you can observe the following NVIDIA NIM metrics for a NIM model deployed on the NVIDIA NIM model serving platform:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>GPU cache usage over time (ms)</strong></p>\n</li>\n<li>\n<p><strong>Current running, waiting, and max requests count</strong></p>\n</li>\n<li>\n<p><strong>Tokens count</strong></p>\n</li>\n<li>\n<p><strong>Time to first token</strong></p>\n</li>\n<li>\n<p><strong>Time per output token</strong></p>\n</li>\n<li>\n<p><strong>Request outcomes</strong></p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, the peak usage hours and model performance at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have enabled the NVIDIA NIM model serving platform.</p>\n</li>\n<li>\n<p>You have deployed a NIM model on the NVIDIA NIM model serving platform.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>The <code>disableKServeMetrics</code> Open Data Hub dashboard configuration option is set to its default value of <code>false</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disableKServeMetrics: false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the NIM model that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click the NIM model that you want to observe.</p>\n</li>\n<li>\n<p>On the <strong>NIM Metrics</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for NIM metrics.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>NIM Metrics</strong> tab shows graphs of NIM metrics for the deployed NIM model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.nvidia.com/nim/large-language-models/latest/observability.html\">NVIDIA NIM observability</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-performance-metrics-for-a-nim-model_serving-large-models\">Viewing performance metrics for a NIM model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can observe the following performance metrics for a NIM model deployed on the NVIDIA NIM model serving platform:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Number of requests</strong> - The number of requests that have failed or succeeded for a specific model.</p>\n</li>\n<li>\n<p><strong>Average response time (ms)</strong> - The average time it takes a specific model to respond to requests.</p>\n</li>\n<li>\n<p><strong>CPU utilization (%)</strong> - The percentage of the CPU limit per model replica that is currently utilized by a specific model.</p>\n</li>\n<li>\n<p><strong>Memory utilization (%)</strong> - The percentage of the memory limit per model replica that is utilized by a specific model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, the peak usage hours and model performance at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have enabled the NVIDIA NIM model serving platform.</p>\n</li>\n<li>\n<p>You have deployed a NIM model on the NVIDIA NIM model serving platform.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>The <code>disableKServeMetrics</code> Open Data Hub dashboard configuration option is set to its default value of <code>false</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disableKServeMetrics: false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the NIM model that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click the NIM model that you want to observe.</p>\n</li>\n<li>\n<p>On the <strong>Endpoint performance</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed to show the latest data. You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for performance metrics.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>Endpoint performance</strong> tab shows graphs of performance metrics for the deployed NIM model.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"serving-small-and-medium-sized-models_model-serving\">Serving models on the multi-model serving platform</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>On the multi-model serving platform, multiple models can be deployed from the same model server and share the server resources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_model_servers\">Configuring model servers</h3>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-multi-model-serving-platform_model-serving\">Enabling the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>To use the multi-model serving platform, you must first enable the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>Your cluster administrator has <em>not</em> edited the Open Data Hub dashboard configuration to disable the ability to select the multi-model serving platform, which uses the ModelMesh component. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2//html/managing_openshift_ai/customizing-the-dashboard#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Settings</strong> → <strong>Cluster settings</strong>.</p>\n</li>\n<li>\n<p>Locate the <strong>Model serving platforms</strong> section.</p>\n</li>\n<li>\n<p>Select the <strong>Multi-model serving platform</strong> checkbox.</p>\n</li>\n<li>\n<p>Click <strong>Save changes</strong>.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a custom model-serving runtime for the multi-model serving platform</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime adds support for a specified set of model frameworks and the model formats supported by those frameworks. By default, the multi-model serving platform includes the OpenVINO Model Server runtime. You can also add your own custom runtime if the default runtime does not meet your needs, such as supporting a specific model format.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub dashboard to add and enable a custom model-serving runtime. You can then choose the custom runtime when you create a new model server for the multi-model serving platform.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nRed&#160;Hat does not provide support for custom runtimes. You are responsible for ensuring that you are licensed to use any custom runtimes that you add, and for correctly configuring and maintaining them.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">add a model server to your project</a>. When you have added a custom model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n<li>\n<p>You have reviewed the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository. You can use these examples as starting points. However, each runtime requires some further modification before you can deploy it in Open Data Hub. The required modifications are described in the following procedure.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nOpen Data Hub includes the OpenVINO Model Server runtime by default. You do not need to add this runtime to Open Data Hub.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a custom runtime, choose one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To start with an existing runtime (for example the OpenVINO Model Server runtime), click the action menu (&#8942;) next to the existing runtime and then click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>To add a new custom runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Multi-model serving platform</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe multi-model serving platform supports only the REST protocol. Therefore, you cannot change the default value in the <strong>Select the API protocol this runtime supports</strong> list.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: If you started a new runtime (rather than duplicating an existing one), add your code by choosing one of the following options:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer. This file might be the one of the example runtimes that you downloaded from the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>Enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor. The YAML that you paste might be copied from one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: If you are adding one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, perform the following modifications:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the YAML editor, locate the <code>kind</code> field for your runtime. Update the value of this field to <code>ServingRuntime</code>.</p>\n</li>\n<li>\n<p>In the <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/config/runtimes/kustomization.yaml\" target=\"_blank\" rel=\"noopener\">kustomization.yaml</a> file in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, take note of the <code>newName</code> and <code>newTag</code> values for the runtime that you want to add. You will specify these values in a later step.</p>\n</li>\n<li>\n<p>In the YAML editor for your custom runtime, locate the <code>containers.image</code> field.</p>\n</li>\n<li>\n<p>Update the value of the <code>containers.image</code> field in the format <code>newName:newTag</code>, based on the values that you previously noted in the <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/config/runtimes/kustomization.yaml\" target=\"_blank\" rel=\"noopener\">kustomization.yaml</a> file. Some examples are shown.</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Nvidia Triton Inference Server</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: nvcr.io/nvidia/tritonserver:23.04-py3</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">Seldon Python MLServer</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: seldonio/mlserver:1.3.2</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">TorchServe</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: pytorch/torchserve:0.7.1-cpu</code></p>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, ensure that the value of the runtime you are adding is unique (that is, the value does not match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To configure a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: mlserver-0.x\n  annotations:\n    openshift.io/display-name: MLServer</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The custom model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a custom model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server to your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a tested and verified model-serving runtime for the multi-model serving platform</h4>\n<div class=\"paragraph\">\n<p>In addition to preinstalled and custom model-serving runtimes, you can also use Red&#160;Hat tested and verified model-serving runtimes such as the <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> to support your needs. For more information about Red&#160;Hat tested and verified runtimes, see <a href=\"https://access.redhat.com/articles/7089743\" target=\"_blank\" rel=\"noopener\">Tested and verified runtimes for Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use the Open Data Hub dashboard to add and enable the <strong>NVIDIA Triton Inference Server</strong> runtime and then choose the runtime when you create a new model server for the multi-model serving platform.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">add a model server to your project</a>. After you have added a tested and verified model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a tested and verified runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Multi-model serving platform</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe multi-model serving platform supports only the REST protocol. Therefore, you cannot change the default value in the <strong>Select the API protocol this runtime supports</strong> list.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations:\n    enable-route: \"true\"\n  name: modelmesh-triton\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    opendatahub.io/modelServingSupport: '[\"multi\"x`x`]'\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  builtInAdapter:\n    env:\n      - name: CONTAINER_MEM_REQ_BYTES\n        value: \"268435456\"\n      - name: USE_EMBEDDED_PULLER\n        value: \"true\"\n    memBufferBytes: 134217728\n    modelLoadingTimeoutMillis: 90000\n    runtimeManagementPort: 8001\n    serverType: triton\n  containers:\n    - args:\n        - -c\n        - 'mkdir -p /models/_triton_models;  chmod 777\n          /models/_triton_models;  exec\n          tritonserver \"--model-repository=/models/_triton_models\" \"--model-control-mode=explicit\" \"--strict-model-config=false\" \"--strict-readiness=false\" \"--allow-http=true\" \"--allow-grpc=true\"  '\n      command:\n        - /bin/sh\n      image: nvcr.io/nvidia/tritonserver@sha256:xxxxx\n      name: triton\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  grpcDataEndpoint: port:8001\n  grpcEndpoint: port:8085\n  multiModel: true\n  protocolVersions:\n    - grpc-v2\n    - v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - autoSelect: true\n      name: pytorch\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"\n    - autoSelect: true\n      name: tensorrt\n      version: \"7\"\n    - autoSelect: false\n      name: xgboost\n      version: \"1\"\n    - autoSelect: true\n      name: python\n      version: \"1\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, make sure that the value of the runtime you are adding does not match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To use a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: modelmesh-triton\n  annotations:\n    openshift.io/display-name: Triton ServingRuntime</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit the runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server to your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server for the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the multi-model serving platform, you must configure a model server to deploy models. If you require extra computing power for use with large datasets, you can assign accelerators to your model server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you use Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a model server to.</p>\n</li>\n<li>\n<p>You have enabled the multi-model serving platform.</p>\n</li>\n<li>\n<p>If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a custom model-serving runtime</a>.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU and AMD GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a model server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you see a <strong>​Multi-model serving platform</strong> tile, click <strong>Add model server</strong> on the tile.</p>\n</li>\n<li>\n<p>If you do not see any tiles, click the <strong>Add model server</strong> button.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Add model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model server name</strong> field, enter a unique name for the model server.</p>\n</li>\n<li>\n<p>From the <strong>Serving runtime</strong> list, select a model-serving runtime that is installed and enabled in your Open Data Hub deployment.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server and want to use GPUs, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Hardware profile</strong> list, select a hardware profile.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, hardware profiles are hidden from appearing in the dashboard navigation menu and user interface. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the <strong>Settings &#8594; Hardware profiles</strong> option in the dashboard navigation menu and the user interface components associated with hardware profiles, set the <code>disableHardwareProfiles</code> value to <code>false</code> in the <code>OdhDashboardConfig</code> custom resource (CR) in OpenShift Container Platform.\nFor more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: Click <strong>Customize resource requests and limit</strong> and update the following values:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>CPUs requests</strong> field, specify the number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limits</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requests</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limits</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> checkbox to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authentication</strong> section, select the <strong>Require token authentication</strong> checkbox to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The model server that you configured appears on the <strong>Models</strong> tab for the project, in the <strong>Models and model servers</strong> list.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: To update the model server, click the action menu (<strong>&#8942;</strong>) beside the model server and select <strong>Edit model server</strong>.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-model-server_model-serving\">Deleting a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>When you no longer need a model server to host models, you can remove it from your data science project.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nWhen you remove a model server, you also remove the models that are hosted on that model server. As a result, the models are no longer available to applications.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a data science project and an associated model server.</p>\n</li>\n<li>\n<p>You have notified the users of the applications that access the models that the models will no longer be available.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project from which you want to delete the model server.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose model server you want to delete and then click <strong>Delete model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the model server in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete model server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you deleted is no longer displayed on the <strong>Models</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_deployed_models\">Working with deployed models</h3>\n<div class=\"sect3\">\n<h4 id=\"deploying-a-model-using-the-multi-model-serving-platform_model-serving\">Deploying a model by using the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>You can deploy trained models on Open Data Hub to enable you to test and implement them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you have enabled the multi-model serving platform, you can deploy models on the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have enabled the multi-model serving platform.</p>\n</li>\n<li>\n<p>You have created a data science project and added a model server.</p>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Deploy model</strong>.</p>\n</li>\n<li>\n<p>Configure properties for deploying your model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model name</strong> field, enter a unique name for the model that you are deploying.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>To specify the location of the model you want to deploy from S3-compatible object storage, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To use an existing connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Existing connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a connection that you previously defined.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, enter the folder path that contains the model in your specified data source.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are deploying a registered model version with an existing S3 or URI data connection, some of your connection details might be autofilled. This depends on the type of data connection and the number of matching connections available in your data science project. For example, if only one matching connection exists, fields like the path, URI, endpoint, bucket, and region might populate automatically. Matching connections will be labeled as <strong>Recommended</strong>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To use a new connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>To define a new connection that your model can access, select <strong>New connection</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Add connection</strong> modal, select a <strong>Connection type</strong>. The <strong>S3 compatible object storage</strong> and <strong>URI</strong> options are pre-installed connection types. Additional options might be available if your Open Data Hub administrator added them.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add connection</strong> form opens with fields specific to the connection type that you selected.</p>\n</div>\n</li>\n<li>\n<p>Enter the connection detail fields.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) Customize the runtime parameters in the <strong>Configuration parameters</strong> section:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Modify the values in <strong>Additional serving runtime arguments</strong> to define how the deployed model behaves.</p>\n</li>\n<li>\n<p>Modify the values in <strong>Additional environment variables</strong> to define variables in the model&#8217;s environment.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model deployments</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to monitor your model for bias, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models\">Monitoring data science models</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-a-deployed-model_model-serving\">Viewing a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>To analyze the results of your work, you can view a list of deployed models on Open Data Hub. You can also view the current statuses of deployed models and their endpoints.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model deployments</strong> page opens.</p>\n</div>\n<div class=\"paragraph\">\n<p>For each model, the page shows details such as the model name, the project in which the model is deployed, the model-serving runtime that the model uses, and the deployment status.</p>\n</div>\n</li>\n<li>\n<p>Optional: For a given model, click the link in the <strong>Inference endpoint</strong> column to see the inference endpoints for the deployed model.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously deployed data science models is displayed on the <strong>Model deployments</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to monitor your model for bias, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models\">Monitoring data science models</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-the-deployment-properties-of-a-deployed-model_model-serving\">Updating the deployment properties of a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update the deployment properties of a model that has been deployed previously. For example, you can change the model&#8217;s connection and name.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model on Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model deployments</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the model whose deployment properties you want to update and click <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Update the deployment properties of the model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model name</strong> field, enter a new, unique name for your model.</p>\n</li>\n<li>\n<p>From the <strong>Model servers</strong> list, select a model server for your model.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optionally, update the connection by specifying an existing connection or by creating a new connection.</p>\n</li>\n<li>\n<p>Click <strong>Redeploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model whose deployment properties you updated is displayed on the <strong>Model deployments</strong> page of the dashboard.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-deployed-model_model-serving\">Deleting a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete models you have previously deployed. This enables you to remove deployed models that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model deployments</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the deployed model that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete deployed model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the deployed model in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete deployed model</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model that you deleted is no longer displayed on the <strong>Model deployments</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-monitoring-for-the-multi-model-serving-platform_model-serving\">Configuring monitoring for the multi-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The multi-model serving platform includes model and model server metrics for the ModelMesh component. ModelMesh generates its own set of metrics and does not rely on the underlying model-serving runtimes to provide them. The set of metrics that ModelMesh generates includes metrics for model request rates and timings, model loading and unloading rates, times and sizes, internal queuing delays, capacity and usage, cache state, and least recently-used models. For more information, see <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/docs/monitoring.md\" target=\"_blank\" rel=\"noopener\">ModelMesh metrics</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>After you have configured monitoring, you can view metrics for the ModelMesh component.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/index#preparing-to-configure-the-monitoring-stack-uwm\">creating a config map</a> for monitoring a user-defined workflow. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/index#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm\">enabling monitoring</a> for user-defined projects in OpenShift. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You have <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/monitoring/configuring-user-workload-monitoring#granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm\">assigned</a> the <code>monitoring-rules-view</code> role to users that will monitor metrics.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-conf.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>user-workload-monitoring-config</code> object configures the components that monitor user-defined projects.  Observe that the retention time is set to the recommended value of 15 days.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>user-workload-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-conf.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define another <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-enable.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>cluster-monitoring-config</code> object enables monitoring for user-defined projects.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-enable.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-metrics-for-the-multi-model-serving-platform_model-serving\">Viewing model-serving runtime metrics for the multi-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>After a cluster administrator has configured monitoring for the multi-model serving platform, non-admin users can use the OpenShift web console to view model-serving runtime metrics for the ModelMesh component.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>As described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>, use the web console to run queries for <code>modelmesh_*</code> metrics.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_monitoring_model_performance_2\">Monitoring model performance</h3>\n<div class=\"paragraph\">\n<p>In the multi-model serving platform, you can view performance metrics for all models deployed on a model server and for a specific model that is deployed on the model server.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-performance-metrics-for-model-server_model-serving\">Viewing performance metrics for all models on a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can monitor the following metrics for all the models that are deployed on a model server:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>HTTP requests per 5 minutes</strong> - The number of HTTP requests that have failed or succeeded for all models on the server.</p>\n</li>\n<li>\n<p><strong>Average response time (ms)</strong> - For all models on the server, the average time it takes the model server to respond to requests.</p>\n</li>\n<li>\n<p><strong>CPU utilization (%)</strong> - The percentage of the CPU&#8217;s capacity that is currently being used by all models on the server.</p>\n</li>\n<li>\n<p><strong>Memory utilization (%)</strong> - The percentage of the system&#8217;s memory that is currently being used by all models on the server.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, when the peak usage hours are and how the models are performing at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed models on the multi-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data science projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the data science models that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>In the row for the model server that you are interested in, click the action menu (&#8942;) and then select <strong>View model server metrics</strong>.</p>\n</li>\n<li>\n<p>Optional: On the metrics page for the model server, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for HTTP requests per 5 minutes, average response time, CPU utilization, and memory utilization.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>On the metrics page for the model server, the graphs provide data on performance metrics.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-http-request-metrics-for-a-deployed-model_model-serving\">Viewing HTTP request metrics for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a graph that illustrates the HTTP requests that have failed or succeeded for a specific model that is deployed on the multi-model serving platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>The following dashboard configuration options are set to the default values as shown:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disablePerformanceMetrics:false\ndisableKServeMetrics:false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed models on the multi-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Models</strong> &#8594; <strong>Model deployments</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Model deployments</strong> page, select the model that you are interested in.</p>\n</li>\n<li>\n<p>Optional: On the <strong>Endpoint performance</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>Endpoint performance</strong> tab shows a graph of the HTTP metrics for the model.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footnotes\">\n<hr>\n<div class=\"footnote\" id=\"_footnotedef_1\">\n<a href=\"#_footnoteref_1\">1</a>. vLLM CPU ServingRuntime for KServe\n</div>\n</div>","id":"51c2e86f-2eb0-5cb6-856d-e1fd131f5ce9","document":{"title":"Serving models"}},"markdownRemark":null},"pageContext":{"id":"51c2e86f-2eb0-5cb6-856d-e1fd131f5ce9"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}