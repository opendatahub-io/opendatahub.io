{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/monitoring-data-science-models/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Accessing S3-compatible object storage with self-signed certificates","level":2,"index":5,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":6,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":5,"id":"viewing-logs-and-audit-records_install"},{"parentId":"viewing-logs-and-audit-records_install","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_install"},{"parentId":"configuring-the-operator-logger_install","name":"Viewing the Open Data Hub Operator log","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"viewing-logs-and-audit-records_install","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_install"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping notebook servers owned by other users","level":3,"index":1,"id":"stopping-notebook-servers-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to Jupyter","level":3,"index":2,"id":"revoking-user-access-to-jupyter_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":2,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":3,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-data-science-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"_enabling_accelerators"},{"parentId":"_enabling_accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"_enabling_accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":"_enabling_accelerators","name":"AMD GPU Integration","level":2,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":3,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":3,"index":1,"id":"enabling-amd-gpus_managing-odh"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":7,"id":"managing-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Overview of Kueue resources","level":2,"index":0,"id":"overview-of-kueue-resources_managing-odh"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Resource flavor","level":3,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Cluster queue","level":3,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Local queue","level":3,"index":2,"id":"_local_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Example Kueue resource configurations","level":2,"index":1,"id":"example-kueue-resource-configurations_managing-odh"},{"parentId":"example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs without shared cohort","level":3,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":3,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":4,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":4,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":4,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":4,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"example-kueue-resource-configurations_managing-odh","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring quota management for distributed workloads","level":2,"index":2,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Enforcing the use of local queues","level":2,"index":3,"id":"enforcing-local-queues_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Enforcing the local-queue labeling policy for all projects","level":3,"index":0,"id":"enforcing-lqlabel-all_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Disabling the local-queue labeling policy for all projects","level":3,"index":1,"id":"disabling-lqlabel-all_managing-odh"},{"parentId":"enforcing-local-queues_managing-odh","name":"Enforcing the local-queue labeling policy for some projects only","level":3,"index":2,"id":"enforcing-lqlabel-some_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring the CodeFlare Operator","level":2,"index":4,"id":"configuring-the-codeflare-operator_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":3,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":3,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":3,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":3,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"},{"parentId":null,"name":"Backing up storage data","level":1,"index":8,"id":"backing-up-storage-data_managing-odh"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":9,"id":"viewing-logs-and-audit-records_managing-odh"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_managing-odh"},{"parentId":"configuring-the-operator-logger_managing-odh","name":"Viewing the Open Data Hub Operator log","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":1,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":2,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing connection types","level":1,"index":3,"id":"managing-connection-types"},{"parentId":"managing-connection-types","name":"Viewing connection types","level":2,"index":0,"id":"viewing-connection-types_managing-resources"},{"parentId":"managing-connection-types","name":"Creating a connection type","level":2,"index":1,"id":"creating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Duplicating a connection type","level":2,"index":2,"id":"duplicating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Editing a connection type","level":2,"index":3,"id":"editing-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Enabling a connection type","level":2,"index":4,"id":"enabling-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Deleting a connection type","level":2,"index":5,"id":"deleting-a-connection-type_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":4,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":0,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":2,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":5,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Troubleshooting common problems in Jupyter for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"A user&#8217;s notebook server does not start","level":3,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":1,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":2,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Supported bias metrics","level":2,"index":3,"id":"supported-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing data drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Supported drift metrics","level":2,"index":3,"id":"supported-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Supported explainers","level":2,"index":2,"id":"supported-explainers_explainers"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":6,"id":"evaluating-large-language-models_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job","level":2,"index":1,"id":"lmeval-evaluation-job_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval scenarios","level":2,"index":2,"id":"lmeval-scenarios_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Configuring the LM-Eval environment","level":3,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":"lmeval-scenarios_monitor","name":"Using a custom Unitxt card","level":3,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":"lmeval-scenarios_monitor","name":"Using PVCs as storage","level":3,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_monitor","name":"Using an InferenceService","level":3,"index":3,"id":"_using_an_inferenceservice"},{"parentId":null,"name":"Configuring the Guardrails Orchestrator service","level":1,"index":7,"id":"configuring-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Deploying the Guardrails Orchestrator service","level":2,"index":0,"id":"deploying-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Guardrails Orchestrator parameters","level":2,"index":1,"id":"guardrails-orchestrator-parameters_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Configuring the OpenTelemetry exporter","level":2,"index":2,"id":"configuring-the-opentelemetry-exporter_monitor"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":8,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":"about-model-serving_about-model-serving","name":"Single-model serving platform","level":2,"index":0,"id":"_single_model_serving_platform"},{"parentId":"about-model-serving_about-model-serving","name":"Multi-model serving platform","level":2,"index":1,"id":"_multi_model_serving_platform"},{"parentId":"about-model-serving_about-model-serving","name":"NVIDIA NIM model serving platform","level":2,"index":2,"id":"_nvidia_nim_model_serving_platform"},{"parentId":null,"name":"Serving models on the single-model serving platform","level":1,"index":1,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About KServe deployment modes","level":2,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":3,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":3,"index":1,"id":"_raw_deployment_mode"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models by using multiple GPU nodes","level":3,"index":4,"id":"deploying-models-using-multiple-gpu-nodes_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Setting a timeout for KServe","level":3,"index":5,"id":"setting-timeout-for-kserve_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizing the parameters of a deployed model-serving runtime","level":3,"index":6,"id":"customizing-parameters-serving-runtime_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizable model serving runtime parameters","level":3,"index":7,"id":"customizable-model-serving-runtime-parameters_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using accelerators with vLLM","level":3,"index":8,"id":"using-accelerators-with-vllm_serving-large-models"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"NVIDIA GPUs","level":4,"index":0,"id":"_nvidia_gpus"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"Intel Gaudi accelerators","level":4,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"AMD GPUs","level":4,"index":2,"id":"_amd_gpus"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":3,"index":9,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Storing a model in an OCI image","level":4,"index":0,"id":"storing-a-model-in-oci-image_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image","level":4,"index":1,"id":"deploying-model-stored-in-oci-image_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the authentication token for a deployed model","level":3,"index":10,"id":"accessing-authentication-token-for-deployed-model_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":11,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Monitoring model performance","level":2,"index":6,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a Grafana metrics dashboard","level":3,"index":1,"id":"Deploying-a-grafana-metrics-dashboard_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Optimizing model-serving runtimes","level":2,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Enabling speculative decoding and multi-modal inferencing","level":3,"index":0,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Supported model-serving runtimes","level":2,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Tested and verified model-serving runtimes","level":2,"index":10,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Inference endpoints","level":2,"index":11,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":3,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":3,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":3,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime with Gaudi accelerators support for KServe","level":3,"index":5,"id":"_vllm_servingruntime_with_gaudi_accelerators_support_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ROCm ServingRuntime for KServe","level":3,"index":6,"id":"_vllm_rocm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":3,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":3,"index":8,"id":"_additional_resources"},{"parentId":"serving-large-models_serving-large-models","name":"About the NVIDIA NIM model serving platform","level":2,"index":12,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":3,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":3,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":3,"index":2,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling graph generation for an existing NIM deployment","level":4,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling metrics collection for an existing NIM deployment","level":4,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing NVIDIA NIM metrics for a NIM model","level":3,"index":3,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing performance metrics for a NIM model","level":3,"index":4,"id":"viewing-performance-metrics-for-a-nim-model_serving-large-models"},{"parentId":null,"name":"Serving models on the multi-model serving platform","level":1,"index":2,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance_2"},{"parentId":"_monitoring_model_performance_2","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance_2","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":1,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Requirements for upgrading Open Data Hub version 2","level":2,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":3,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Requirements for upgrading Open Data Hub version 1","level":2,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":1,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":3,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing notebooks","level":2,"index":0,"id":"creating-and-importing-notebooks_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on notebooks by using Git","level":2,"index":1,"id":"collaborating-on-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in Jupyter for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-jupyter-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"_working_in_code_server"},{"parentId":"_working_in_code_server","name":"Creating code-server workbenches","level":2,"index":0,"id":"creating-code-server-workbenches_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Creating a workbench","level":3,"index":0,"id":"creating-a-project-workbench_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Uploading an existing notebook file to code-server from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_ide"},{"parentId":"_working_in_code_server","name":"Collaborating on workbenches in code-server by using Git","level":2,"index":1,"id":"collaborating-on-workbenches-in-code-server-by-using-git_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using code-server","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Updating your project in code-server with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Pushing project changes in code-server to a Git repository","level":3,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_ide"},{"parentId":"_working_in_code_server","name":"Managing Python packages in code-server","level":2,"index":2,"id":"managing-python-packages-in-code-server_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Viewing Python packages installed on your code-server workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Installing Python packages on your code-server workbench","level":3,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_ide"},{"parentId":"_working_in_code_server","name":"Installing extensions with code-server","level":2,"index":3,"id":"installing-extensions-with-code-server_ide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using connections","level":1,"index":2,"id":"using-connections_projects"},{"parentId":"using-connections_projects","name":"Adding a connection to your data science project","level":2,"index":0,"id":"adding-a-connection-to-your-data-science-project_projects"},{"parentId":"using-connections_projects","name":"Updating a connection","level":2,"index":1,"id":"updating-a-connection_projects"},{"parentId":"using-connections_projects","name":"Deleting a connection","level":2,"index":2,"id":"deleting-a-connection_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Configuring access to a data science project","level":2,"index":0,"id":"configuring-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":1,"id":"enabling-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":2,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":3,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":4,"id":"amd-gpu-integration_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":5,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"About GPU time slicing","level":1,"index":6,"id":"about-gpu-time-slicing_accelerators"},{"parentId":null,"name":"Enabling GPU time slicing","level":1,"index":7,"id":"enabling-gpu-time-slicing_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using the Jupyter application","level":1,"index":3,"id":"_using_the_jupyter_application"},{"parentId":"_using_the_jupyter_application","name":"Starting a Jupyter notebook server","level":2,"index":0,"id":"starting-a-jupyter-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Creating and importing notebooks","level":2,"index":1,"id":"creating-and-importing-notebooks_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_using_the_jupyter_application","name":"Collaborating on notebooks by using Git","level":2,"index":2,"id":"collaborating-on-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Updating notebook server settings by restarting your server","level":2,"index":4,"id":"updating-notebook-server-settings-by-restarting-your-server_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":1,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Managing data science pipelines","level":1,"index":0,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"configuring-a-pipeline-server_ds-pipelines","name":"Configuring a pipeline server with an external Amazon RDS database","level":3,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of data science pipelines caching","level":2,"index":12,"id":"overview-of-data-science-pipelines-caching_ds-pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Disabling caching for specific tasks or pipelines","level":3,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":4,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Verification and troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":1,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in an experiment","level":2,"index":7,"id":"comparing-runs-in-an-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in different experiments","level":2,"index":8,"id":"comparing-runs-in-different-experiments_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":5,"id":"troubleshooting-dspa-component-errors_ds-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ds-pipelines","name":"Common errors across DSP components","level":2,"index":0,"id":"_common_errors_across_dsp_components"},{"parentId":null,"name":"Migrating to data science pipelines 2.0","level":1,"index":6,"id":"migrating-to-data-science-pipelines-2_ds-pipelines"},{"parentId":"migrating-to-data-science-pipelines-2_ds-pipelines","name":"Upgrading to data science pipelines 2.0","level":2,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":"migrating-to-data-science-pipelines-2_ds-pipelines","name":"Removing data science pipelines 1.0 resources","level":2,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"},{"parentId":null,"name":"Additional resources","level":1,"index":7,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Managing custom training images","level":1,"index":1,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Downloading the demo notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Running the demo notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Tuning a model by using the Training Operator","level":1,"index":4,"id":"tuning-a-model-by-using-the-training-operator_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the training job","level":2,"index":0,"id":"configuring-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Running the training job","level":2,"index":1,"id":"running-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Monitoring the training job","level":2,"index":2,"id":"monitoring-the-training-job_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster doesn&#8217;t start","level":2,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of model registries","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":null,"name":"Configuring the model registry component","level":0,"index":1,"id":"_configuring_the_model_registry_component"},{"parentId":"_configuring_the_model_registry_component","name":"Configuring the model registry component","level":1,"index":0,"id":"configuring-the-model-registry-component_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Editing a model registry","level":1,"index":1,"id":"editing-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":2,"id":"managing-model-registry-permissions_model-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":3,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing the deployment properties of a deployed model version from a model registry","level":2,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":3,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":3,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deleting a deployed model version from a model registry","level":2,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":12,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-workbenches-in-code-server-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using code-server","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project in code-server with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes in code-server to a Git repository","level":1,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":1,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-guardrails-orchestrator-service/"},"sections":[{"parentId":null,"name":"Deploying the Guardrails Orchestrator service","level":1,"index":0,"id":"deploying-the-guardrails-orchestrator-service_{context}"},{"parentId":null,"name":"Guardrails Orchestrator parameters","level":1,"index":1,"id":"guardrails-orchestrator-parameters_{context}"},{"parentId":null,"name":"Configuring the OpenTelemetry exporter","level":1,"index":2,"id":"configuring-the-opentelemetry-exporter_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for your model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-code-server-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench","level":1,"index":0,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-large-language-models/"},"sections":[{"parentId":null,"name":"Setting up LM-Eval","level":1,"index":0,"id":"setting-up-lmeval_{context}"},{"parentId":null,"name":"LM-Eval evaluation job","level":1,"index":1,"id":"lmeval-evaluation-job_{context}"},{"parentId":null,"name":"LM-Eval scenarios","level":1,"index":2,"id":"lmeval-scenarios_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Configuring the LM-Eval environment","level":2,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":"lmeval-scenarios_{context}","name":"Using a custom Unitxt card","level":2,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":"lmeval-scenarios_{context}","name":"Using PVCs as storage","level":2,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":3,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":3,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_{context}","name":"Using an InferenceService","level":2,"index":3,"id":"_using_an_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enforcing-local-queues/"},"sections":[{"parentId":null,"name":"Enforcing the local-queue labeling policy for all projects","level":1,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":null,"name":"Disabling the local-queue labeling policy for all projects","level":1,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":null,"name":"Enforcing the local-queue labeling policy for some projects only","level":1,"index":2,"id":"enforcing-lqlabel-some_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Configuring custom namespaces","level":1,"index":0,"id":"configuring-custom-namespaces"},{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Configuring access to a data science project","level":1,"index":0,"id":"configuring-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Sharing access to a data science project","level":1,"index":1,"id":"sharing-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":2,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":3,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-connection-types/"},"sections":[{"parentId":null,"name":"Viewing connection types","level":1,"index":0,"id":"viewing-connection-types_{context}"},{"parentId":null,"name":"Creating a connection type","level":1,"index":1,"id":"creating-a-connection-type_{context}"},{"parentId":null,"name":"Duplicating a connection type","level":1,"index":2,"id":"duplicating-a-connection-type_{context}"},{"parentId":null,"name":"Editing a connection type","level":1,"index":3,"id":"editing-a-connection-type_{context}"},{"parentId":null,"name":"Enabling a connection type","level":1,"index":4,"id":"enabling-a-connection-type_{context}"},{"parentId":null,"name":"Deleting a connection type","level":1,"index":5,"id":"deleting-a-connection-type_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":"configuring-a-pipeline-server_{context}","name":"Configuring a pipeline server with an external Amazon RDS database","level":2,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"},{"parentId":null,"name":"Overview of data science pipelines caching","level":1,"index":12,"id":"overview-of-data-science-pipelines-caching_{context}"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Disabling caching for specific tasks or pipelines","level":2,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":3,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Verification and troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of Kueue resources","level":1,"index":0,"id":"overview-of-kueue-resources_{context}"},{"parentId":"overview-of-kueue-resources_{context}","name":"Resource flavor","level":2,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_{context}","name":"Cluster queue","level":2,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_{context}","name":"Local queue","level":2,"index":2,"id":"_local_queue"},{"parentId":null,"name":"Example Kueue resource configurations","level":1,"index":1,"id":"example-kueue-resource-configurations_{context}"},{"parentId":"example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs without shared cohort","level":2,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":3,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":3,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":3,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":3,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":2,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":3,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":3,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":3,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":3,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"example-kueue-resource-configurations_{context}","name":"Additional resources","level":2,"index":2,"id":"_additional_resources"},{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":2,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Enforcing the use of local queues","level":1,"index":3,"id":"enforcing-local-queues_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Enforcing the local-queue labeling policy for all projects","level":2,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Disabling the local-queue labeling policy for all projects","level":2,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":"enforcing-local-queues_{context}","name":"Enforcing the local-queue labeling policy for some projects only","level":2,"index":2,"id":"enforcing-lqlabel-some_{context}"},{"parentId":null,"name":"Configuring the CodeFlare Operator","level":1,"index":4,"id":"configuring-the-codeflare-operator_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":2,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs in an experiment","level":1,"index":7,"id":"comparing-runs-in-an-experiment_{context}"},{"parentId":null,"name":"Comparing runs in different experiments","level":1,"index":8,"id":"comparing-runs-in-different-experiments_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_{context}"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":1,"id":"installing-python-packages-on-your-notebook-server_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages-in-code-server/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your code-server workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your code-server workbench","level":1,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"Configuring storage class settings","level":1,"index":0,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":2,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping notebook servers owned by other users","level":2,"index":1,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to Jupyter","level":2,"index":2,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing data drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Supported drift metrics","level":1,"index":3,"id":"supported-drift-metrics_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":3,"id":"supported-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Downloading the demo notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Running the demo notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"About KServe deployment modes","level":1,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":2,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":2,"index":1,"id":"_raw_deployment_mode"},{"parentId":null,"name":"Installing KServe","level":1,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models by using multiple GPU nodes","level":2,"index":4,"id":"deploying-models-using-multiple-gpu-nodes_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Setting a timeout for KServe","level":2,"index":5,"id":"setting-timeout-for-kserve_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizing the parameters of a deployed model-serving runtime","level":2,"index":6,"id":"customizing-parameters-serving-runtime_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Customizable model serving runtime parameters","level":2,"index":7,"id":"customizable-model-serving-runtime-parameters_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using accelerators with vLLM","level":2,"index":8,"id":"using-accelerators-with-vllm_serving-large-models"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"Intel Gaudi accelerators","level":3,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"using-accelerators-with-vllm_serving-large-models","name":"AMD GPUs","level":3,"index":2,"id":"_amd_gpus"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":2,"index":9,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Storing a model in an OCI image","level":3,"index":0,"id":"storing-a-model-in-oci-image_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image","level":3,"index":1,"id":"deploying-model-stored-in-oci-image_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the authentication token for a deployed model","level":2,"index":10,"id":"accessing-authentication-token-for-deployed-model_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":11,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":6,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"_monitoring_model_performance","name":"Deploying a Grafana metrics dashboard","level":2,"index":1,"id":"Deploying-a-grafana-metrics-dashboard_serving-large-models"},{"parentId":null,"name":"Optimizing model-serving runtimes","level":1,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Enabling speculative decoding and multi-modal inferencing","level":2,"index":0,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Supported model-serving runtimes","level":1,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":null,"name":"Tested and verified model-serving runtimes","level":1,"index":10,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":null,"name":"Inference endpoints","level":1,"index":11,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":2,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":2,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":2,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime with Gaudi accelerators support for KServe","level":2,"index":5,"id":"_vllm_servingruntime_with_gaudi_accelerators_support_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ROCm ServingRuntime for KServe","level":2,"index":6,"id":"_vllm_rocm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":2,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":2,"index":8,"id":"_additional_resources"},{"parentId":null,"name":"About the NVIDIA NIM model serving platform","level":1,"index":12,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":2,"index":2,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling graph generation for an existing NIM deployment","level":3,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_serving-large-models","name":"Enabling metrics collection for an existing NIM deployment","level":3,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing NVIDIA NIM metrics for a NIM model","level":2,"index":3,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Viewing performance metrics for a NIM model","level":2,"index":4,"id":"viewing-performance-metrics-for-a-nim-model_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":1,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":2,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/tuning-a-model-by-using-the-training-operator/"},"sections":[{"parentId":null,"name":"Configuring the training job","level":1,"index":0,"id":"configuring-the-training-job_{context}"},{"parentId":null,"name":"Running the training job","level":1,"index":1,"id":"running-the-training-job_{context}"},{"parentId":null,"name":"Monitoring the training job","level":1,"index":2,"id":"monitoring-the-training-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 1","level":1,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":1,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 2","level":1,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-connections/"},"sections":[{"parentId":null,"name":"Adding a connection to your data science project","level":1,"index":0,"id":"adding-a-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating a connection","level":1,"index":1,"id":"updating-a-connection_{context}"},{"parentId":null,"name":"Deleting a connection","level":1,"index":2,"id":"deleting-a-connection_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Supported explainers","level":1,"index":2,"id":"supported-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/viewing-logs-and-audit-records/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_{context}"},{"parentId":"configuring-the-operator-logger_{context}","name":"Viewing the {productname-short} Operator log","level":2,"index":0,"id":"_viewing_the_productname_short_operator_log"},{"parentId":null,"name":"Viewing audit records","level":1,"index":1,"id":"viewing-audit-records_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":5,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":6,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":0,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Editing the deployment properties of a deployed model version from a model registry","level":1,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":2,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":2,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":null,"name":"Deleting a deployed model version from a model registry","level":1,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":12,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":2,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":3,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":4,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":5,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":6,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-model-stored-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Configuring the LM-Eval environment","level":1,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using an InferenceService","level":1,"index":3,"id":"_using_an_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-to-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Removing data science pipelines 1.0 resources","level":1,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM ServingRuntime with Gaudi accelerators support for KServe","level":1,"index":5,"id":"_vllm_servingruntime_with_gaudi_accelerators_support_for_kserve"},{"parentId":null,"name":"vLLM ROCm ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_rocm_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-accelerators-with-vllm/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-model-stored-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Configuring the LM-Eval environment","level":1,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"_using_a_custom_unitxt_card"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using an InferenceService","level":1,"index":3,"id":"_using_an_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-to-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":0,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Removing data science pipelines 1.0 resources","level":1,"index":1,"id":"_removing_data_science_pipelines_1_0_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM ServingRuntime with Gaudi accelerators support for KServe","level":1,"index":5,"id":"_vllm_servingruntime_with_gaudi_accelerators_support_for_kserve"},{"parentId":null,"name":"vLLM ROCm ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_rocm_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-accelerators-with-vllm/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#overview-of-model-monitoring_monitor\">Overview of model monitoring</a></li>\n<li><a href=\"#configuring-trustyai_monitor\">Configuring TrustyAI</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-monitoring-for-your-model-serving-platform_monitor\">Configuring monitoring for your model serving platform</a></li>\n<li><a href=\"#enabling-trustyai-component_monitor\">Enabling the TrustyAI component</a></li>\n<li><a href=\"#configuring-trustyai-with-a-database_monitor\">Configuring TrustyAI with a database</a></li>\n<li><a href=\"#installing-trustyai-service_monitor\">Installing the TrustyAI service for a project</a></li>\n</ul>\n</li>\n<li><a href=\"#setting-up-trustyai-for-your-project_monitor\">Setting up TrustyAI for your project</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a></li>\n<li><a href=\"#sending-training-data-to-trustyai_monitor\">Sending training data to TrustyAI</a></li>\n<li><a href=\"#labeling-data-fields_monitor\">Labeling data fields</a></li>\n</ul>\n</li>\n<li><a href=\"#monitoring-model-bias_bias-monitoring\">Monitoring model bias</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#creating-a-bias-metric_bias-monitoring\">Creating a bias metric</a></li>\n<li><a href=\"#deleting-a-bias-metric_bias-monitoring\">Deleting a bias metric</a></li>\n<li><a href=\"#viewing-bias-metrics_bias-monitoring\">Viewing bias metrics for a model</a></li>\n<li><a href=\"#supported-bias-metrics_bias-monitoring\">Supported bias metrics</a></li>\n</ul>\n</li>\n<li><a href=\"#monitoring-data-drift_drift-monitoring\">Monitoring data drift</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#creating-a-drift-metric_drift-monitoring\">Creating a drift metric</a></li>\n<li><a href=\"#deleting-a-drift-metric-using-cli_drift-monitoring\">Deleting a drift metric by using the CLI</a></li>\n<li><a href=\"#viewing-drift-metrics_drift-monitoring\">Viewing data drift metrics for a model</a></li>\n<li><a href=\"#supported-drift-metrics_drift-monitoring\">Supported drift metrics</a></li>\n</ul>\n</li>\n<li><a href=\"#using-explainability_explainers\">Using explainability</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#requesting-a-lime-explanation_explainers\">Requesting a LIME explanation</a></li>\n<li><a href=\"#requesting-a-shap-explanation_explainers\">Requesting a SHAP explanation</a></li>\n<li><a href=\"#supported-explainers_explainers\">Supported explainers</a></li>\n</ul>\n</li>\n<li><a href=\"#evaluating-large-language-models_monitor\">Evaluating large language models</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#setting-up-lmeval_monitor\">Setting up LM-Eval</a></li>\n<li><a href=\"#lmeval-evaluation-job_monitor\">LM-Eval evaluation job</a></li>\n<li><a href=\"#lmeval-scenarios_monitor\">LM-Eval scenarios</a></li>\n</ul>\n</li>\n<li><a href=\"#configuring-the-guardrails-orchestrator-service_monitor\">Configuring the Guardrails Orchestrator service</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#deploying-the-guardrails-orchestrator-service_monitor\">Deploying the Guardrails Orchestrator service</a></li>\n<li><a href=\"#guardrails-orchestrator-parameters_monitor\">Guardrails Orchestrator parameters</a></li>\n<li><a href=\"#configuring-the-opentelemetry-exporter_monitor\">Configuring the OpenTelemetry exporter</a></li>\n</ul>\n</li>\n<li><a href=\"#bias-monitoring-tutorial_bias-tutorial\">Bias monitoring tutorial - Gender bias example</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#t-bias-introduction_bias-tutorial\">Introduction</a></li>\n<li><a href=\"#t-bias-setting-up-your-environment_bias-tutorial\">Setting up your environment</a></li>\n<li><a href=\"#t-bias-deploying-models_bias-tutorial\">Deploying models</a></li>\n<li><a href=\"#t-bias-sending-training-data-to-the-models_bias-tutorial\">Sending training data to the models</a></li>\n<li><a href=\"#t-bias-labeling-data-fields_bias-tutorial\">Labeling data fields</a></li>\n<li><a href=\"#t-bias-checking-model-fairness_bias-tutorial\">Checking model fairness</a></li>\n<li><a href=\"#t-bias-scheduling-a-fairness-metric-request_bias-tutorial\">Scheduling a fairness metric request</a></li>\n<li><a href=\"#t-bias-scheduling-an-identity-metric-request_bias-tutorial\">Scheduling an identity metric request</a></li>\n<li><a href=\"#t-bias-simulating-real-world-data_bias-tutorial\">Simulating real world data</a></li>\n<li><a href=\"#t-bias-reviewing-the-results_bias-tutorial\">Reviewing the results</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"overview-of-model-monitoring_monitor\">Overview of model monitoring</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To ensure that machine-learning models are transparent, fair, and reliable, data scientists can use TrustyAI in Open Data Hub to monitor their data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>Data scientists can monitor their data science models in Open Data Hub for the following metrics:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Bias</dt>\n<dd>\n<p>Check for unfair patterns or biases in data and model predictions to ensure your model&#8217;s decisions are unbiased.</p>\n</dd>\n<dt class=\"hdlist1\">Data drift</dt>\n<dd>\n<p>Detect changes in input data distributions over time by comparing the latest real-world data to the original training data. Comparing the data identifies shifts or deviations that could impact model performance, ensuring that the model remains accurate and reliable.</p>\n</dd>\n<dt class=\"hdlist1\">Explainability</dt>\n<dd>\n<p>Understand how your model makes its predictions and decisions.</p>\n</dd>\n<dt class=\"hdlist1\">LLM evaluation</dt>\n<dd>\n<p>Monitor your Large Language Models (LLMs) against a range of metrics, in order to ensure the accuracy and quality of its output.</p>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"configuring-trustyai_monitor\">Configuring TrustyAI</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To configure model monitoring with TrustyAI for data scientists to use in Open Data Hub, a cluster administrator does the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Configure monitoring for the model serving platform</p>\n</li>\n<li>\n<p>Enable the TrustyAI component in the Open Data Hub Operator</p>\n</li>\n<li>\n<p>Configure TrustyAI to use a database, if you want to use your database instead of a PVC for storage with TrustyAI</p>\n</li>\n<li>\n<p>Install the TrustyAI service on each data science project that contains models that the data scientists want to monitor</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-monitoring-for-your-model-serving-platform_monitor\">Configuring monitoring for your model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub provides the following model serving platforms:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Single-model serving platform</dt>\n<dd>\n<p>For deploying large models such as large language models (LLMs), Open Data Hub includes a single model serving platform that is based on the <a href=\"https://github.com/kserve/kserve\">KServe</a> component. Each model is deployed from its own model server. Use the single model serving platform in situations where you need to deploy, monitor, scale, and maintain large models that require increased resources.</p>\n</dd>\n<dt class=\"hdlist1\">Multi-model serving platform</dt>\n<dd>\n<p>For deploying small and medium-sized models, Open Data Hub includes a multi-model serving platform that is based on the <a href=\"https://github.com/kserve/modelmesh\">ModelMesh</a> component. On the multi-model serving platform, you can deploy multiple models on the same model server. Each of the deployed models shares the server resources. This approach can be useful on Open Data Hub clusters that have finite compute resources or pods.</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p>The process for configuring monitoring for either a single (KServe) or a multi-model (ModelMesh) serving platform is the same.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-core-platform-monitoring#preparing-to-configure-the-monitoring-stack\">creating a config map</a> for monitoring a user-defined workflow. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm\">enabling monitoring</a> for user-defined projects in OpenShift. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You have <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-user-workload-monitoring#granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm\">assigned</a> the <code>monitoring-rules-view</code> role to users that will monitor metrics.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-conf.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>user-workload-monitoring-config</code> object configures the components that monitor user-defined projects.  Observe that the retention time is set to the recommended value of 15 days.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>user-workload-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-conf.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define another <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-enable.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>cluster-monitoring-config</code> object enables monitoring for user-defined projects.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-enable.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"enabling-trustyai-component_monitor\">Enabling the TrustyAI component</h3>\n<div class=\"paragraph _abstract\">\n<p>To allow your data scientists to use model monitoring with TrustyAI, you must enable the TrustyAI component in Open Data Hub.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have access to the data science cluster.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>In the <code>spec:components</code> section, set the <code>managementState</code> field for the <code>trustyai</code> component to <code>Managed</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre> trustyai:\n    managementState: Managed</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the <strong>trustyai-service-operator</strong> pod:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, from the <strong>Project</strong> list, select <strong>opendatahub</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>trustyai-service-operator-contoller-manager</strong> deployment.\nCheck the status:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click the deployment name to open the deployment details page.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.</p>\n</li>\n<li>\n<p>View the pod status.</p>\n<div class=\"paragraph\">\n<p>When the status of the <strong>trustyai-service-operator-controller-manager-<em>&lt;pod-id&gt;</em></strong> pod is <strong>Running</strong>, the pod is ready to use.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-trustyai-with-a-database_monitor\">Configuring TrustyAI with a database</h3>\n<div class=\"paragraph _abstract\">\n<p>If you have a relational database in your OpenShift Container Platform cluster such as MySQL or MariaDB, you can configure TrustyAI to use your database instead of a persistent volume claim (PVC). Using a database instead of a PVC for storage can improve scalability, performance, and data management in TrustyAI.\nProvide TrustyAI with a database configuration secret before deployment. You can create a secret or specify the name of an existing Kubernetes secret within your project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You have enabled the TrustyAI component, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#enabling-trustyai-component_monitor\">Enabling the TrustyAI component</a>.</p>\n</li>\n<li>\n<p>The data scientist has created a data science project, as described in <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>, that contains the models that the data scientist wants to monitor.</p>\n</li>\n<li>\n<p>If you are configuring the TrustyAI service with an external MySQL database, your database must already be in your cluster and use at least MySQL version 5.x. However, Red&#160;Hat recommends that you use MySQL version 8.x.</p>\n</li>\n<li>\n<p>If you are configuring the TrustyAI service with a MariaDB database, your database must already be in your cluster and use MariaDB version 10.3 or later. However, Red&#160;Hat recommends that you use at least MariaDB version 10.5.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The transport security layer (TLS) protocol does not work with the MariaDB operator 0.29 or later versions.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: If you want to use a TLS connection between TrustyAI and the database, create a TrustyAI service database TLS secret that uses the same certificates that you want to use for the database.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file to contain your TLS secret and add the following code:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: <em>&lt;service_name&gt;</em>-db-tls\ntype: kubernetes.io/tls\ndata:\n  tls.crt: |\n    <em>&lt;TLS CERTIFICATE&gt;</em>\n\n  tls.key: |\n    <em>&lt;TLS KEY&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Save the file with the file name <strong><em>&lt;service_name&gt;</em>-db-tls.yaml</strong>. For example, if your service name is <code>trustyai-service</code>, save the file as <strong>trustyai-service-db-tls.yaml</strong>.</p>\n</li>\n<li>\n<p>Apply the YAML file in the data science project that contains the models that the data scientist wants to monitor:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f <em>&lt;service_name&gt;</em>-db-tls.yaml -n <em>&lt;project_name&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create a secret (or specify an existing one) that has your database credentials.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file to contain your secret and add the following code:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\nstringData:\n  databaseKind: <em>&lt;mariadb&gt;</em> <b class=\"conum\">(1)</b>\n  databaseUsername: <em>&lt;TrustyAI_username&gt;</em> <b class=\"conum\">(2)</b>\n  databasePassword: <em>&lt;TrustyAI_password&gt;</em> <b class=\"conum\">(3)</b>\n  databaseService: mariadb-service <b class=\"conum\">(4)</b>\n  databasePort: <em>3306</em> <b class=\"conum\">(5)</b>\n  databaseGeneration: update <b class=\"conum\">(6)</b>\n  databaseName: trustyai_service <b class=\"conum\">(7)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>The only currently supported <code>databaseKind</code> value is <code>mariadb</code>.</p>\n</li>\n<li>\n<p>The username you want TrustyAI to use when interfacing with the database.</p>\n</li>\n<li>\n<p>The password that TrustyAI must use when connecting to the database.</p>\n</li>\n<li>\n<p>The Kubernetes (K8s) service that TrustyAI must use when connecting to the database (the default <code>mariadb</code>) .</p>\n</li>\n<li>\n<p>The port that TrustyAI must use when connecting to the database (default is 3306).</p>\n</li>\n<li>\n<p>The database schema generation strategy to be used by TrustyAI. It is the setting for the <a href=\"https://quarkus.io/guides/hibernate-orm#quarkus-hibernate-orm_quarkus-hibernate-orm-database-generation\"><code>quarkus.hibernate-orm.database.generation</code></a> argument, which determines how TrustyAI interacts with the database on its initial connection. Set to <code>none</code>, <code>create</code>, <code>drop-and-create</code>, <code>drop</code>, <code>update</code>, or <code>validate</code>.</p>\n</li>\n<li>\n<p>The name of the individual database within the database service that the username and password authenticate to, as well as the specific database name that TrustyAI should read and write to on the database server.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Save the file with the file name <strong>db-credentials.yaml</strong>. You will need this name later when you install or change the TrustyAI service.</p>\n</li>\n<li>\n<p>Apply the YAML file in the data science project that contains the models that the data scientist wants to monitor:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f db-credentials.yaml -n <em>&lt;project_name&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If you are installing TrustyAI for the first time on a project, continue to <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#installing-trustyai-service_monitor\">Installing the TrustyAI service for a project</a>.</p>\n<div class=\"paragraph\">\n<p>If you already installed TrustyAI on a project, you can migrate the existing TrustyAI service from using a PVC to using a database.</p>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file to update the TrustyAI service custom resource (CR) and add the following code:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: TrustyAIService\nmetadata:\n  annotations:\n    trustyai.opendatahub.io/db-migration: \"true\" <b class=\"conum\">(1)</b>\n  name: trustyai-service <b class=\"conum\">(2)</b>\nspec:\n  storage:\n    format: \"DATABASE\" <b class=\"conum\">(3)</b>\n    folder: \"/inputs\" <b class=\"conum\">(4)</b>\n      size: \"1Gi\" <b class=\"conum\">(5)</b>\n    databaseConfigurations: <em>&lt;database_secret_credentials&gt;</em> <b class=\"conum\">(6)</b>\n  data:\n    filename: \"data.csv\" <b class=\"conum\">(7)</b>\n  metrics:\n    schedule: \"5s\" <b class=\"conum\">(8)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Set to <code>true</code> to prompt the migration from PVC to database storage.</p>\n</li>\n<li>\n<p>The name of the TrustyAI service instance.</p>\n</li>\n<li>\n<p>The storage format for the data. Set this field to <code>DATABASE</code>.</p>\n</li>\n<li>\n<p>The location within the PVC where you were storing the data. This must match the value specified in the existing CR.</p>\n</li>\n<li>\n<p>The size of the data to request.</p>\n</li>\n<li>\n<p>The name of the secret with your database credentials that you created in an earlier step. For example, <code>db-credentials</code>.</p>\n</li>\n<li>\n<p>The suffix for the existing stored data files. This must match the value specified in the existing CR.</p>\n</li>\n<li>\n<p>The interval at which to calculate the metrics. The default is <code>5s</code>. The duration is specified with the ISO-8601 format. For example, <code>5s</code> for 5 seconds, <code>5m</code> for 5 minutes, and <code>5h</code> for 5 hours.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Save the file. For example, <strong>trustyai_crd.yaml</strong>.</p>\n</li>\n<li>\n<p>Apply the new TrustyAI service CR to the data science project that contains the models that the data scientist wants to monitor:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f trustyai_crd.yaml -n <em>&lt;project_name&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-trustyai-service_monitor\">Installing the TrustyAI service for a project</h3>\n<div class=\"paragraph _abstract\">\n<p>Install the TrustyAI service on a data science project to provide access to its features for all models deployed within that project. An instance of the TrustyAI service is required for each data science project, or namespace, that contains models that the data scientists want to monitor.</p>\n</div>\n<div class=\"paragraph\">\n<p>Use the Open Data Hub dashboard or the OpenShift command-line interface (CLI) to install an instance of the TrustyAI service.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Install only one instance of the TrustyAI service in a project. Multiple instances in the same project can result in unexpected behavior.</p>\n</div>\n<div class=\"paragraph\">\n<p>Installing TrustyAI into a namespace where non-OVMS models are deployed can cause errors in the TrustyAI service.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"installing-trustyai-service-using-dashboard_monitor\">Installing the TrustyAI service by using the dashboard</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the Open Data Hub dashboard to install an instance of the TrustyAI service.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A cluster administrator has configured monitoring for the model serving platform, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-monitoring-for-the-multi-model-serving-platform_monitor\">Configuring monitoring for the multi-model serving platform</a>.</p>\n</li>\n<li>\n<p>A cluster administrator has enabled the TrustyAI component, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#enabling-trustyai-component_monitor\">Enabling the TrustyAI component</a>.</p>\n</li>\n<li>\n<p>If you are using TrustyAI with a database instead of PVC, a cluster administrator has configured TrustyAI to use the database, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-trustyai-with-a-database_monitor\">Configuring TrustyAI with a database</a>.</p>\n</li>\n<li>\n<p>The data scientist has created a data science project, as described in <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>, that contains the models that the data scientist wants to monitor.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the models that the data scientist wants to monitor.</p>\n<div class=\"paragraph\">\n<p>The project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Settings</strong> tab.</p>\n</li>\n<li>\n<p>Select the <strong>Enable model bias monitoring</strong> checkbox.</p>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Workloads</strong> → <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>From the project list, select the project in which you installed TrustyAI.</p>\n</li>\n<li>\n<p>Confirm that the <strong>Pods</strong> list includes a running pod for the TrustyAI service. The pod has a naming pattern similar to the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>trustyai-service-5d45b5884f-96h5z</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"installing-trustyai-service-using-cli_monitor\">Installing the TrustyAI service by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to install an instance of the TrustyAI service.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You have configured monitoring for the model serving platform, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-monitoring-for-the-multi-model-serving-platform_monitor\">Configuring monitoring for the multi-model serving platform</a>.</p>\n</li>\n<li>\n<p>You have enabled the TrustyAI component, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#enabling-trustyai-component_monitor\">Enabling the TrustyAI component</a>.</p>\n</li>\n<li>\n<p>If you are using TrustyAI with a database instead of PVC, you have configured TrustyAI to use the database, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-trustyai-with-a-database_monitor\">Configuring TrustyAI with a database</a>.</p>\n</li>\n<li>\n<p>The data scientist has created a data science project, as described in <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>, that contains the models that the data scientist wants to monitor.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster as a cluster administrator:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the OpenShift Container Platform web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Navigate to the data science project that contains the models that the data scientist wants to monitor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc project <em>&lt;project_name&gt;</em></code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>oc project my-project</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a <code>TrustyAIService</code> custom resource (CR) file, for example <code>trustyai_crd.yaml</code>:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example CR file for TrustyAI using a database</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: TrustyAIService\nmetadata:\n  name: trustyai-service <b class=\"conum\">(1)</b>\nspec:\n  storage:\n\t  format: \"DATABASE\" <b class=\"conum\">(2)</b>\n\t  size: \"1Gi\" <b class=\"conum\">(3)</b>\n\t  databaseConfigurations: <em>&lt;database_secret_credentials&gt;</em> <b class=\"conum\">(4)</b>\n  metrics:\n  \tschedule: \"5s\" <b class=\"conum\">(5)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>The name of the TrustyAI service instance.</p>\n</li>\n<li>\n<p>The storage format for the data, either <code>DATABASE</code> or <code>PVC</code> (persistent volume claim). Red&#160;Hat recommends that you use a database setup for better scalability, performance, and data management in TrustyAI.</p>\n</li>\n<li>\n<p>The size of the data to request.</p>\n</li>\n<li>\n<p>The name of the secret with your database credentials that you created in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-trustyai-with-a-database_monitor\">Configuring TrustyAI with a database</a>. For example, <code>db-credentials</code>.</p>\n</li>\n<li>\n<p>The interval at which to calculate the metrics. The default is <code>5s</code>. The duration is specified with the ISO-8601 format. For example, <code>5s</code> for 5 seconds, <code>5m</code> for 5 minutes, and <code>5h</code> for 5 hours.</p>\n</li>\n</ol>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example CR file for TrustyAI using a PVC</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: TrustyAIService\nmetadata:\n  name: trustyai-service <b class=\"conum\">(1)</b>\nspec:\n  storage:\n\t  format: \"PVC\" <b class=\"conum\">(2)</b>\n\t  folder: \"/inputs\" <b class=\"conum\">(3)</b>\n\t  size: \"1Gi\" <b class=\"conum\">(4)</b>\n  data:\n\t  filename: \"data.csv\" <b class=\"conum\">(5)</b>\n\t  format: \"CSV\" <b class=\"conum\">(6)</b>\n  metrics:\n  \tschedule: \"5s\" <b class=\"conum\">(7)</b>\n  \tbatchSize: 5000 <b class=\"conum\">(8)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>The name of the TrustyAI service instance.</p>\n</li>\n<li>\n<p>The storage format for the data, either <code>DATABASE</code> or <code>PVC</code> (persistent volume claim).</p>\n</li>\n<li>\n<p>The location within the PVC where you want to store the data.</p>\n</li>\n<li>\n<p>The size of the PVC to request.</p>\n</li>\n<li>\n<p>The suffix for the stored data files.</p>\n</li>\n<li>\n<p>The format of the data. Currently, only comma-separated value (CSV) format is supported.</p>\n</li>\n<li>\n<p>The interval at which to calculate the metrics. The default is <code>5s</code>. The duration is specified with the ISO-8601 format. For example, <code>5s</code> for 5 seconds, <code>5m</code> for 5 minutes, and <code>5h</code> for 5 hours.</p>\n</li>\n<li>\n<p>(Optional) The observation&#8217;s historical window size to use for metrics calculation. The default is <code>5000</code>, which means that the metrics are calculated using the 5,000 latest inferences.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Add the TrustyAI service&#8217;s CR to your project:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>oc apply -f trustyai_crd.yaml</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This command returns output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>trusty-service created</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Verify that you installed the TrustyAI service:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>oc get pods | grep trustyai</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You should see a response similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>trustyai-service-5d45b5884f-96h5z             1/1     Running</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"setting-up-trustyai-for-your-project_monitor\">Setting up TrustyAI for your project</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To set up model monitoring with TrustyAI for a data science project, a data scientist does the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Authenticate the TrustyAI service</p>\n</li>\n<li>\n<p>Send training data to TrustyAI for bias or data drift monitoring</p>\n</li>\n<li>\n<p>Label your data fields (optional)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>After setting up, a data scientist can create and view bias and data drift metrics for deployed models.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</h3>\n<div class=\"paragraph _abstract\">\n<p>To access TrustyAI service external endpoints, you must provide OAuth proxy (oauth-proxy) authentication. You must obtain a user token, or a token from a service account with sufficient privileges, and then pass the token to the TrustyAI service when using <code>curl</code> commands.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You installed the OpenShift command line interface (<code>oc</code>) as described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Enter the following command to set a user token variable on OpenShift Container Platform:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>export TOKEN=$(oc whoami -t)</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Enter the following command to check the user token variable:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>echo $TOKEN</pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Next step</div>\n<p>When running <code>curl</code> commands, pass the token to the TrustyAI service using the Authorization header. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" $TRUSTY_ROUTE</pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"sending-training-data-to-trustyai_monitor\">Sending training data to TrustyAI</h3>\n<div class=\"paragraph _abstract\">\n<p>To use TrustyAI for bias monitoring or data drift detection, you must send training data for your model to TrustyAI.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You authenticated the TrustyAI service as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a>.</p>\n</li>\n<li>\n<p>Your deployed model is registered with TrustyAI.</p>\n<div class=\"paragraph\">\n<p>Verify that the TrustyAI service has registered your deployed model, as follows:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, navigate to <strong>Workloads</strong> → <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>From the project list, select the project that contains your deployed model.</p>\n</li>\n<li>\n<p>Select the pod for your serving platform (for example, <code>modelmesh-serving-ovms-1.x-xxxxx</code>).</p>\n</li>\n<li>\n<p>On the <strong>Environment</strong> tab, verify that the <code>MM_PAYLOAD_PROCESSORS</code> environment variable is set.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Get the inference endpoints for the deployed model, as described in <a href=\"https://opendatahub.io/docs/serving-models/#accessing-inference-endpoint-for-deployed-model_serving-large-models\">Accessing the inference endpoint for a deployed model</a>.</p>\n</li>\n<li>\n<p>Send data to this endpoint. For more information, see the <a href=\"https://kserve.github.io/website/0.8/modelserving/inference_api/#server-metadata-response-json-object\">KServe v2 Inference Protocol documentation</a>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Follow these steps to view cluster metrics and verify that TrustyAI is receiving data.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Metrics</strong> page, click the <strong>Select query</strong> list and then select <strong>Custom query</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Expression</strong> field, enter <code>trustyai_model_observations_total</code> and press Enter. Your model should be listed and reporting observed inferences.</p>\n</li>\n<li>\n<p>Optional: Select a time range from the list above the graph. For example, select <strong>5m</strong>.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"labeling-data-fields_monitor\">Labeling data fields</h3>\n<div class=\"paragraph _abstract\">\n<p>After you send model training data to TrustyAI, you might want to apply a set of name mappings to your inputs and outputs so that the field names are meaningful and easier to work with.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You sent training data to TrustyAI as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#sending-training-data-to-trustyai_monitor\">Sending training data to TrustyAI</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the OpenShift CLI, get the route to the TrustyAI service:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>To examine TrustyAI&#8217;s model metadata, query the <code>/info</code> endpoint:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -H \"Authorization: Bearer $TOKEN\" $TRUSTY_ROUTE/info | jq \".[0].data\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This outputs a JSON file containing the following information for each model:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The names, data types, and positions of input fields and output fields.</p>\n</li>\n<li>\n<p>The observed field values.</p>\n</li>\n<li>\n<p>The total number of input-output pairs observed.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Use <code>POST /info/names</code> to apply name mappings to the fields, similar to the following example.</p>\n<div class=\"paragraph\">\n<p>Change the <code>model-name</code>, <code>original-name</code>, and <code>Prediction</code> values to those used in your model. Change the <code>New name</code> values to the labels that you want to use.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -sk -H \"Authorization: Bearer $TOKEN\" -X POST --location $TRUSTY_ROUTE/info/names \\\n  -H \"Content-Type: application/json\"   \\\n  -d \"{\n    \\\"modelId\\\": \\\"model-name\\\",\n    \\\"inputMapping\\\":\n      {\n        \\\"original-name-0\\\": \\\"New name 0\\\",\n        \\\"original-name-1\\\": \\\"New name 1\\\",\n        \\\"original-name-2\\\": \\\"New name 2\\\",\n        \\\"original-name-3\\\": \\\"New name 3\\\",\n      },\n    \\\"outputMapping\\\": {\n      \\\"predict-0\\\": \\\"Prediction 0\\\"\n    }\n  }\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For another example, see <a href=\"https://github.com/trustyai-explainability/odh-trustyai-demos/blob/main/2-BiasMonitoring/kserve-demo/scripts/apply_name_mapping.sh\" class=\"bare\">https://github.com/trustyai-explainability/odh-trustyai-demos/blob/main/2-BiasMonitoring/kserve-demo/scripts/apply_name_mapping.sh</a>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>A \"Feature and output name mapping successfully applied\" message is displayed.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"monitoring-model-bias_bias-monitoring\">Monitoring model bias</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you might want to monitor your machine learning models for bias. This means monitoring for algorithmic deficiencies that might skew the outcomes or decisions that the model produces. Importantly, this type of monitoring helps you to ensure that the model is not biased against particular protected groups or features.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub provides a set of metrics that help you to monitor your models for bias. You can use the Open Data Hub interface to choose an available metric and then configure model-specific details such as a protected attribute, the privileged and unprivileged groups, the outcome you want to monitor, and a threshold for bias. You then see a chart of the calculated values for a specified number of model inferences.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about the specific bias metrics, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-bias-metrics_bias-monitoring\">Supported bias metrics</a>.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-bias-metric_bias-monitoring\">Creating a bias metric</h3>\n<div class=\"paragraph _abstract\">\n<p>To monitor a deployed model for bias, you must first create bias metrics. When you create a bias metric, you specify details relevant to your model such as a protected attribute, privileged and unprivileged groups, a model outcome and a value that you want to monitor, and the acceptable threshold for bias.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about the specific bias metrics, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-bias-metrics_bias-monitoring\">Supported bias metrics</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>For the complete list of TrustyAI metrics, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html\">TrustyAI service API</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can create a bias metric for a model by using the Open Data Hub dashboard or by using the OpenShift command-line interface (CLI).</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-a-bias-metric-using-dashboard_bias-monitoring\">Creating a bias metric by using the dashboard</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the Open Data Hub dashboard to create a bias metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You are familiar with <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-bias-metrics_bias-monitoring\">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>\n</li>\n<li>\n<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You set up TrustyAI for your data science project, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#setting-up-trustyai-for-your-project_monitor\">Setting up TrustyAI for your project</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Optional: To set the <code>TRUSTY_ROUTE</code> variable, follow these steps.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In a terminal window, log in to the OpenShift cluster where Open Data Hub is deployed.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>oc login</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Deployed models</strong> page, select your project from the drop-down list.</p>\n</li>\n<li>\n<p>Click the name of the model that you want to configure bias metrics for.</p>\n</li>\n<li>\n<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Configure bias metrics</strong> dialog, complete the following steps to configure bias metrics:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Metric name</strong> field, type a unique name for your bias metric. Note that you cannot change the name of this metric later.</p>\n</li>\n<li>\n<p>From the <strong>Metric type</strong> list, select one of the metrics types that are available in Open Data Hub.</p>\n</li>\n<li>\n<p>In the <strong>Protected attribute</strong> field, type the name of an attribute in your model that you want to monitor for bias.</p>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Tip</div>\n</td>\n<td class=\"content\">\nYou can use a <code>curl</code> command to query the metadata endpoint and view input attribute names and values. For example: <code>curl -H \"Authorization: Bearer $TOKEN\" $TRUSTY_ROUTE/info | jq \".[0].data.inputSchema\"</code>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Privileged value</strong> field, type the name of a privileged group for the protected attribute that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Unprivileged value</strong> field, type the name of an unprivileged group for the protected attribute that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Output</strong> field, type the name of the model outcome that you want to monitor for bias.</p>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Tip</div>\n</td>\n<td class=\"content\">\nYou can use a <code>curl</code> command to query the metadata endpoint and view output attribute names and values. For example: <code>curl -H \"Authorization: Bearer $TOKEN\" $TRUSTY_ROUTE/info | jq \".[0].data.outputSchema\"</code>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Output value</strong> field, type the value of the outcome that you want to monitor for bias.</p>\n</li>\n<li>\n<p>In the <strong>Violation threshold</strong> field, type the bias threshold for your selected metric type. This threshold value defines how far the specified metric can be from the fairness value for your metric, before the model is considered biased.</p>\n</li>\n<li>\n<p>In the <strong>Metric batch size</strong> field, type the number of model inferences that Open Data Hub includes each time it calculates the metric.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Ensure that the values you entered are correct.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You cannot edit a model bias metric configuration after you create it. Instead, you can duplicate a metric and then edit (configure) it; however, the history of the original metric is not applied to the copy.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Bias metric configuration</strong> page shows the bias metrics that you configured for your model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Next step</div>\n<p>To view metrics, on the <strong>Bias metric configuration</strong> page, click <strong>View metrics</strong> in the upper-right corner.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-a-bias-metric-using-cli_bias-monitoring\">Creating a bias metric by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to create a bias metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You are familiar with <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-bias-metrics_bias-monitoring\">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>\n</li>\n<li>\n<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You set up TrustyAI for your data science project, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#setting-up-trustyai-for-your-project_monitor\">Setting up TrustyAI for your project</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, log in to the OpenShift cluster where Open Data Hub is deployed.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>oc login</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optionally, get the full list of TrustyAI service endpoints and payloads.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" --location $TRUSTY_ROUTE/q/openapi</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>POST /metrics/group/fairness/spd/request</code> to schedule a recurring bias monitoring metric with the following syntax and payload structure:</p>\n<div class=\"paragraph\">\n<p><strong>Syntax</strong>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -sk -H \"Authorization: Bearer $TOKEN\" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/request  \\\n --header 'Content-Type: application/json' \\\n --data &lt;payload&gt;</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>Payload structure</strong>:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><code>modelId</code></dt>\n<dd>\n<p>The name of the model to query.</p>\n</dd>\n<dt class=\"hdlist1\"><code>protectedAttribute</code></dt>\n<dd>\n<p>The name of the feature that distinguishes the groups that you are checking for fairness.</p>\n</dd>\n<dt class=\"hdlist1\"><code>privilegedAttribute</code></dt>\n<dd>\n<p>The suspected favored (positively biased) class.</p>\n</dd>\n<dt class=\"hdlist1\"><code>unprivilegedAttribute</code></dt>\n<dd>\n<p>The suspected unfavored (negatively biased) class.</p>\n</dd>\n<dt class=\"hdlist1\"><code>outcomeName</code></dt>\n<dd>\n<p>The name of the output that provides the output you are examining for fairness.</p>\n</dd>\n<dt class=\"hdlist1\"><code>favorableOutcome</code></dt>\n<dd>\n<p>The value of the <code>outcomeName</code> output that describes the favorable or desired model prediction.</p>\n</dd>\n<dt class=\"hdlist1\"><code>batchSize</code></dt>\n<dd>\n<p>The number of previous inferences to include in the calculation.</p>\n</dd>\n</dl>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -sk -H \"Authorization: Bearer $TOKEN\" -X POST --location $TRUSTY_ROUTE /metrics/group/fairness/spd/request \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"demo-loan-nn-onnx-alpha\\\",\n                 \\\"protectedAttribute\\\": \\\"Is Male-Identifying?\\\",\n                 \\\"privilegedAttribute\\\": 1.0,\n                 \\\"unprivilegedAttribute\\\": 0.0,\n                 \\\"outcomeName\\\": \\\"Will Default?\\\",\n                 \\\"favorableOutcome\\\": 0,\n                 \\\"batchSize\\\": 5000\n             }\"</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The bias metrics request should return output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>{\n   \"timestamp\":\"2023-10-24T12:06:04.586+00:00\",\n   \"type\":\"metric\",\n   \"value\":-0.0029676404469311524,\n   \"namedValues\":null,\n   \"specificDefinition\":\"The SPD of -0.002968 indicates that the likelihood of Group:Is Male-Identifying?=1.0 receiving Outcome:Will Default?=0 was -0.296764 percentage points lower than that of Group:Is Male-Identifying?=0.0.\",\n   \"name\":\"SPD\",\n   \"id\":\"d2707d5b-cae9-41aa-bcd3-d950176cbbaf\",\n   \"thresholds\":{\"lowerBound\":-0.1,\"upperBound\":0.1,\"outsideBounds\":false}\n}</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>specificDefinition</code> field helps you understand the real-world interpretation of these metric values. For this example, the model is fair over the <code>Is Male-Identifying?</code> field, with the rate of positive outcome only differing by about -0.3%.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-a-bias-metric_bias-monitoring\">Duplicating a bias metric</h4>\n<div class=\"paragraph _abstract\">\n<p>If you want to edit an existing metric, you can duplicate (copy) it in the Open Data Hub interface and then edit the values in the copy. However, note that the history of the original metric is not applied to the copy.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You are familiar with <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-bias-metrics_bias-monitoring\">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>\n</li>\n<li>\n<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>\n</li>\n<li>\n<p>There is an existing bias metric that you want to duplicate.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Deployed models</strong> page, click the name of the model with the bias metric that you want to duplicate.</p>\n</li>\n<li>\n<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Bias metric configuration</strong> page, click the action menu (&#8942;) next to the metric that you want to copy and then click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Configure bias metric</strong> dialog, follow these steps:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Metric name</strong> field, type a unique name for your bias metric. Note that you cannot change the name of this metric later.</p>\n</li>\n<li>\n<p>Change the values of the fields as needed. For a description of these fields, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#creating-a-bias-metric-using-dashboard_bias-monitoring\">Creating a bias metric by using the dashboard</a>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Ensure that the values you entered are correct, and then click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Bias metric configuration</strong> page shows the bias metrics that you configured for your model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Next step</div>\n<p>To view metrics, on the <strong>Bias metric configuration</strong> page, click <strong>View metrics</strong> in the upper-right corner.</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-bias-metric_bias-monitoring\">Deleting a bias metric</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete a bias metric for a model by using the Open Data Hub dashboard or by using the OpenShift command-line interface (CLI).</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-bias-metric-using-dashboard_bias-monitoring\">Deleting a bias metric by using the dashboard</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the Open Data Hub dashboard to delete a bias metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>There is an existing bias metric that you want to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Deployed models</strong> page, click the name of the model with the bias metric that you want to delete.</p>\n</li>\n<li>\n<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n<li>\n<p>Click the action menu (&#8942;) next to the metric that you want to delete and then click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete bias metric</strong> dialog, type the metric name to confirm the deletion.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You cannot undo deleting a bias metric.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Delete bias metric</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Bias metric configuration</strong> page does not show the bias metric that you deleted.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-bias-metric-using-cli_bias-monitoring\">Deleting a bias metric by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to delete a bias metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>).</p>\n</li>\n<li>\n<p>You have a user token for authentication as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a>.</p>\n</li>\n<li>\n<p>There is an existing bias metric that you want to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the OpenShift CLI, get the route to the TrustyAI service:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: To list all currently active requests for a metric, use <code>GET /metrics/{{metric}}/requests</code>. For example, to list all currently scheduled SPD metrics, type:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/spd/requests\"</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, to list all currently scheduled metric requests, use <code>GET /metrics/all/requests</code>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/all/requests\"</pre>\n</div>\n</div>\n</li>\n<li>\n<p>To delete a metric, send an HTTP <code>DELETE</code> request to the <code>/metrics/$METRIC/request</code> endpoint to stop the periodic calculation, including the id of periodic task that you want to cancel in the payload. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X DELETE --location \"$TRUSTY_ROUTE/metrics/spd/request\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n          \\\"requestId\\\": \\\"3281c891-e2a5-4eb3-b05d-7f3831acbb56\\\"\n        }\"</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Use <code>GET /metrics/{{metric}}/requests</code> to list all currently active requests for the metric and verify the metric that you deleted is not shown. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/spd/requests\"</pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-bias-metrics_bias-monitoring\">Viewing bias metrics for a model</h3>\n<div class=\"paragraph _abstract\">\n<p>After you create bias monitoring metrics, you can use the Open Data Hub dashboard to view and update the metrics that you configured.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisite</div>\n<ul>\n<li>\n<p>You configured bias metrics for your model as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#creating-a-bias-metric_bias-monitoring\">Creating a bias metric</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Deployed models</strong> page, click the name of a model that you want to view bias metrics for.</p>\n</li>\n<li>\n<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>\n</li>\n<li>\n<p>To update the metrics shown on the page, follow these steps:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Metrics to display</strong> section, use the <strong>Select a metric</strong> list to select a metric to show on the page.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nEach time you select a metric to show on the page, an additional <strong>Select a metric</strong> list appears. This enables you to show multiple metrics on the page.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>From the <strong>Time range</strong> list in the upper-right corner, select a value.</p>\n</li>\n<li>\n<p>From the <strong>Refresh interval</strong> list in the upper-right corner, select a value.</p>\n<div class=\"paragraph\">\n<p>The metrics page shows the metrics that you selected.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: To remove one or more metrics from the page, in the <strong>Metrics to display</strong> section, perform one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To remove an individual metric, click the cancel icon (&#10006;) next to the metric name.</p>\n</li>\n<li>\n<p>To remove all metrics, click the cancel icon (&#10006;) in the <strong>Select a metric</strong> list.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: To return to configuring bias metrics for the model, on the metrics page, click <strong>Configure</strong> in the upper-right corner.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The metrics page shows the metrics selections that you made.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"supported-bias-metrics_bias-monitoring\">Supported bias metrics</h3>\n<div class=\"paragraph\">\n<p>Open Data Hub supports the following bias metrics:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Statistical Parity Difference</dt>\n<dd>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p><em>Statistical Parity Difference</em> (SPD) is the difference in the probability of a favorable outcome prediction between unprivileged and privileged groups. The formal definition of SPD is the following:</p>\n</div>\n<div class=\"imageblock text-center\">\n<div class=\"content\">\n<img src=\"/static/docs/images/bias-metric-spd.png\" alt=\"SPD definition\">\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><em>&#375;</em> = 1 is the favorable outcome.</p>\n</li>\n<li>\n<p><em>D&#7524;</em> and <em>D&#8346;</em> are the unprivileged and privileged group data.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can interpret SPD values as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>A value of <code>0</code> means that the model is behaving fairly for a selected attribute (for example,  race, gender).</p>\n</li>\n<li>\n<p>A value in the range  <code>-0.1</code> to <code>0.1</code> means that the model is reasonably fair for a selected attribute. Instead, you can attribute the difference in probability to other factors, such as the sample size.</p>\n</li>\n<li>\n<p>A value outside the range <code>-0.1</code> to <code>0.1</code> indicates that the model is unfair for a selected attribute.</p>\n</li>\n<li>\n<p>A negative value indicates that the model has bias against the unprivileged group.</p>\n</li>\n<li>\n<p>A positive value indicates that the model has bias against the privileged group.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</dd>\n<dt class=\"hdlist1\">Disparate Impact Ratio</dt>\n<dd>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p><em>Disparate Impact Ratio</em> (DIR) is the ratio of the probability of a favorable outcome prediction for unprivileged groups to that of privileged groups. The formal definition of DIR is the following:</p>\n</div>\n<div class=\"imageblock text-center\">\n<div class=\"content\">\n<img src=\"/static/docs/images/bias-metric-dir.png\" alt=\"DIR definition\">\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><em>&#375;</em> = 1 is the favorable outcome.</p>\n</li>\n<li>\n<p><em>D&#7524;</em> and <em>D&#8346;</em> are the unprivileged and privileged group data.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The threshold to identify bias depends on your own criteria and specific use case.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, if your threshold for identifying bias is represented  by a DIR value below <code>0.8</code> or above <code>1.2</code>, you can interpret the DIR values as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>A value of <code>1</code> means that the model is fair for a selected attribute.</p>\n</li>\n<li>\n<p>A value of between <code>0.8</code> and <code>1.2</code> means that the model is reasonably fair for a selected attribute.</p>\n</li>\n<li>\n<p>A value below <code>0.8</code> or above <code>1.2</code> indicates bias.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"monitoring-data-drift_drift-monitoring\">Monitoring data drift</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you might want to monitor your deployed models for data drift. Data drift refers to changes in the distribution or properties of incoming data that differ significantly from the data on which the model was originally trained. Detecting data drift helps ensure that your models continue to perform as expected and that they remain accurate and reliable.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use data drift monitoring metrics from TrustyAI in Open Data Hub to provide a quantitative measure of the alignment between the training data and the inference data.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about the specific data drift metrics, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-drift-metrics_drift-monitoring\">Supported drift metrics</a>.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-drift-metric_drift-monitoring\">Creating a drift metric</h3>\n<div class=\"paragraph _abstract\">\n<p>To monitor a deployed model for data drift, you must first create drift metrics.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about the specific data drift metrics, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-drift-metrics_drift-monitoring\">Supported drift metrics</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>For the complete list of TrustyAI metrics, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html\">TrustyAI service API</a>.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-a-drift-metric-using-cli_drift-monitoring\">Creating a drift metric by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to create a data drift metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You are familiar with the specific data set schema and understand the relevant inputs and outputs.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You set up TrustyAI for your data science project, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#setting-up-trustyai-for-your-project_monitor\">Setting up TrustyAI for your project</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optionally, get the full list of TrustyAI service endpoints and payloads.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" --location $TRUSTY_ROUTE/q/openapi</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>POST /metrics/drift/meanshift/request</code> to schedule a recurring drift monitoring metric with the following syntax and payload structure:</p>\n<div class=\"paragraph\">\n<p><strong>Syntax</strong>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -k -H \"Authorization: Bearer $TOKEN\" -X POST --location $TRUSTY_ROUTE/metrics/drift/meanshift/request \\\n --header 'Content-Type: application/json' \\\n --data &lt;payload&gt;</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>Payload structure</strong>:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><code>modelId</code></dt>\n<dd>\n<p>The name of the model to monitor.</p>\n</dd>\n<dt class=\"hdlist1\"><code>referenceTag</code></dt>\n<dd>\n<p>The data to use as the reference distribution.</p>\n</dd>\n</dl>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -k -H \"Authorization: Bearer $TOKEN\" -X POST --location $TRUSTY_ROUTE/metrics/drift/meanshift/request \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"gaussian-credit-model\\\",\n                 \\\"referenceTag\\\": \\\"TRAINING\\\"\n             }\"</pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-drift-metric-using-cli_drift-monitoring\">Deleting a drift metric by using the CLI</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to delete a drift metric for a model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>).</p>\n</li>\n<li>\n<p>You have a user token for authentication as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a>.</p>\n</li>\n<li>\n<p>There is an existing drift metric that you want to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the OpenShift Container Platform web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the OpenShift CLI, get the route to the TrustyAI service:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: To list all currently active requests for a metric, use <code>GET /metrics/{{metric}}/requests</code>. For example, to list all currently scheduled MeanShift metrics, type:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -k -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/drift/meanshift/requests\"</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, to list all currently scheduled metric requests, use <code>GET /metrics/all/requests</code>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/all/requests\"</pre>\n</div>\n</div>\n</li>\n<li>\n<p>To delete a metric, send an HTTP <code>DELETE</code> request to the <code>/metrics/$METRIC/request</code> endpoint to stop the periodic calculation, including the id of periodic task that you want to cancel in the payload. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -k -H \"Authorization: Bearer $TOKEN\" -X DELETE --location \"$TRUSTY_ROUTE/metrics/drift/meanshift/request\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n          \\\"requestId\\\": \\\"$id\\\"\n        }\"</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Use <code>GET /metrics/{{metric}}/requests</code> to list all currently active requests for the metric and verify the metric that you deleted is not shown. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -H \"Authorization: Bearer $TOKEN\" -X GET --location \"$TRUSTY_ROUTE/metrics/drift/meanshift/requests\"</pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-drift-metrics_drift-monitoring\">Viewing data drift metrics for a model</h3>\n<div class=\"paragraph _abstract\">\n<p>After you create data drift monitoring metrics, use the OpenShift Container Platform web console to view and update the metrics that you configured.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have been assigned the <code>monitoring-rules-view</code> role. For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-user-workload-monitoring#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm#granting-users-permission-to-configure-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects\" target=\"_blank\" rel=\"noopener\">Granting users permission to configure monitoring for user-defined projects</a>.</p>\n</li>\n<li>\n<p>You are familiar with how to monitor project metrics in the OpenShift Container Platform web console. For more information, see\n<a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>As described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>, use the web console to run queries for <code>trustyai_*</code> metrics.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"supported-drift-metrics_drift-monitoring\">Supported drift metrics</h3>\n<div class=\"paragraph\">\n<p>Open Data Hub supports the following data drift metrics:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">MeanShift</dt>\n<dd>\n<p>The MeanShift metric calculates the per-column probability that the data values in a test data set are from the same distribution as those in a training data set (assuming that the values are normally distributed). This metric measures the difference in the means of specific features between the two datasets.</p>\n<div class=\"paragraph\">\n<p>MeanShift is useful for identifying straightforward changes in data distributions, such as when the entire distribution has shifted to the left or right along the feature axis.</p>\n</div>\n<div class=\"paragraph\">\n<p>This metric returns the probability that the distribution seen in the \"real world\" data is derived from the same distribution as the reference data. The closer the value is to 0, the more likely there is to be significant drift.</p>\n</div>\n</dd>\n<dt class=\"hdlist1\">FourierMMD</dt>\n<dd>\n<p>The FourierMMD metric provides the probability that the data values in a test data set have drifted from the training data set distribution, assuming that the computed Maximum Mean Discrepancy (MMD) values are normally distributed. This metric compares the empirical distributions of the data sets by using an MMD measure in the Fourier domain.</p>\n<div class=\"paragraph\">\n<p>FourierMMD is useful for detecting subtle shifts in data distributions that might be overlooked by simpler statistical measures.</p>\n</div>\n<div class=\"paragraph\">\n<p>This metric returns the probability that the distribution seen in the \"real world\" data has drifted from the reference data. The closer the value is to 1, the more likely there is to be significant drift.</p>\n</div>\n</dd>\n<dt class=\"hdlist1\">KSTest</dt>\n<dd>\n<p>The KSTest metric calculates two Kolmogorov-Smirnov tests for each column to determine whether the data sets are derived from the same distributions. This metric measures the maximum distance between the empirical cumulative distribution functions (CDFs) of the data sets, without assuming any specific underlying distribution.</p>\n<div class=\"paragraph\">\n<p>KSTest is useful for detecting changes in distribution shape, location, and scale.</p>\n</div>\n<div class=\"paragraph\">\n<p>This metric returns the probability that the distribution seen in the \"real world\" data is derived from the same distribution as the reference data. The closer the value is to 0, the more likely there is to be significant drift.</p>\n</div>\n</dd>\n<dt class=\"hdlist1\">ApproxKSTest</dt>\n<dd>\n<p>The ApproxKSTest metric performs an approximate Kolmogorov-Smirnov test, ensuring that the maximum error is <code>6*epsilon</code> compared to an exact KSTest.</p>\n<div class=\"paragraph\">\n<p>ApproxKSTest is useful for detecting changes in distributions for large data sets where performing an exact KSTest might be computationally expensive.</p>\n</div>\n<div class=\"paragraph\">\n<p>This metric returns the probability that the distribution seen in the \"real world\" data is derived from the same distribution as the reference data. The closer the value is to 0, the more likely there is to be significant drift.</p>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"using-explainability_explainers\">Using explainability</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>As a data scientist, you can learn how your machine learning model makes its predictions and decisions. You can use explainers from TrustyAI to provide saliency explanations for model inferences in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about the specific explainers, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-explainers_explainers\">Supported explainers</a>.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"requesting-a-lime-explanation_explainers\">Requesting a LIME explanation</h3>\n<div class=\"paragraph _abstract\">\n<p>To understand how a model makes its predictions and decisions, you can use a <em>Local Interpretable Model-agnostic Explanations</em> (LIME) explainer. LIME explains a model&#8217;s predictions by showing how much each feature affected the outcome. For example, for a model predicting not to target a user for a marketing campaign, LIME provides a list of weights, both positive and negative, indicating how each feature influenced the model&#8217;s outcome.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-explainers_explainers\">Supported explainers</a>.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"requesting-a-lime-explanation-using-CLI_explainers\">Requesting a LIME explanation by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to request a LIME explanation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You authenticated the TrustyAI service, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a>.</p>\n</li>\n<li>\n<p>You have real-world data from the deployed models.</p>\n</li>\n<li>\n<p>You installed the OpenShift command line interface (<code>oc</code>) as described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Set an environment variable to define the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>export TRUSTY_ROUTE=$(oc get route trustyai-service -n $NAMESPACE -o jsonpath='{.spec.host}')</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set an environment variable to define the name of your model.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>export MODEL=\"model-name\"</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>GET /info/inference/ids/${MODEL}</code> to get a list of all inference IDs within your model inference data set.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -skv -H \"Authorization: Bearer ${TOKEN}\" \\\n   https://${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[\n  {\n    \"id\":\"a3d3d4a2-93f6-4a23-aedb-051416ecf84f\",\n    \"timestamp\":\"2024-06-25T09:06:28.75701201\"\n  }\n]</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set environment variables to define the two latest inference IDs (highest and lowest predictions).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>export ID_LOWEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-1].id')\n\nexport ID_HIGHEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-2].id')</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>POST /explainers/local/lime</code> to request the LIME explanation with the following syntax and payload structure:</p>\n<div class=\"paragraph\">\n<p><strong>Sytnax</strong>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n -H \"Content-Type: application/json\" \\\n -d &lt;payload&gt;</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>Payload structure</strong>:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><code>PredictionId</code></dt>\n<dd>\n<p>The inference ID.</p>\n</dd>\n<dt class=\"hdlist1\"><code>config</code></dt>\n<dd>\n<p>The configuration for the LIME explanation, including <code>model</code> and <code>explainer</code> parameters. For more information, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#ModelConfig\">Model configuration parameters</a> and <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#LimeExplainerConfig\">LIME explainer configuration parameters</a>.</p>\n</dd>\n</dl>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo \"Requesting LIME for lowest\"\ncurl -s -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n        \\\"predictionId\\\": \\\"$ID_LOWEST\\\",\n        \\\"config\\\": {\n            \\\"model\\\": { <b class=\"conum\">(1)</b>\n                \\\"target\\\": \\\"modelmesh-serving:8033\\\", <b class=\"conum\">(2)</b>\n                \\\"name\\\": \\\"${MODEL}\\\",\n                \\\"version\\\": \\\"v1\\\"\n            },\n            \\\"explainer\\\": { <b class=\"conum\">(3)</b>\n              \\\"n_samples\\\": 50,\n              \\\"normalize_weights\\\": \\\"false\\\",\n              \\\"feature_selection\\\": \\\"false\\\"\n            }\n        }\n    }\" \\\n    ${TRUSTYAI_ROUTE}/explainers/local/lime</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo \"Requesting LIME for highest\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n        \\\"predictionId\\\": \\\"$ID_HIGHEST\\\",\n        \\\"config\\\": {\n            \\\"model\\\": { <b class=\"conum\">(1)</b>\n                \\\"target\\\": \\\"modelmesh-serving:8033\\\", <b class=\"conum\">(2)</b>\n                \\\"name\\\": \\\"${MODEL}\\\",\n                \\\"version\\\": \\\"v1\\\"\n            },\n            \\\"explainer\\\": { <b class=\"conum\">(3)</b>\n              \\\"n_samples\\\": 50,\n              \\\"normalize_weights\\\": \\\"false\\\",\n              \\\"feature_selection\\\": \\\"false\\\"\n            }\n        }\n    }\" \\\n    ${TRUSTYAI_ROUTE}/explainers/local/lime</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Specifies configuration for the model. For more information about the model configuration options, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#ModelConfig\">Model configuration parameters</a>.</p>\n</li>\n<li>\n<p>Specifies the model server service URL. This field only accepts model servers in the same namespace as the TrustyAI service, with or without protocol or port number.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>http[s]://service[:port]</code></p>\n</li>\n<li>\n<p><code>service[:port]</code></p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Specifies the configuration for the explainer. For more information about the explainer configuration parameters, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#LimeExplainerConfig\">LIME explainer configuration parameters</a>.</p>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"requesting-a-shap-explanation_explainers\">Requesting a SHAP explanation</h3>\n<div class=\"paragraph _abstract\">\n<p>To understand how a model makes its predictions and decisions, you can use a <em>SHapley Additive exPlanations</em> (SHAP) explainer. SHAP explains a model&#8217;s prediction by showing a detailed breakdown of each feature&#8217;s contribution to the final outcome. For example, for a model predicting the price of a house, SHAP provides a list of how much each feature contributed (in monetary value) to the final price.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#supported-explainers_explainers\">Supported explainers</a>.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"requesting-a-shap-explanation-using-CLI_explainers\">Requesting a SHAP explanation by using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift command-line interface (CLI) to request a SHAP explanation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Your OpenShift cluster administrator added you as a user to the OpenShift Container Platform cluster and has installed the TrustyAI service for the data science project that contains the deployed models.</p>\n</li>\n<li>\n<p>You authenticated the TrustyAI service, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#authenticating-trustyai-service_monitor\">Authenticating the TrustyAI service</a>.</p>\n</li>\n<li>\n<p>You have real-world data from the deployed models.</p>\n</li>\n<li>\n<p>You installed the OpenShift command line interface (<code>oc</code>) as described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Follow these steps to log in to your OpenShift Container Platform cluster:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the OpenShift command-line interface (CLI).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Set an environment variable to define the external route for the TrustyAI service pod.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>export TRUSTY_ROUTE=$(oc get route trustyai-service -n $NAMESPACE -o jsonpath='{.spec.host}')</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set an environment variable to define the name of your model.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>export MODEL=\"model-name\"</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>GET /info/inference/ids/${MODEL}</code> to get a list of all inference IDs within your model inference data set.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -skv -H \"Authorization: Bearer ${TOKEN}\" \\\n   https://${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[\n  {\n    \"id\":\"a3d3d4a2-93f6-4a23-aedb-051416ecf84f\",\n    \"timestamp\":\"2024-06-25T09:06:28.75701201\"\n  }\n]</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Set environment variables to define the two latest inference IDs (highest and lowest predictions).</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>export ID_LOWEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-1].id')\n\nexport ID_HIGHEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-2].id')</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use <code>POST /explainers/local/shap</code> to request the SHAP explanation with the following syntax and payload structure:</p>\n<div class=\"paragraph\">\n<p><strong>Sytnax</strong>:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>curl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n -H \"Content-Type: application/json\" \\\n -d &lt;payload&gt;</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>Payload structure</strong>:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><code>PredictionId</code></dt>\n<dd>\n<p>The inference ID.</p>\n</dd>\n<dt class=\"hdlist1\"><code>config</code></dt>\n<dd>\n<p>The configuration for the SHAP explanation, including <code>model</code> and <code>explainer</code> parameters. For more information, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#ModelConfig\">Model configuration parameters</a> and <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#SHAPExplainerConfig\">SHAP explainer configuration parameters</a>.</p>\n</dd>\n</dl>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo \"Requesting SHAP for lowest\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"predictionId\\\": \\\"$ID_LOWEST\\\",\n    \\\"config\\\": {\n        \\\"model\\\": { <b class=\"conum\">(1)</b>\n            \\\"target\\\": \\\"modelmesh-serving:8033\\\", <b class=\"conum\">(2)</b>\n            \\\"name\\\": \\\"${MODEL}\\\",\n            \\\"version\\\": \\\"v1\\\"\n        },\n        \\\"explainer\\\": { <b class=\"conum\">(3)</b>\n          \\\"n_samples\\\": 75\n        }\n    }\n  }\" \\\n  ${TRUSTYAI_ROUTE}/explainers/local/shap</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo \"Requesting SHAP for highest\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n        \\\"predictionId\\\": \\\"$ID_HIGHEST\\\",\n        \\\"config\\\": {\n            \\\"model\\\": { <b class=\"conum\">(1)</b>\n                \\\"target\\\": \\\"modelmesh-serving:8033\\\", <b class=\"conum\">(2)</b>\n                \\\"name\\\": \\\"${MODEL}\\\",\n                \\\"version\\\": \\\"v1\\\"\n            },\n            \\\"explainer\\\": { <b class=\"conum\">(3)</b>\n              \\\"n_samples\\\": 75\n            }\n        }\n    }\" \\\n    ${TRUSTYAI_ROUTE}/explainers/local/shap</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Specifies configuration for the model. For more information about the model configuration options, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#ModelConfig\">Model configuration parameters</a>.</p>\n</li>\n<li>\n<p>Specifies the model server service URL. This field only accepts model servers in the same namespace as the TrustyAI service, with or without protocol or port number.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>http[s]://service[:port]</code></p>\n</li>\n<li>\n<p><code>service[:port]</code></p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Specifies the configuration for the explainer. For more information about the explainer configuration parameters, see <a href=\"https://trustyai-explainability.github.io/trustyai-site/main/trustyai-service-api-reference.html#SHAPExplainerConfig\">SHAP explainer configuration parameters</a>.</p>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"supported-explainers_explainers\">Supported explainers</h3>\n<div class=\"paragraph\">\n<p>Open Data Hub supports the following explainers:</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>LIME</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><em>Local Interpretable Model-agnostic Explanations</em> (LIME) <sup class=\"footnote\" id=\"_footnote_1\">[<a id=\"_footnoteref_1\" class=\"footnote\" href=\"#_footnotedef_1\" title=\"View footnote.\">1</a>]</sup> is a saliency explanation method. LIME aims to explain a prediction &#119901; &#61; &#40;&#119909;, &#119910;&#41; (an input-output pair) generated by a black-box model &#119891; &#58;  &#8477;<sup>&#119889;</sup> &#8594; &#8477;. The explanations come in the form of a \"saliency\" &#119908;<sub>&#119894;</sub> attached to each feature &#119909;<sub>&#119894;</sub> in the prediction &#119909;. LIME generates a local explanation &#958;&#40;&#119909;&#41; according to the following model:</p>\n</div>\n<div class=\"imageblock text-center\">\n<div class=\"content\">\n<img src=\"/static/docs/images/explainer-lime.png\" alt=\"LIME model\">\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>&#120587;<sub>&#119909;</sub> is a proximity function</p>\n</li>\n<li>\n<p>&#119866; is the family of interpretable models</p>\n</li>\n<li>\n<p>&#937;&#40;&#119892;&#41; is a measure of complexity of an explanation &#119892; &#8712; &#119866;</p>\n</li>\n<li>\n<p>&#119871;&#40;&#119891;, &#119892;, &#120587;<sub>&#119909;</sub>&#41; is a measure of how unfaithful &#119892; is in approximating &#119891; in the locality defined by &#120587;<sub>&#119909;</sub></p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>In the original paper, &#119866; is the class of linear models and &#120587;<sub>&#119909;</sub> is an exponential kernel on a distance function &#119863; (for example, cosine distance). LIME converts samples &#119909;<sub>&#119894;</sub> from the original domain into interpretable samples as binary vectors &#119909;&#8242;<sub>&#119894;</sub> &#8712; 0,1. An encoded data set &#119864; is built by taking nonzero elements of &#119909;&#8242;<sub>&#119894;</sub> , recovering the original representation &#119911; &#8712; &#8477;<sup>&#119889;</sup> and then computing &#119891;&#40;&#119911;&#41;. A weighted linear model &#119892; (with weights provided via &#120587;<sub>&#119909;</sub>) is then trained on the generated sparse data set &#119864; and the model weights &#119908; are used as feature weights for the final explanation &#958;&#40;&#119909;&#41;.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>SHAP</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><em>SHapley Additive exPlanations</em> (SHAP), <sup class=\"footnote\">[<a id=\"_footnoteref_2\" class=\"footnote\" href=\"#_footnotedef_2\" title=\"View footnote.\">2</a>]</sup> seeks to unify several common explanation methods, notably LIME <sup class=\"footnoteref\">[<a class=\"footnote\" href=\"#_footnotedef_1\" title=\"View footnote.\">1</a>]</sup> and DeepLIFT, <sup class=\"footnote\">[<a id=\"_footnoteref_3\" class=\"footnote\" href=\"#_footnotedef_3\" title=\"View footnote.\">3</a>]</sup> under a common umbrella of additive feature attributions. These methods explain how an input &#119909; &#61; &#91;&#119909;<sub>1</sub>, &#119909;<sub>2</sub>, &#8230;&#8203;, &#119909;<sub>&#119872;</sub>&#93; affects the output of some model &#119891; by transforming &#119909; &#8712; &#8477;<sup>&#119872;</sup> into simplified inputs &#119911;&#8242; &#8712; 0, 1<sup>&#119872;</sup> , such that &#119911;&#8242;<sub>&#119894;</sub> indicates the inclusion or exclusion of feature &#119894;. The simplified inputs are then passed to an explanatory model &#119892; that takes the following form:</p>\n</div>\n<div class=\"imageblock text-center\">\n<div class=\"content\">\n<img src=\"/static/docs/images/explainer-shap.png\" alt=\"SHAP explanatory model\">\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In that form, each value &#120567;<sub>&#119894;</sub> marks the contribution that feature &#119894; had on the output model (called the attribution), &#120567;<sub>0</sub> marks the null output of the model; the model output when every feature is excluded. Therefore, this presents an easily interpretable explanation of the importance of each feature and a framework to permute the various input features to establish their collection contributions.</p>\n</div>\n<div class=\"paragraph\">\n<p>The final result of the algorithm are the Shapley values of each feature, which give an itemized \"receipt\" of all the contributing factors to the decision. For example, a SHAP explanation of a loan application might be as follows:</p>\n</div>\n<table class=\"tableblock frame-all grid-all fit-content\">\n<colgroup>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Feature</th>\n<th class=\"tableblock halign-left valign-top\">Shapley Value φ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Null Output</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">50%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Income</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">+10%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"># Children</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">-15%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Age</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">+22%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Own Home?</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">-30%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Acceptance%</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">37%</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Deny</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">63%</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p>From this, the applicant can see that the biggest contributor to their denial was their home ownership status, which reduced their acceptance probability by 30 percentage points. Meanwhile, their number of children was of particular benefit, increasing their probability by 22 percentage points.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"evaluating-large-language-models_monitor\">Evaluating large language models</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>A large language model (LLM) is a type of artificial intelligence (AI) program that is designed for natural language processing tasks, such as recognizing and generating text.</p>\n</div>\n<div class=\"paragraph\">\n<p>As a data scientist, you might want to monitor your large language models against a range of metrics, in order to ensure the accuracy and quality of its output.  Features such as summarization, language toxicity, and question-answering accuracy can be assessed to inform and improve your model parameters.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub now offers Language Model Evaluation as a Service (LM-Eval-aaS), in a feature called LM-Eval. LM-Eval provides a unified framework to test generative language models on a vast range of different evaluation tasks.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following sections show you how to create an <code>LMEvalJob</code> custom resource (CR) which allows you to activate an evaluation job and generate an analysis of your model&#8217;s ability.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"setting-up-lmeval_monitor\">Setting up LM-Eval</h3>\n<div class=\"paragraph _abstract\">\n<p>LM-Eval is a service designed for evaluating large language models that has been integrated into the TrustyAI Operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>The service is built on top of two open-source projects:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>LM Evaluation Harness, developed by EleutherAI, that provides a comprehensive framework for evaluating language models</p>\n</li>\n<li>\n<p>Unitxt, a tool that enhances the evaluation process with additional functionalities</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following information explains how to create an <code>LMEvalJob</code> custom resource (CR) to initiate an evaluation job and get the results.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>LM-Eval is only available in the latest community builds. To use LM-Eval on Open Data Hub, ensure that you use ODH 2.20 or later versions and add the following <code>devFlag</code> to your <code>DataScienceCluster</code> resource:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>    trustyai:\n    devFlags:\n        manifests:\n        - contextDir: config\n            sourcePath: ''\n            uri: https://github.com/trustyai-explainability/trustyai-service-operator/tarball/main\n    managementState: Managed</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Global settings for LM-Eval</div>\n<p>Configurable global settings for LM-Eval services are stored in the TrustyAI operator global <code>ConfigMap</code>, named <code>trustyai-service-operator-config</code>. The global settings are located in the same namespace as the operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can configure the following properties for LM-Eval:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. LM-Eval properties</caption>\n<colgroup>\n<col style=\"width: 14.2857%;\">\n<col style=\"width: 14.2857%;\">\n<col style=\"width: 71.4286%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Property</th>\n<th class=\"tableblock halign-left valign-top\">Default</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-detect-device</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>true/false</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Detect if there are GPUs available and assign a value for <code>--device argument</code> for LM Evaluation Harness. If GPUs are available, the value is <code>cuda</code>. If there are no GPUs available, the value is <code>cpu</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-pod-image</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>quay.io/trustyai/ta-lmes-job:latest</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image for the LM-Eval job. The image contains the Python packages for LM Evaluation Harness and Unitxt.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-driver-image</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>quay.io/trustyai/ta-lmes-driver:latest</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image for the LM-Eval driver. For detailed information about the driver, see the <code>cmd/lmes_driver</code> directory.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-image-pull-policy</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Always</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image-pulling policy when running the evaluation job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-default-batch-size</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">8</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The default batch size when invoking the model inference API. Default batch size is only available for local models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-max-batch-size</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">24</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The maximum batch size that users can specify in an evaluation job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-pod-checking-interval</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">10s</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The interval to check the job pod for an evaluation job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-allow-online</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">true</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Whether LMEval jobs can set the online mode to <code>on</code> to access artifacts (models, datasets, tokenizers) from the internet.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-code-execution</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">true</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Determines whether LMEval jobs can set the <code>trust remote code</code> mode to <code>on</code>.</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p>After updating the settings in the <code>ConfigMap</code>, restart the operator to apply the new values.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>allowOnline</code> setting is enabled by default in Open Data Hub. Using <code>allowOnline</code> gives the job permissions to automatically download artifacts from external sources. Change this setting to <code>false</code> if you do not want LM-Eval to access external sources.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lmeval-evaluation-job_monitor\">LM-Eval evaluation job</h3>\n<div class=\"paragraph _abstract\">\n<p>LM-Eval service defines a new Custom Resource Definition (CRD) called <code>LMEvalJob</code>. An <code>LMEvalJob</code> object represents an evaluation job. <code>LMEvalJob</code> objects are monitored by the TrustyAI Kubernetes operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>To run an evaluation job, create an <code>LMEvalJob</code> object with the following information: <code>model</code>, <code>model arguments</code>, <code>task</code>, and <code>secret</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>After the <code>LMEvalJob</code> is created, the LM-Eval service runs the evaluation job.  The status and results of the <code>LMEvalJob</code> object update when the information is available.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Other TrustyAI features (such as bias and drift metrics) do not support non-tabular models (including LLMs). Deploying the <code>TrustyAIService</code> custom resource (CR) in a namespace that contains non-tabular models (such as the namespace where an evaluation job is being executed) can cause errors within the TrustyAI service.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Sample LMEvalJob object</div>\n<p>The sample <code>LMEvalJob</code> object contains the following features:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <code>google/flan-t5-base</code> model from Hugging Face.</p>\n</li>\n<li>\n<p>The dataset from the <code>wnli</code> card, a subset of the GLUE (General Language Understanding Evaluation) benchmark evaluation framework from Hugging Face. For more information about the <code>wnli</code> Unitxt card, see the Unitxt website.</p>\n</li>\n<li>\n<p>The following default parameters for the <code>multi_class.relation</code> Unitxt task: <code>f1_micro</code>, <code>f1_macro</code>, and <code>accuracy</code>. This template can be found on the Unitxt website: click <strong>Catalog</strong>, then click <strong>Tasks</strong> and select <strong>Classification</strong> from the menu.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following is an example of an <code>LMEvalJob</code> object:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: google/flan-t5-base\n  taskList:\n    taskRecipes:\n    - card:\n        name: \"cards.wnli\"\n      template: \"templates.classification.multi_class.relation.default\"\n  logSamples: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After you apply the sample <code>LMEvalJob</code>, check its state by using the following command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get lmevaljob evaljob-sample</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Output similar to the following appears:\nNAME: <code>evaljob-sample</code>\nSTATE: <code>Running</code></p>\n</div>\n<div class=\"paragraph\">\n<p>Evaluation results are available when the state of the object changes to <code>Complete</code>. Both the model and dataset in this example are small. The evaluation job should finish within 10 minutes on a CPU-only node.</p>\n</div>\n<div class=\"paragraph\">\n<p>Use the following command to get the results:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get lmevaljobs.trustyai.opendatahub.io evaljob-sample \\\n  -o template --template={{.status.results}} | jq '.results'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command returns results similar to the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>{\n  \"tr_0\": {\n    \"alias\": \"tr_0\",\n    \"f1_micro,none\": 0.5633802816901409,\n    \"f1_micro_stderr,none\": \"N/A\",\n    \"accuracy,none\": 0.5633802816901409,\n    \"accuracy_stderr,none\": \"N/A\",\n    \"f1_macro,none\": 0.36036036036036034,\n    \"f1_macro_stderr,none\": \"N/A\"\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the results</div>\n<ul>\n<li>\n<p>The <code>f1_micro</code>, <code>f1_macro</code>, and <code>accuracy</code> scores are 0.56, 0.36, and 0.56.</p>\n</li>\n<li>\n<p>The full results are stored in the <code>.status.results</code> of the <code>LMEvalJob</code> object as a JSON document.</p>\n</li>\n<li>\n<p>The command above only retrieves the results field of the JSON document.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">LMEvalJob properties</div>\n<p>The following table lists each property in the <code>LMEvalJob</code> and its usage:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. LM-Eval properties</caption>\n<colgroup>\n<col style=\"width: 28.5714%;\">\n<col style=\"width: 71.4286%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Parameter</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>model</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies which model type or provider is evaluated. This field directly maps to the <code>--model</code> argument of the <code>lm-evaluation-harness</code>. Supported model types and providers include:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>hf</code>: HuggingFace models</p>\n</li>\n<li>\n<p><code>openai-completions</code>: OpenAI Completions API models</p>\n</li>\n<li>\n<p><code>openai-chat-completions</code>: OpenAI Chat Completions API models</p>\n</li>\n<li>\n<p><code>local-completions</code> and <code>local-chat-completions</code>: OpenAI API-compatible servers</p>\n</li>\n<li>\n<p><code>textsynth</code>: TextSynth APIs</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>modelArgs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>A list of paired name and value arguments for the model type. Each model type or provider supports different arguments. You can find further details in the models section of the LM Evaluation Harness library on GitHub.</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>hf</code> (HuggingFace)</p>\n</li>\n<li>\n<p><code>local-completions</code> (An OpenAI API-compatible server)</p>\n</li>\n<li>\n<p><code>local-chat-completions</code> (An OpenAI API-compatible server)</p>\n</li>\n<li>\n<p><code>openai-completions</code> (OpenAI Completions API models)</p>\n</li>\n<li>\n<p><code>openai-chat-completions</code> (ChatCompletions API models)</p>\n</li>\n<li>\n<p><code>textsynth</code> (TextSynth APIs)</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>taskList.taskNames</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Specifies a list of tasks supported by <code>lm-evaluation-harness</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>taskList.taskRecipes</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies the task using the Unitxt recipe format:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>card</code>: Use the <code>name</code> to specify a Unitxt card or <code>custom</code> for a custom card.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: Specifies a Unitxt card from the Unitxt catalog. Use the card ID as the value. For example, the ID of the Wnli card is <code>cards.wnli</code>.</p>\n</li>\n<li>\n<p><code>custom</code>: Defines and uses a custom card. The value is a JSON object that contains the custom dataset. For more information about creating a custom card, see the Unitxt documentation on their website. If the dataset used by the custom card requires an API key from an environment variable or a persistent volume, configure the necessary resources in the <code>pod</code> field.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>template</code>: Specifies a Unitxt template from the Unitxt catalog. Use the template ID as the value.</p>\n</li>\n<li>\n<p><code>task</code> (optional): Specifies a Unitxt task from the Unitxt catalog. Use the task ID as the value. A Unitxt card has a predefined task. Only specify a value for this if you want to run a different task.</p>\n</li>\n<li>\n<p><code>metrics</code> (optional):  Specifies a Unitxt task from the Unitxt catalog. Use the metric ID as the value. A Unitxt task has a set of pre-defined metrics. Only specify a set of metrics if you need different metrics.</p>\n</li>\n<li>\n<p><code>format</code> (optional): Specifies a Unitxt format from the Unitxt catalog. Use the format ID as the value.</p>\n</li>\n<li>\n<p><code>loaderLimit</code> (optional): Specifies the maximum number of instances per stream to be returned from the loader. You can use this parameter to reduce loading time in large datasets.</p>\n</li>\n<li>\n<p><code>numDemos</code> (optional): Number of few-shot to be used.</p>\n</li>\n<li>\n<p><code>demosPoolSize</code> (optional): Size of the few-shot pool.</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>numFewShot</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Sets the number of few-shot examples to place in context. If you are using a task from Unitxt, do not use this field. Use <code>numDemos</code> under the <code>taskRecipes</code> instead.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>limit</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Set a limit to run the tasks instead of running the entire dataset. Accepts either an integer or a float between 0.0 and 1.0.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>genArgs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Maps to the <code>--gen_kwargs</code> parameter for the <code>lm-evaluation-harness</code>. For more information, see the LM Evaluation Harness documentation on GitHub.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>logSamples</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this flag is passed, then the model outputs and the text fed into the model will be saved at per-document granularity.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>batchSize</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Specifies the batch size for the evaluation in integer format. The <code>auto:N</code> batch size is not used for API models, but numeric batch sizes are used for APIs.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>pod</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies extra information for the <code>lm-eval</code> job pod:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>container</code>: Specifies additional container settings for the <code>lm-eval</code> container.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>env</code>: Specifies environment variables. This parameter uses the <code>EnvVar</code> data structure of Kubernetes.</p>\n</li>\n<li>\n<p><code>volumeMounts</code>: Mounts the volumes into the <code>lm-eval</code> container.</p>\n</li>\n<li>\n<p><code>resources</code>: Specifies the resources for the <code>lm-eval</code> container.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>volumes</code>: Specifies the volume information for the <code>lm-eval</code> and other containers. This parameter uses the <code>Volume</code> data structure of Kubernetes.</p>\n</li>\n<li>\n<p><code>sideCars</code>: A list of containers that run along with the <code>lm-eval</code> container. It uses the <code>Container</code> data structure of Kubernetes.</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">This parameter defines a custom output location to store the the evaluation results. Only Persistent Volume Claims (PVC) are supported.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs.pvcManaged</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Creates an operator-managed PVC to store the job results. The PVC is named <code>&lt;job-name&gt;-pvc</code> and is owned by the <code>LMEvalJob</code>. After the job finishes, the PVC is still be available, but it is deleted with the <code>LMEvalJob</code>. Supports the following fields:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>size</code>: The PVC size, compatible with standard PVC syntax (for example, 5Gi)</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs.pvcName</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Binds an existing PVC to a job by specifying its name. The PVC must be created separately and must already exist when creating the job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>allowOnline</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this parameter is set to <code>true</code>, the LMEval job downloads artifacts as needed (for example, models, datasets or tokenizers). If set to <code>false</code>, artifacts are not downloaded and are pulled from local storage instead. This setting is disabled by default. If you want to enable <code>allowOnline</code> mode, you can patch the TrustyAI operator <code>ConfigMap</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>allowCodeExecution</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this parameter is set to <code>true</code>, the LMEval job executes the necessary code for preparing models or datasets. If set to <code>false</code> it does not execute downloaded code.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>offline</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Mount a PVC as the local storage for models and datasets.</p></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lmeval-scenarios_monitor\">LM-Eval scenarios</h3>\n<div class=\"paragraph _abstract\">\n<p>The following procedures outline example scenarios that can be useful for an ML-Eval setup.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_configuring_the_lm_eval_environment\">Configuring the LM-Eval environment</h4>\n<div class=\"paragraph\">\n<p>If the <code>LMEvalJob</code> needs to access a model on HuggingFace with the access token, you can set up the <code>HF_TOKEN</code> as one of the environment variables for the <code>lm-eval</code> container.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Red Hat OpenShift AI.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator has installed OpenShift AI and enabled the TrustyAI service for the data science project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>To start an evaluation job for a <code>huggingface</code> model, apply the following YAML file:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: huggingfacespace/model\n  taskList:\n    taskNames:\n    - unfair_tos/\n  logSamples: true\n  pod:\n    container:\n      env:\n      - name: HF_TOKEN\n        value: \"My HuggingFace token\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) You can also create a secret to store the token, then refer the key from the <code>secretKeyRef</code> object using the following reference syntax:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>env:\n  - name: HF_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: my-secret\n        key: hf-token</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_using_a_custom_unitxt_card\">Using a custom Unitxt card</h4>\n<div class=\"paragraph\">\n<p>You can run evaluations using custom Unitxt cards. To do this, include the custom Unitxt card in JSON format within the <code>LMEvalJob</code> YAML.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Red Hat OpenShift AI.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator has installed OpenShift AI and enabled the TrustyAI service for the data science project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Pass a custom Unitxt Card in JSON format:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: google/flan-t5-base\n  taskList:\n    taskRecipes:\n    - template: \"templates.classification.multi_class.relation.default\"\n      card:\n        custom: |\n          {\n            \"__type__\": \"task_card\",\n            \"loader\": {\n              \"__type__\": \"load_hf\",\n              \"path\": \"glue\",\n              \"name\": \"wnli\"\n            },\n            \"preprocess_steps\": [\n              {\n                \"__type__\": \"split_random_mix\",\n                \"mix\": {\n                  \"train\": \"train[95%]\",\n                  \"validation\": \"train[5%]\",\n                  \"test\": \"validation\"\n                }\n              },\n              {\n                \"__type__\": \"rename\",\n                \"field\": \"sentence1\",\n                \"to_field\": \"text_a\"\n              },\n              {\n                \"__type__\": \"rename\",\n                \"field\": \"sentence2\",\n                \"to_field\": \"text_b\"\n              },\n              {\n                \"__type__\": \"map_instance_values\",\n                \"mappers\": {\n                  \"label\": {\n                    \"0\": \"entailment\",\n                    \"1\": \"not entailment\"\n                  }\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"classes\": [\n                    \"entailment\",\n                    \"not entailment\"\n                  ]\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"type_of_relation\": \"entailment\"\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"text_a_type\": \"premise\"\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"text_b_type\": \"hypothesis\"\n                }\n              }\n            ],\n            \"task\": \"tasks.classification.multi_class.relation\",\n            \"templates\": \"templates.classification.multi_class.relation.all\"\n          }\n  logSamples: true</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Inside the custom card specify the Hugging Face dataset loader:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>\"loader\": {\n              \"__type__\": \"load_hf\",\n              \"path\": \"glue\",\n              \"name\": \"wnli\"\n            },</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) You can use other Unitxt loaders (found on the Unitxt website) that contain the <code>volumes</code> and <code>volumeMounts</code> parameters to mount the dataset from persistent volumes. For example, if you use the <code>LoadCSV</code> Unitxt command, mount the files to the container and make the dataset accessible for the evaluation process.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_using_pvcs_as_storage\">Using PVCs as storage</h4>\n<div class=\"paragraph\">\n<p>To use a PVC as storage for the <code>LMEvalJob</code> results, you can use either managed PVCS or existing PVCs. Managed PVCs are managed by the TrustyAI operator. Existing PVCs are created by the end-user before the <code>LMEvalJob</code> is created.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If both managed and existing PVCs are referenced in outputs, the TrustyAI operator defaults to the managed PVC.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Red Hat OpenShift AI.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator has installed OpenShift AI and enabled the TrustyAI service for the data science project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_managed_pvcs\">Managed PVCs</h5>\n<div class=\"paragraph\">\n<p>To create a managed PVC, specify its size. The managed PVC is named <code>&lt;job-name&gt;-pvc</code> and is available after the job finishes. When the <code>LMEvalJob</code> is deleted, the managed PVC is also deleted.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>Enter the following code:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  # other fields omitted ...\n  outputs:\n    pvcManaged:\n      size: 5Gi</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the code</div>\n<ul>\n<li>\n<p><code>outputs</code> is the section for specifying custom storage locations</p>\n</li>\n<li>\n<p><code>pvcManaged</code> will create an operator-managed PVC</p>\n</li>\n<li>\n<p><code>size</code> (compatible with standard PVC syntax) is the only supported value</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_existing_pvcs\">Existing PVCs</h5>\n<div class=\"paragraph\">\n<p>To use an existing PVC, pass its name as a reference. The PVC must exist when you create the <code>LMEvalJob</code>.\nThe PVC is not managed by the TrustyAI operator, so it is available after deleting the <code>LMEvalJob</code>.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a PVC. An example is the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: \"my-pvc\"\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Reference the new PVC from the <code>LMEvalJob</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  # other fields omitted ...\n  outputs:\n    pvcName: \"my-pvc\"</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_using_an_inferenceservice\">Using an InferenceService</h4>\n<div class=\"paragraph\">\n<p>To run an evaluation job on an <code>InferenceService</code> which is already deployed and running in your namespace, define your LMEvalJob CR, then apply this CR into the same namespace as your model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Red Hat OpenShift AI.</p>\n</li>\n<li>\n<p>Your OpenShift cluster administrator has installed OpenShift AI and enabled the TrustyAI service for the data science project where the models are deployed.</p>\n</li>\n<li>\n<p>You have a namespace that contains an InferenceService with a vLLM model. This example assumes that the vLLM model is already deployed in your cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Define your <code>LMEvalJob</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>  apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob\nspec:\n  model: local-completions\n  taskList:\n    taskNames:\n      - mmlu\n  logSamples: true\n  batchSize: 1\n  modelArgs:\n    - name: model\n      value: granite\n    - name: base_url\n      value: $ROUTE_TO_MODEL/v1/completions\n    - name: num_concurrent\n      value:  \"1\"\n    - name: max_retries\n      value:  \"3\"\n    - name: tokenized_requests\n      value: \"False\"\n    - name: tokenizer\n      value: huggingfacespace/model\n env:\n   - name: OPENAI_TOKEN\n     valueFrom:\n          secretKeyRef:\n            name: &lt;secret-name&gt;\n            key: token</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Apply this CR into the same namespace as your model.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>A pod spins up in your model namespace called <code>evaljob</code>. In the pod terminal, you can see the output via <code>tail -f output/stderr.log</code>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the code</div>\n<ul>\n<li>\n<p><code>base_url</code> should be set to the route/service URL of your model. Make sure to include the <code>/v1/completions</code> endpoint in the URL.</p>\n</li>\n<li>\n<p><code>env.valueFrom.secretKeyRef.name</code> should point to a secret that contains a token that can authenticate to your model. <code>secretRef.name</code> should be the secret&#8217;s name in the namespace, while <code>secretRef.key</code> should point at the token&#8217;s key within the secret.</p>\n</li>\n<li>\n<p><code>secretKeyRef.name</code> can equal the output of:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get secrets -o custom-columns=SECRET:.metadata.name --no-headers | grep user-one-token</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p><code>secretKeyRef.key</code> is set to <code>token</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"configuring-the-guardrails-orchestrator-service_monitor\">Configuring the Guardrails Orchestrator service</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>The TrustyAI Guardrails Orchestrator service is a tool to invoke detections on text generation inputs and outputs, as well as standalone detections.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is underpinned by the open-source project <a href=\"https://github.com/foundation-model-stack/fms-guardrails-orchestrator\">FMS-Guardrails Orchestrator</a> from IBM. You can deploy the Guardrails Orchestrator service through a Custom Resource Definition (CRD) that is managed by the TrustyAI Operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following sections describe how to do the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Set up the Guardrails Orchestrator service</p>\n</li>\n<li>\n<p>Create a custom resource (CR)</p>\n</li>\n<li>\n<p>Deploy a Guardrails Orchestrator instance</p>\n</li>\n<li>\n<p>Monitor user-inputs to your LLM using this service</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deploying-the-guardrails-orchestrator-service_monitor\">Deploying the Guardrails Orchestrator service</h3>\n<div class=\"paragraph _abstract\">\n<p>You can deploy a Guardrails Orchestrator instance in your namespace to monitor elements, such as user inputs to your Large Language Model (LLM).</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift Container Platform command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with how to create a <code>configMap</code> for monitoring a user-defined workflow. You perform similar steps in this procedure. See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html-single/nodes/index#nodes-pods-configmap-overview_configmaps\">Understanding config maps</a>.</p>\n</li>\n<li>\n<p>You have KServe set to <code>RawDeployment</code>. See <a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models#deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode_serving-large-models\" target=\"_blank\" rel=\"noopener\">Deploying models on single-node OpenShift using KServe Raw Deployment mode</a>.</p>\n</li>\n<li>\n<p>You have the TrustyAI component in your Open Data Hub <code>DataScienceCluster</code> set to <code>Managed</code>.</p>\n</li>\n<li>\n<p>You have an LLM for chat generation deployed in your namespace.</p>\n</li>\n<li>\n<p>You have an LLM for text classification deployed in your namespace.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file to specify the <code>chat_generation</code> and <code>detectors</code> services. For example, create a file named <code>orchestrator_cm.yaml</code> with the following content:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example <code>orchestrator_cm.yaml</code></div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: fms-orchestr8-config-nlp\ndata:\n  config.yaml: |\n    chat_generation: <b class=\"conum\">(1)</b>\n      service:\n        hostname: &lt;CHAT_GENERATION_HOSTNAME&gt;\n        port: 8080\n    detectors:       <b class=\"conum\">(2)</b>\n      &lt;DETECTOR_NAME&gt;:\n        type: text_contents\n        service:\n          hostname: &lt;DETECTOR_HOSTNAME&gt;\n          port: 8000\n        chunker_id: whole_doc_chunker\n        default_threshold: 0.5\n---\n&lt;1&gt; A service for chat generation referring to a deployed LLM in your namespace where you are adding guardrails.\n&lt;2&gt; A list of services responsible for running detection of a certain class of content on text spans. Each of these services refer to a deployed LLM for text classification in your namespace.</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>orchestrator_cm.yaml</code> config map:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n$ oc apply -f orchestrator_cm.yaml -n &lt;TEST_NAMESPACE&gt;\n---</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Specify the previously created <code>ConfigMap</code> object created in the <code>GuardrailsOrchestrator</code> custom resource (CR). For example, create a file named <code>orchestrator_cr.yaml</code> with the following content:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example <code>orchestrator_cr.yaml</code> CR</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">---\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: GuardrailsOrchestrator\nmetadata:\n  name: gorch-sample\nspec:\n  orchestratorConfig: \"fms-orchestr8-config-nlp\"\n  replicas: 1\n---</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the orchestrator CR, which creates a service account, deployment, service, and route object in your namespace:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\noc apply -f orchestrator_cr.yaml -n &lt;TEST_NAMESPACE&gt;\n---</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>Confirm that the orchestrator and LLM pods are running:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n$ oc get pods -n &lt;TEST_NAMESPACE&gt;\n---</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example response</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\nNAME                                       READY   STATUS    RESTARTS   AGE\ngorch-test-55bf5f84d9-dd4vm                3/3     Running   0          3h53m\nibm-container-deployment-bd4d9d898-52r5j   1/1     Running   0          3h53m\nibm-hap-predictor-5d54c877d5-rbdms         1/1     Running   0          3h53m\nllm-container-deployment-bd4d9d898-52r5j   1/1     Running   0          3h53m\nllm-predictor-5d54c877d5-rbdms             1/1     Running   0          57m\n---</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Query the <code>/health</code> endpoint of the orchestrator route to check the current status of the detector and generator services. If a <code>200 OK</code> response is returned, the services are functioning normally:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n$ GORCH_ROUTE_HEALTH=$(oc get routes gorch-test-health -o jsonpath='{.spec.host}')\n---</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n$ curl -v https://$GORCH_ROUTE_HEALTH/health\n---</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example response</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n*   Trying ::1:8034...\n* connect to ::1 port 8034 failed: Connection refused\n*   Trying 127.0.0.1:8034...\n* Connected to localhost (127.0.0.1) port 8034 (#0)\n&gt; GET /health HTTP/1.1\n&gt; Host: localhost:8034\n&gt; User-Agent: curl/7.76.1\n&gt; Accept: */*\n&gt;\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; content-type: application/json\n&lt; content-length: 36\n&lt; date: Fri, 31 Jan 2025 14:04:25 GMT\n&lt;\n* Connection #0 to host localhost left intact\n{\"fms-guardrails-orchestr8\":\"0.1.0\"}\n---</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"guardrails-orchestrator-parameters_monitor\">Guardrails Orchestrator parameters</h3>\n<div class=\"paragraph _abstract\">\n<p>A <code>GuardrailsOrchestrator</code> object represents an orchestration service that invokes detectors on text generation input and output and standalone detections.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can modify the following parameters for the <code>GuardrailsOrchestrator</code> object you created previously:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 66.6667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Parameter</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>replicas</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>The number of orchestrator pods to create.</p>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>orchestratorConfig</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>The name of the <code>ConfigMap</code> object that contains generator, detector, and chunker arguments.</p>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>otelExporter **(optional)**</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>A list of paired name and value arguments for configuring OpenTelemetry traces or metrics, or both:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>protocol</code> - Sets the protocol for all the OpenTelemetry protocol (OTLP) endpoints. Valid values are <code>grpc</code> or <code>http</code></p>\n</li>\n<li>\n<p><code>tracesProtocol</code> - Sets the protocol for traces. Acceptable values are <code>grpc</code> or <code>http</code></p>\n</li>\n<li>\n<p><code>metricsProtocol</code> - Sets the protocol for metrics. Acceptable values are <code>grpc</code> or <code>http</code></p>\n</li>\n<li>\n<p><code>otlpEndpoint</code> - Sets the OTLP endpoint. Default values are <code>gRPC localhost:4317</code> and <code>HTTP localhost:4318</code></p>\n</li>\n<li>\n<p><code>metricsEndpoint</code> - Sets the OTLP endpoint for metrics</p>\n</li>\n<li>\n<p><code>tracesEndpoint</code> -  Sets the OTLP endpoint for traces</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-opentelemetry-exporter_monitor\">Configuring the OpenTelemetry exporter</h3>\n<div class=\"paragraph _abstract\">\n<p>Enable traces and metrics that are provided for the observability of the <code>GuardrailsOrchestrator</code> service with the OpenTelemetry exporter.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the Open Data Hub distributed tracing platform from the OperatorHub and created a Jaeger instance using the default settings.</p>\n</li>\n<li>\n<p>You have installed the Red&#160;Hat build of OpenTelemetry from the OperatorHub and created an OpenTelemetry instance.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Define a <code>GuardrailsOrchestrator</code> custom resource object to specify the <code>otelExporter</code> configurations in a YAML file named <code>orchestrator_otel_cr.yaml</code>:</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Example of a <code>orchestrator_otel_cr.yaml</code> object that has OpenTelemetry configured:</div>\n<p>+</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">---\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: GuardrailsOrchestrator\nmetadata:\n  name: gorch-test\nspec:\n  orchestratorConfig: \"fms-orchestr8-config-nlp\"    <b class=\"conum\">(1)</b>\n  vllmGatewayConfig: \"fms-orchestr8-config-gateway\" <b class=\"conum\">(1)</b>\n  replicas: 1\n  otelExporter:\n    protocol: \"http\"\n    otlpEndpoint: \"localhost:4318\"\n    otlpExport: \"metrics\"\n---\n&lt;1&gt; These speficications are the same as Step 7 from \"Configuring the regex detector and vLLM gateway\". This example CR adds `otelExporter` configurations.</code></pre>\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Deploy the orchestrator custom resource.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">---\n$ oc apply -f orchestrator_otel_cr.yaml\n---</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"bias-monitoring-tutorial_bias-tutorial\">Bias monitoring tutorial - Gender bias example</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Step-by-step guidance for using TrustyAI in Open Data Hub to monitor machine learning models for bias.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-introduction_bias-tutorial\">Introduction</h3>\n<div class=\"paragraph\">\n<p>Ensuring that your machine learning models are fair and unbiased is essential for building trust with your users. Although you can assess fairness during model training, it is only in deployment that your models use real-world data. Even if your models are unbiased on training data, they can exhibit serious biases in real-world scenarios. Therefore, it is crucial to monitor your models for fairness during their real-world deployment.</p>\n</div>\n<div class=\"paragraph\">\n<p>In this tutorial, you learn how to monitor models for bias. You will use two example models to complete the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Deploy the models by using multi-model serving.</p>\n</li>\n<li>\n<p>Send training data to the models.</p>\n</li>\n<li>\n<p>Examine the metadata for the models.</p>\n</li>\n<li>\n<p>Check model fairness.</p>\n</li>\n<li>\n<p>Schedule and check fairness and identity metric requests.</p>\n</li>\n<li>\n<p>Simulate real-world data.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_about_the_example_models\">About the example models</h4>\n<div class=\"paragraph\">\n<p>For this tutorial, your role is a DevOps engineer for a credit lending company. The company&#8217;s data scientists have created two candidate neural network models to predict whether a borrower will default on a loan. Both models make predictions based on the following information from the borrower&#8217;s application:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Number of Children</p>\n</li>\n<li>\n<p>Total Income</p>\n</li>\n<li>\n<p>Number of Total Family Members</p>\n</li>\n<li>\n<p>Is Male-Identifying?</p>\n</li>\n<li>\n<p>Owns Car?</p>\n</li>\n<li>\n<p>Owns Realty?</p>\n</li>\n<li>\n<p>Is Partnered?</p>\n</li>\n<li>\n<p>Is Employed?</p>\n</li>\n<li>\n<p>Lives with Parents?</p>\n</li>\n<li>\n<p>Age (in days)</p>\n</li>\n<li>\n<p>Length of Employment (in days)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>As the DevOps engineer, your task is to verify that the models are not biased against the <code>Is Male-Identifying?</code> gender field. To complete this task, you can monitor the models by using the Statistical Parity Difference (SPD) metric, which reports whether there is a difference between how often male-identifying and non-male-identifying applicants are given favorable predictions (that is, they are predicted to pay off their loans). An ideal SPD metric is 0, meaning both groups are equally likely to receive a positive outcome. An SPD between -0.1 and 0.1 also indicates fairness, as it reflects only a +/-10% variation between the groups.</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-setting-up-your-environment_bias-tutorial\">Setting up your environment</h3>\n<div class=\"paragraph\">\n<p>To set up your environment for this tutorial, complete the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Download tutorial files from the <a href=\"https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main\" target=\"_blank\" rel=\"noopener\">trustyai-explainability</a> repository.</p>\n</li>\n<li>\n<p>Log in to the OpenShift cluster from the command line.</p>\n</li>\n<li>\n<p>Configure monitoring for the model serving platform.</p>\n</li>\n<li>\n<p>Enable the TrustyAI component in the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>Set up a project.</p>\n</li>\n<li>\n<p>Authenticate the TrustyAI service.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>The Open Data Hub Operator is installed on your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_downloading_the_tutorial_files\">Downloading the tutorial files</h4>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Go to <a href=\"https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main\" class=\"bare\">https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main</a>.</p>\n</li>\n<li>\n<p>Click the <strong>Code</strong> button and then click <strong>Download ZIP</strong> to download the repository.</p>\n</li>\n<li>\n<p>Extract the downloaded repository files.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_logging_in_to_the_openshift_cluster_from_the_command_line\">Logging in to the OpenShift cluster from the command line</h4>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Obtain the command for logging in to the OpenShift cluster from the command line:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the upper-right corner of the OpenShift Container Platform web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>Log in with your credentials and then click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command, which has the following syntax:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In a terminal window, paste and run the login command.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_configuring_monitoring_for_the_model_serving_platform\">Configuring monitoring for the model serving platform</h4>\n<div class=\"paragraph\">\n<p>To enable monitoring on user-defined projects, you must configure monitoring for the model serving platform.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 1-Installation/resources/enable_uwm.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>To configure monitoring to store metric data for 15 days, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 1-Installation/resources/uwm_configmap.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-monitoring-for-the-multi-model-serving-platform_monitor\">Configuring monitoring for the multi-model serving platform</a>.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-trustyai-component_bias-tutorial\">Enabling the TrustyAI component</h4>\n<div class=\"paragraph _abstract\">\n<p>To allow your data scientists to use model monitoring with TrustyAI, you must enable the TrustyAI component in Open Data Hub.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have access to the data science cluster.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>In the <code>spec:components</code> section, set the <code>managementState</code> field for the <code>trustyai</code> component to <code>Managed</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre> trustyai:\n    managementState: Managed</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the <strong>trustyai-service-operator</strong> pod:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, from the <strong>Project</strong> list, select <strong>opendatahub</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>trustyai-service-operator-contoller-manager</strong> deployment.\nCheck the status:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click the deployment name to open the deployment details page.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.</p>\n</li>\n<li>\n<p>View the pod status.</p>\n<div class=\"paragraph\">\n<p>When the status of the <strong>trustyai-service-operator-controller-manager-<em>&lt;pod-id&gt;</em></strong> pod is <strong>Running</strong>, the pod is ready to use.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_setting_up_a_project\">Setting up a project</h4>\n<div class=\"paragraph\">\n<p>For this tutorial, you must create a project named <code>model-namespace</code>.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>To create a new project named <code>model-namespace</code>, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc new-project model-namespace</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Prepare the <code>model-namespace</code> project for multi-model serving:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc label namespace model-namespace \"modelmesh-enabled=true\" --overwrite=true</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_authenticating_the_trustyai_service\">Authenticating the TrustyAI service</h4>\n<div class=\"paragraph\">\n<p>TrustyAI endpoints are authenticated with a Bearer token. To obtain this token and set a variable (TOKEN) to use later, run the following command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>export TOKEN=$(oc whoami -t)</code></pre>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-deploying-models_bias-tutorial\">Deploying models</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Procedure</div>\n<p>To deploy the models for this tutorial, run the following commands from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>).</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Navigate to the <code>model-namespace</code> project you created:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc project model-namespace</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the model&#8217;s storage container:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 2-BiasMonitoring/resources/model_storage_container.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the OVMS 1.x serving runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 2-BiasMonitoring/resources/ovms-1.x.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the first model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 2-BiasMonitoring/resources/model_alpha.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the second model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc apply -f 2-BiasMonitoring/resources/model_beta.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Workloads</strong> → <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>Confirm that there are four pods:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>minio</code></p>\n</li>\n<li>\n<p><code>modelmesh-serving-ovms-1.x-xxxxxxxxxx-xxxxx</code></p>\n</li>\n<li>\n<p><code>modelmesh-serving-ovms-1.x-xxxxxxxxxx-xxxxx</code></p>\n</li>\n<li>\n<p><code>trustyai-service-xxxxxxxxxx-xxxxx</code></p>\n<div class=\"paragraph\">\n<p>When the TrustyAI service has registered the deployed models, the  <code>modelmesh-serving-ovms-1.x-xxxxx</code> pods are redeployed.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To verify that TrustyAI has registered the models:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select one of the <code>modelmesh-serving-ovms-1.x-xxxxx</code> pods.</p>\n</li>\n<li>\n<p>Click the <strong>Environment</strong> tab and confirm that the <code>MM_PAYLOAD_PROCESSORS</code> field is set.</p>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/model_environment.png\" alt=\"model environment\">\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-sending-training-data-to-the-models_bias-tutorial\">Sending training data to the models</h3>\n<div class=\"paragraph\">\n<p>Pass the training data through the models.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>In a terminal window, run the following command from the directory that contains the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>for batch in 0 250 500 750 1000 1250 1500 1750 2000 2250; do\n  2-BiasMonitoring/scripts/send_data_batch\n2-BiasMonitoring/data/training/$batch.json\ndone</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This process can take several minutes.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>View the script verification messages that indicate whether TrustyAI is receiving the data.</p>\n</li>\n<li>\n<p>Verify that the process is running by viewing the cluster metrics:</p>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Expression</strong> field, enter <code>trustyai_model_observations_total</code> and click <strong>Run Queries</strong>.</p>\n</li>\n<li>\n<p>Confirm that both models are listed with around 2250 inferences each, which indicates that TrustyAI has cataloged enough inputs and outputs to begin analysis.</p>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/observed_inferences.png\" alt=\"observed inferences\">\n</div>\n</div>\n</li>\n<li>\n<p>Optional: You can select a time range and refresh interval:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>From the <strong>Time range</strong> list, select 5 minutes.</p>\n</li>\n<li>\n<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Verify that TrustyAI can access the models by examining the model metadata:</p>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Find the route to the TrustyAI service:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}}); echo $TRUSTY_ROUTE</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Query the <code>/info</code> endpoint:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -H \"Authorization: Bearer ${TOKEN}\" $TRUSTY_ROUTE/info | jq</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>A JSON file is generated with the following information for each model:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The names, data types, and positions of fields in the input and output.</p>\n</li>\n<li>\n<p>The observed values that these fields take.</p>\n</li>\n<li>\n<p>The total number of input-output pairs observed.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>For an example output file, see the <code>odh-trustyai-demos-main/2-BiasMonitoring/scripts/info_response.json</code> file in your downloaded tutorial files.</p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-labeling-data-fields_bias-tutorial\">Labeling data fields</h3>\n<div class=\"paragraph\">\n<p>You can apply name mappings to your inputs and outputs for more meaningful field names by sending a POST request to the <code>/info/names</code> endpoint.</p>\n</div>\n<div class=\"paragraph\">\n<p>For this tutorial, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>2-BiasMonitoring/scripts/apply_name_mapping.sh</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For general steps, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#labeling-data-fields_monitor\">Labeling data fields</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>To understand the payload structure, see the <code>odh-trustyai-demos-main/2-BiasMonitoring/scripts/apply_name_mapping.sh</code> file in your downloaded tutorial files.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-checking-model-fairness_bias-tutorial\">Checking model fairness</h3>\n<div class=\"paragraph\">\n<p>Compute the model&#8217;s cumulative fairness up to this point.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>In a terminal window, run the following script from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>) to check the <code>/metrics/group/fairness/spd</code> endpoint:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo -e \"=== MODEL ALPHA ===\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/ \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"demo-loan-nn-onnx-alpha\\\",\n                 \\\"protectedAttribute\\\": \\\"Is Male-Identifying?\\\",\n                 \\\"privilegedAttribute\\\": 1.0,\n                 \\\"unprivilegedAttribute\\\": 0.0,\n                 \\\"outcomeName\\\": \\\"Will Default?\\\",\n                 \\\"favorableOutcome\\\": 0,\n                 \\\"batchSize\\\": 5000\n             }\" | jq\necho -e \"\\n\\n=== MODEL BETA ===\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"demo-loan-nn-onnx-beta\\\",\n                 \\\"protectedAttribute\\\": \\\"Is Male-Identifying?\\\",\n                 \\\"privilegedAttribute\\\": 1.0,\n                 \\\"unprivilegedAttribute\\\": 0.0,\n                 \\\"outcomeName\\\": \\\"Will Default?\\\",\n                 \\\"favorableOutcome\\\": 0,\n                 \\\"batchSize\\\": 5000\n             }\" | jq\necho</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The payload structure is as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>modelId</code>: The name of the model to query.</p>\n</li>\n<li>\n<p><code>protectedAttribute</code>: The name of the feature that distinguishes the groups that you are checking for fairness over.</p>\n</li>\n<li>\n<p><code>privilegedAttribute</code>: The value of the <code>protectedAttribute</code> that describes the suspected favored (positively biased) class.</p>\n</li>\n<li>\n<p><code>unprivilegedAttribute</code>: The value of the <code>protectedAttribute</code> that describes the suspected unfavored (negatively biased) class.</p>\n</li>\n<li>\n<p><code>outcomeName</code>: The name of the output that provides the output you are examining for fairness.</p>\n</li>\n<li>\n<p><code>favorableOutcome</code>: The value of the <code>outcomeName</code> output that describes the favorable model prediction.</p>\n</li>\n<li>\n<p><code>batchSize</code>: The number of previous inferences to include in the calculation.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Confirm that you see outputs similar to the following examples:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Model Alpha</dt>\n</dl>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>=== MODEL ALPHA ===\n{\n  \"timestamp\": \"2024-07-25T16:26:50.412+00:00\",\n  \"type\": \"metric\",\n  \"value\": 0.003056835834369387,\n  \"namedValues\": null,\n  \"specificDefinition\": \"The SPD of 0.003057 indicates that the likelihood of Group:Is Male-Identifying?=[1.0] receiving Outcome:Will Default?=[0] was 0.305684 percentage points higher than that of Group:Is Male-Identifying?=[0.0].\",\n  \"name\": \"SPD\",\n  \"id\": \"542bd51e-dd2f-40f6-947f-c1c22bd71765\",\n  \"thresholds\": {\n    \"lowerBound\": -0.1,\n    \"upperBound\": 0.1,\n    \"outsideBounds\": false\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Model Beta</dt>\n</dl>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>=== MODEL BETA ===\n{\n  \"timestamp\": \"2024-07-25T16:26:50.648+00:00\",\n  \"type\": \"metric\",\n  \"value\": 0.029078518433627354,\n  \"namedValues\": null,\n  \"specificDefinition\": \"The SPD of 0.029079 indicates that the likelihood of Group:Is Male-Identifying?=[1.0] receiving Outcome:Will Default?=[0] was 2.907852 percentage points higher than that of Group:Is Male-Identifying?=[0.0].\",\n  \"name\": \"SPD\",\n  \"id\": \"df292f06-9255-4158-8b02-4813a8777b7b\",\n  \"thresholds\": {\n    \"lowerBound\": -0.1,\n    \"upperBound\": 0.1,\n    \"outsideBounds\": false\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>specificDefinition</code> field is important in understanding the real-world interpretation of these metric values; you can see that both model Alpha and Beta are fair over the <code>Is Male-Identifying</code> field, with the two groups' rates of positive outcomes only differing by -0.3% for model Alpha and 2.8% for model Beta.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-scheduling-a-fairness-metric-request_bias-tutorial\">Scheduling a fairness metric request</h3>\n<div class=\"paragraph\">\n<p>After you confirm that the models are fair over the training data, you want to ensure that they remain fair over real-world inference data. To monitor their fairness, you can schedule a metric request to compute at recurring intervals throughout deployment by passing the same payloads to the <code>/metrics/group/fairness/spd/request</code> endpoint.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>In a terminal window, run the following script from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>echo -e \"\\n\\n=== MODEL ALPHA ===\\n\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/request \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"demo-loan-nn-onnx-alpha\\\",\n                 \\\"protectedAttribute\\\": \\\"Is Male-Identifying?\\\",\n                 \\\"privilegedAttribute\\\": 1.0,\n                 \\\"unprivilegedAttribute\\\": 0.0,\n                 \\\"outcomeName\\\": \\\"Will Default?\\\",\n                 \\\"favorableOutcome\\\": 0,\n                 \\\"batchSize\\\": 5000\n             }\"\necho -e \"\\n\\n=== MODEL BETA ===\\n\"\ncurl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/request \\\n     --header 'Content-Type: application/json' \\\n     --data \"{\n                 \\\"modelId\\\": \\\"demo-loan-nn-onnx-beta\\\",\n                 \\\"protectedAttribute\\\": \\\"Is Male-Identifying?\\\",\n                 \\\"privilegedAttribute\\\": 1.0,\n                 \\\"unprivilegedAttribute\\\": 0.0,\n                 \\\"outcomeName\\\": \\\"Will Default?\\\",\n                 \\\"favorableOutcome\\\": 0,\n                 \\\"batchSize\\\": 5000\n             }\"\necho</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>These commands return the IDs of the created requests. Later, you can use these IDs to delete the scheduled requests.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Expression</strong> field, enter <code>trustyai_spd</code> and click <strong>Run Queries</strong>.</p>\n</li>\n<li>\n<p>Optional: After running a query, you can select a time range and refresh interval:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>From the <strong>Time range</strong> list, select 5 minutes.</p>\n</li>\n<li>\n<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/initial_spd.png\" alt=\"initial spd\">\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-scheduling-an-identity-metric-request_bias-tutorial\">Scheduling an identity metric request</h3>\n<div class=\"paragraph\">\n<p>You can monitor the average values of various data fields over time to see the average ratio of loan-payback to loan-default predictions and the average ratio of male-identifying to non-male-identifying applicants. To monitor the average values, you create an identity metric request by sending a POST request to the <code>/metrics/identity/request</code> endpoint.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>In a terminal window, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>for model in \"demo-loan-nn-onnx-alpha\" \"demo-loan-nn-onnx-beta\"; do\n  for field in \"Is Male-Identifying?\" \"Will Default?\"; do\n      curl -sk -H \"Authorization: Bearer ${TOKEN}\" -X POST --location $TRUSTY_ROUTE/metrics/identity/request \\\n       --header 'Content-Type: application/json' \\\n       --data \"{\n                 \\\"columnName\\\": \\\"$field\\\",\n                 \\\"batchSize\\\": 250,\n                 \\\"modelId\\\": \\\"$model\\\"\n               }\"\n\techo -e\n  done\ndone</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The payload structure is as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>columnName</code>: The name of the field to compute the averaging over.</p>\n</li>\n<li>\n<p><code>batchSize</code>: The number of previous inferences to include in the average-value calculation.</p>\n</li>\n<li>\n<p><code>modelId</code>: The name of the model to query.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Expression</strong> field, enter <code>trustyai_identity</code> and click <strong>Run Queries</strong>.</p>\n</li>\n<li>\n<p>Optional: After running a query, you can select a time range and refresh interval:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>From the <strong>Time range</strong> list, select 5 minutes.</p>\n</li>\n<li>\n<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/initial_identities.png\" alt=\"initial identities\">\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-simulating-real-world-data_bias-tutorial\">Simulating real world data</h3>\n<div class=\"paragraph\">\n<p>Now that you have scheduled your fairness and identify metric requests, you can simulate sending some \"real world\" data through your models to see if they remain fair.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>In a terminal window, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>for batch in \"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\"; do\n  ./2-BiasMonitoring/scripts/send_data_batch 2-BiasMonitoring/data/batch_$batch.json\n  sleep 5\ndone</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong> and watch the SPD and identity metric request values change.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"t-bias-reviewing-the-results_bias-tutorial\">Reviewing the results</h3>\n<div class=\"sect3\">\n<h4 id=\"_are_the_models_biased\">Are the models biased?</h4>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/final_spd.png\" alt=\"final spd\">\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The two models have drastically different fairness levels when applied to the simulated real-world data. Model Alpha (blue) stayed within the \"acceptably fair\" range between -0.1 and 0.1, ending around 0.09. However, Model Beta (yellow) plummeted out of the fair range, ending at -0.274. This indicates that non-male-identifying applicants were 27% less likely to receive a favorable outcome from Model Beta compared to male-identifying applicants.</p>\n</div>\n<div class=\"paragraph\">\n<p>To explore this further, you can analyze your identity metrics, starting by looking at the inbound ratio of male-identifying to non-male-identifying applicants:</p>\n</div>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/final_male_ident.png\" alt=\"final male ident\">\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In the training data, the ratio between male and non-male was around 0.8, but in the real-world data, it dropped to 0, meaning all applicants were non-male. This is a strong indicator that the training data did not match the real-world data, which is likely to indicate poor or biased model performance.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_how_does_the_production_data_compare_to_the_training_data\">How does the production data compare to the training data?</h4>\n<div class=\"imageblock\">\n<div class=\"content\">\n<img src=\"/static/docs/images/final_default.png\" alt=\"final default\">\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Even though Model Alpha (green) was only exposed to non-male applicants, it still provided varying outcomes to the various applicants, predicting \"will-default\" in about 25% of cases. In contrast, Model Beta (purple) predicted \"will-default\" 100% of the time, meaning it predicted that every non-male applicant would default on their loan. This suggests that Model Beta is performing poorly on the real-world data or has encoded a systematic bias from its training, leading to the assumption that all non-male applicants will default.</p>\n</div>\n<div class=\"paragraph\">\n<p>These examples highlight the critical importance of monitoring bias in production. Models that are equally fair during training can perform very differently when applied to real-world data, with hidden biases emerging only in actual use. By using TrustyAI to detect these biases early, you can safeguard against the potential harm caused by biased models in production.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footnotes\">\n<hr>\n<div class=\"footnote\" id=\"_footnotedef_1\">\n<a href=\"#_footnoteref_1\">1</a>. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier.\" <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data mining</em>, 2016. Pages 1135-1144.\n</div>\n<div class=\"footnote\" id=\"_footnotedef_2\">\n<a href=\"#_footnoteref_2\">2</a>. Scott Lundberg, Su-In Lee. \"A Unified Approach to Interpreting Model Predictions.\" <em>Advances in Neural Information Processing Systems</em>, 2017.\n</div>\n<div class=\"footnote\" id=\"_footnotedef_3\">\n<a href=\"#_footnoteref_3\">3</a>. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje. \"Learning Important Features Through Propagating Activation Differences.\" <em>CoRR abs/1704.02685</em>, 2017.\n</div>\n</div>","id":"33975cad-ec87-5816-9a94-4096edcbe87f","document":{"title":"Monitoring data science models"}},"markdownRemark":null},"pageContext":{"id":"33975cad-ec87-5816-9a94-4096edcbe87f"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}