:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]


[id="running-kfto-based-distributed-training-workloads_{context}"]
= Running Training Operator-based distributed training workloads

[role='_abstract']
To reduce the time needed to train a Large Language Model (LLM), you can run the training job in parallel.
In {productname-long}, the Kubeflow Training Operator and Kubeflow Training Operator Python Software Development Kit (Training Operator SDK) simplify the job configuration.

You can use the Training Operator and the Training Operator SDK to configure a training job in a variety of ways.
For example, you can use multiple nodes and multiple GPUs per node, fine-tune a model, or configure a training job to use Remote Direct Memory Access (RDMA).

include::./using-the-kubeflow-training-operator-to-run-distributed-training-workloads.adoc[leveloffset=+1]
include::./using-the-kfto-sdk-to-run-distributed-training-workloads.adoc[leveloffset=+1]
include::./fine-tuning-a-model-by-using-kubeflow-training.adoc[leveloffset=+1]
include::modules/creating-a-multi-node-pytorch-training-job-with-rdma.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
