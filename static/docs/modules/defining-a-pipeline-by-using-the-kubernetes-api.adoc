:_module-type: PROCEDURE

[id="defining-a-pipeline-by-using-the-kubernetes-api_{context}"]
= Defining a pipeline by using the Kubernetes API

[role='_abstract']

You can define data science pipelines and pipeline versions by using the Kubernetes API, which stores them as custom resources in the cluster instead of the internal database. This approach makes it easier to use OpenShift GitOps (Argo CD) or similar tools to manage pipelines and pipeline versions, while still allowing you to manage them through the {productname-short} user interface (UI), API, and the Kubeflow Pipelines (`kfp`) Software Development Kit (SDK).

ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
Defining pipeline versions by using the Kubernetes API is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
Defining pipeline versions by using the Kubernetes API is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]

.Prerequisites
* You have previously created a data science project that is available and has a pipeline server (a `DataSciencePipelinesApplication` custom resource).
* You have installed the OpenShift command-line interface (CLI).

.Procedure

. In a terminal window, run the following command to log in to the OpenShift CLI:
+
[source,subs="+quotes"]
----
$ oc login -u __<user_name>__
----
+
When prompted, enter the {openshift-platform} server URL, connection type, and your password.


. To configure the pipeline server to use Kubernetes API storage instead of the default `database` option, set the `spec.apiServer.pipelineStore` field to `kubernetes` in your project's `DataSciencePipelinesApplication` (DSPA) custom resource.
+
In the following command, replace __<dspa_name>__ with the name of your DSPA custom resource, and replace __<namespace>__ with the name of your project:
+
[source,subs="+quotes"]
----
$ oc patch dspa __<dspa_name>__ -n __<namespace>__ \
  --type=merge \
  -p '{"spec": {"apiServer": {"pipelineStore": "kubernetes"}}}'
----
+
[WARNING]
====
After changing to Kubernetes API storage, existing pipelines stored in the internal database are no longer accessible through the {productname-short} UI or REST API. To restore access to those pipelines, change the `spec.apiServer.pipelineStore` field back to `database`.
====

. Define a `Pipeline` custom resource in a YAML file with the following contents:
+
.Example pipeline definition
[source,yaml]
----
apiVersion: pipelines.kubeflow.org/v2beta1
kind: Pipeline
metadata:
  name: <pipeline_name> <1>
  namespace: <namespace> <2>
spec:
  displayName: <pipeline_display_name> <3>
----
<1> Replace this value with the name of your pipeline.
<2> Replace this value with name of your project.
<3> Replace this value with the display name of your pipeline.

. Apply the pipeline definition to create the `Pipeline` custom resource in your cluster.
+
In the following command, replace __<pipeline_yaml_file>__ with the name of your YAML file:
+
.Example command
[source,subs="+quotes"]
----
$ oc apply -f __<pipeline_yaml_file>__.yaml
----

. Define a `PipelineVersion` custom resource in a YAML file with the following contents:
+
.Example pipeline version definition
[source,yaml]
----
apiVersion: pipelines.kubeflow.org/v2beta1
kind: PipelineVersion
metadata:
  name: <pipeline_version_name> <1>
  namespace: <namespace> <2>
spec:
  pipelineName: <pipeline_name> <3>
  displayName: <pipeline_version_display_name> <4>
  description: This is the first version of the pipeline.
  pipelineSpec: <5>
    # ... generated by kfp compile ...
----
<1> Replace this value with the name of your pipeline version.
<2> Replace this value with name of your project.
<3> Replace this value with the name of your pipeline. This value must match the `metadata.name` value in the `Pipeline` custom resource.
<4> Replace this value with the display name of your pipeline version.
<5> Replace the `spec.pipelineSpec` field with the YAML representation of your pipeline after compilation with the `kfp` SDK.

. Apply the pipeline version definition to create the `PipelineVersion` custom resource in your cluster.
+
In the following command, replace __<pipeline_version_yaml_file>__ with the name of your YAML file:
+
.Example command
[source,subs="+quotes"]
----
$ oc apply -f __<pipeline_version_yaml_file>__.yaml
----
+
After creating the pipeline version, the system automatically applies the following labels to the pipeline version for easier filtering: 
+
.Example automatic labels
[source,yaml]
----
pipelines.kubeflow.org/pipeline-id: <metadata.uid of the pipeline>
pipelines.kubeflow.org/pipeline: <pipeline name>
----

.Verification
. Check that the `Pipeline` custom resource was successfully created:
+
[source,subs="+quotes"]
----
$ oc get pipeline __<pipeline_name>__ -n __<namespace>__
----

. Check that the `PipelineVersion` custom resource was successfully created:
+
[source,subs="+quotes"]
----
$ oc get pipelineversion __<pipeline_version_name>__ -n __<namespace>__
----