:_module-type: REFERENCE

ifdef::context[:parent-context: {context}]
[id="setting-up-lmeval_{context}"]
= Setting up LM-Eval

[role='_abstract']
LM-Eval is a service designed for evaluating large language models that has been integrated into the TrustyAI Operator. 

The service is built on top of two open-source projects: 

* LM Evaluation Harness, developed by EleutherAI, that provides a comprehensive framework for evaluating language models
* Unitxt, a tool that enhances the evaluation process with additional functionalities

The following information explains how to create an `LMEvalJob` custom resource (CR) to initiate an evaluation job and get the results.

ifdef::upstream[]
[NOTE]

--
LM-Eval is only available in the latest community builds. To use LM-Eval on {productname-long}, ensure that you use ODH 2.20 or later versions and add the following `devFlag` to your `DataScienceCluster` resource:
[source]
----
    trustyai:
    devFlags:
        manifests:
        - contextDir: config
            sourcePath: ''
            uri: https://github.com/trustyai-explainability/trustyai-service-operator/tarball/main
    managementState: Managed
----
--
endif::[]


.Global settings for LM-Eval

Configurable global settings for LM-Eval services are stored in the TrustyAI operator global `ConfigMap`, named `trustyai-service-operator-config`. The global settings are located in the same namespace as the operator.

You can configure the following properties for LM-Eval:

.LM-Eval properties
[cols="1,1,5"]
|===
| Property | Default | Description

| `lmes-detect-device`
| `true/false`
| Detect if there are GPUs available and assign a value for `--device argument` for LM Evaluation Harness. If GPUs are available, the value is `cuda`. If there are no GPUs available, the value is `cpu`.

| `lmes-pod-image`
| `quay.io/trustyai/ta-lmes-job:latest`
| The image for the LM-Eval job. The image contains the Python packages for LM Evaluation Harness and Unitxt.

| `lmes-driver-image`
| `quay.io/trustyai/ta-lmes-driver:latest`
| The image for the LM-Eval driver. For detailed information about the driver, see the `cmd/lmes_driver` directory.

| `lmes-image-pull-policy` 
| `Always`
| The image-pulling policy when running the evaluation job.

| `lmes-default-batch-size`
| 8
| The default batch size when invoking the model inference API. Default batch size is only available for local models.

| `lmes-max-batch-size`
| 24
| The maximum batch size that users can specify in an evaluation job.

| `lmes-pod-checking-interval`
| 10s
| The interval to check the job pod for an evaluation job.

| `lmes-allow-online`
| true
| Whether LMEval jobs can set the online mode to `on` to access artifacts (models, datasets, tokenizers) from the internet. 

| `lmes-code-execution`
| true
| Determines whether LMEval jobs can set the `trust remote code` mode to `on`.
 
|===

After updating the settings in the `ConfigMap`, restart the operator to apply the new values.


// Notes on the 'allow online' setting and patches 

// upstream - allowOnline setting is enabled; end-user can disable if they wish
ifdef::upstream[]
[IMPORTANT]
--
The `allowOnline` setting is enabled by default in {productname-long}. Using `allowOnline` gives the job permissions to automatically download artifacts from external sources. Change this setting to `false` if you do not want LM-Eval to access external sources.
--
endif::[]

// downstream - allowOnlie is disabled; end-user needs include a patch in the ConfigMap in order to enable it.
ifndef::upstream[]
[IMPORTANT]
--
The `allowOnline` setting is disabled by default at the operator level in {productname-long}, as using `allowOnline` gives the job permissions to automatically download artifacts from external sources.
--

.Enabling `allowOnline` mode

To enable `allowOnline` mode, patch the TrustyAI operator `ConfigMap` with the following code: 

[source]
----
 kubectl patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'
----

Then restart the TrustyAI operator with: 

[source]
----
kubectl rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications
----
endif::[]

