:_module-type: REFERENCE

[id='ref-vllm-metrics_{context}']
= vLLM metrics

[role='_abstract']
You can track metrics related to your vLLM model.

GPU and CPU cache utilization::
--
Tracks the percentage of GPU memory used by the vLLM model, providing insights into memory efficiency.

_Query_
[source,bash]
----
sum_over_time(vllm:gpu_cache_usage_perc{namespace="${namespace}",pod=~"$model_name.*"}[24h])
----
--

// == Request and resource utilization metrics
--

Running requests::
--
The number of requests actively being processed. Helps monitor workload concurrency.

[source,bash]
----
num_requests_running{namespace="$namespace", pod=~"$model_name.*"}
----
--

Waiting requests::
--
Tracks requests in the queue, indicating system saturation.

[source,bash]
----
num_requests_waiting{namespace="$namespace", pod=~"$model_name.*"}
----
--

Prefix cache hit rates::

--
High hit rates imply efficient reuse of cached computations, optimizing resource usage.

_Queries_
[source,bash]
----
vllm:gpu_cache_usage_perc{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
vllm:cpu_cache_usage_perc{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
----
--

Request total count::
--

_Query_
[source,bash]
----
vllm:request_success_total{finished_reason="length",namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
----
The request ended because it reached the maximum token limit set for the model inference.

_Query_
[source,bash]
----
vllm:request_success_total{finished_reason="stop",namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
----
The request completed naturally based on the model's output or a stop condition, for example, the end of a sentence or token completion.
--
--

// == Performance metrics
--
End-to-end latency::

Measures the overall time to process a request for an optimal user experience.

_Histogram queries_
[source,bash]
----
histogram_quantile(0.99, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.95, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.9, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.5, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
rate(vllm:e2e_request_latency_seconds_sum{namespace="$namespace", pod=~"$model_name.*",model_name="$model_name"}[5m])
rate(vllm:e2e_request_latency_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
----
--

// == Throughput metrics
--
Time to first token (TTFT) latency::
--
The time taken to generate the first token in a response.

_Histogram queries_
[source,bash]
----
histogram_quantile(0.99, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.95, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.9, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.5, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
rate(vllm:time_to_first_token_seconds_sum{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
rate(vllm:time_to_first_token_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
----
--

Time per output token (TPOT) latency::

--
The average time taken to generate each output token.

_Histogram queries_
[source,bash]
----
histogram_quantile(0.99, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.95, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.9, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
histogram_quantile(0.5, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
rate(vllm:time_per_output_token_seconds_sum{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
rate(vllm:time_per_output_token_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
----
--

Prompt token throughput and generation throughput::

--
Tracks the speed of processing prompt tokens for LLM optimization.

_Queries_
[source,bash]
----
rate(vllm:prompt_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
rate(vllm:generation_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
----
--
--

Total tokens generated::
Measures the efficiency of generating response tokens, critical for real-time applications.

_Query_
[source,bash]
----
sum(vllm:generation_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"})
----

//[role="_additional-resources"]
//.Additional resources