:_module-type: REFERENCE

[id='glossary-of-common-terms_{context}']
= Glossary of common terms

This glossary defines common terms for {productname-long}.

accelerator:: In high-performance computing, a specialized circuit that is used to take some of the computational load from the CPU, increasing the efficiency of the system. For example, in deep learning, GPU-accelerated computing is often employed to offload part of the compute workload to a GPU while the main application runs off the CPU. 
//Reference: IBM glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2048370

artificial intelligence (AI):: The capability to acquire, process, create and apply knowledge in the form of a model to make predictions, recommendations or decisions.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x3448902

bias detection:: The process of calculating fairness metrics to detect when AI models are delivering unfair outcomes based on certain attributes.
//Reference:: IBM glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x9721361

custom resource (CR):: A resource implemented through the Kubernetes CustomResourceDefinition API. A custom resource is distinct from the built-in Kubernetes resources, such as the pod and service resources. Every CR is part of an API group.
//Reference: https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/term_glossary.adoc#custom-resource

custom resource definition (CRD):: In Red Hat OpenShift, a custom resource definition (CRD) defines a new, unique object `Kind` in the cluster and lets the Kubernetes API server handle its entire lifecycle.
// Reference:: https://redhat-documentation.github.io/supplementary-style-guide/#custom-resource-definition

connections:: A configuration that stores the parameters required to connect to an S3-compatible object storage, database or OCI-compliant container registry from a data science project.

connection type:: The type of external source to connect to from a data science project, such as an OCI-compliant container registry, S3-compatible object storage, or Uniform Resource Identifiers (URIs).

data science pipelines:: A workflow engine that is used by data scientists and AI engineers to automate pipelines, such as model training and evaluation pipelines. Data science pipelines also includes experiment tracking capabilities, artifact storage, and versioning.

data science project::  An OpenShift project for organizing data science work. Each project is scoped to its own Kubernetes namespace.

disconnected environment:: An environment on a restricted network that does not have an active connection to the internet.
// Reference: https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/term_glossary.adoc#disconnected

distributed workloads:: Data science workloads that are run simultaneously across multiple nodes in an OpenShift cluster.

fine-tuning:: The process of adapting a pre-trained model to perform a specific task by conducting additional training. Fine tuning may involve (1) updating the model's existing parameters, known as full fine tuning, or (2) updating a subset of the model's existing parameters or adding new parameters to the model and training them while freezing the model's existing parameters, known as parameter-efficient fine tuning.
// Reference: RHEL AI glossary, https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x9094307

graphics processing unit (GPU):: A specialized processor designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are heavily utilized in machine learning due to their parallel processing capabilities.
//Reference IBM Glossary https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x8987320

inference:: The process of using a trained AI model to generate predictions or conclusions based on the input data provided to the model. 

inference server:: A server that performs inference. Inference servers feed the input requests through a machine learning model and return an output.

large language model (LLM):: A language model with a large number of parameters, trained on a large quantity of text.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x10298052

machine learning (ML):: A branch of artificial intelligence (AI) and computer science that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving the accuracy of AI models.

model:: In a machine learning context, a set of functions and algorithms that have been trained and tested on a data set to provide predictions or decisions. 
// https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2245601

model registry:: A central repository containing metadata related to machine learning models from inception to deployment. The metadata ranges from high-level information such as the deployment environment and project origins, to intricate details like training hyperparameters, performance metrics, and deployment events.
// Reference: {rhoaidocshome}{default-format-url}/working_with_model_registries/overview-of-model-registries_working-model-registry
// {odhdocshome}/working-with-model-registries/#overview-of-model-registries_model-registry

model server:: A container that hosts a machine learning model, exposes an API to handle incoming requests, performs inference, and returns model predictions.
// Reference: https://www.axelmendoza.com/posts/model-serving-runtime/

model-serving runtime:: A component or framework that helps create model servers for deploying machine learning models and build APIs optimized for inference.
//Reference: https://www.axelmendoza.com/posts/model-serving-runtime/

MLOps:: The practice for collaboration between data scientists and operations professionals to help manage the production machine learning (or deep learning) lifecycle. MLOps looks to increase automation and improve the quality of production ML while also focusing on business and regulatory requirements. It involves model development, training, validation, deployment, monitoring, and management and uses methods like CI/CD.
//Reference: https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x10072886

notebook interface:: An interactive document that contains executable code, descriptive text for that code, and the results of any code that is run.
// Reference https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x2031718

object storage:: A method of storing data, typically used in the cloud, in which data is stored as discrete units, or objects, in a storage pool or repository that does not use a file hierarchy but that stores all objects at the same level.
// Reference https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x5852343

OpenShift Container Platform cluster::  A group of physical machines that contains the controllers, pods, services, and configurations required to build and run containerized applications.
//Reference: https://redhat-documentation.github.io/supplementary-style-guide/#ocp-cluster

persistent storage:: A persistent volume that retains files, models or other artifacts across components such as model deployments, data science pipelines and workbenches.

persistent volume claim (PVC):: A persistent volume claim (PVC) is a request for storage in the cluster by a user.
//Reference https://redhat-documentation.github.io/supplementary-style-guide/#persistent-volume-claim

quantization:: A method of compressing foundation model weights to speed up inferencing and reduce memory needs.
//Reference https://dataplatform.cloud.ibm.com/docs/content/wsj/wscommon/glossary-wx.html?context=wx#x10451003

serving:: The process of hosting a trained machine learning model as a network-accessible service. Real-world applications can send inference requests to the service by using a REST or gRPC API and receive predictions.
// Reference: https://www.hopsworks.ai/dictionary/model-serving

ServingRuntime:: A custom resource definition (CRD) that defines the templates for pods that can serve one or more particular model formats. Each ServingRuntime CRD defines key information such as the container image of the runtime and a list of the model formats that the runtime supports. Other configuration settings for the runtime can be conveyed through environment variables in the container specification. It also dynamically loads and unloads models from disk into memory on demand and exposes a gRPC service endpoint to serve inferencing requests for loaded models.

vLLM:: A high-throughput and efficient inference engine for running large-language models that integrates with popular models and frameworks.

workbench:: An isolated environment for development and experimentation with ML models. Workbenches typically contain integrated development environments (IDEs), such as JupyterLab, RStudio, and Visual Studio Code.

workbench image:: An image that includes preinstalled tools and libraries that you need for model development. Includes an IDE for developing your machine learning (ML) models.

YAML:: A human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted.
